{
  "results": [
    {
      "doc": "1809.01202.pdf",
      "question": "What baseline approaches using state-of-the-art PDTB taggers were employed for the evaluation of causality prediction in the automatic causal explanation analysis pipeline?",
      "prediction": "The baseline approaches used state-of-the-art PDTB taggers for the evaluation of causality prediction in the automatic causal explanation analysis pipeline.",
      "answer": "[{'answer': 'state-of-the-art PDTB taggers', 'type': 'extractive'}, {'answer': 'Linear SVM, RBF SVM, and Random Forest', 'type': 'abstractive'}]",
      "evidence": "We first use state-of-the-art PDTB taggers for our baseline BIBREF13 , BIBREF12 for the evaluation of the causality prediction of our models ( BIBREF12 requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message).",
      "pages_used": [
        {
          "doc": "1903.00172.pdf",
          "page": 13,
          "matched": false,
          "score": 0.18787878787878787
        }
      ]
    },
    {
      "doc": "1909.00754.pdf",
      "question": "Based on the joint goal accuracy and inference time complexity, does the model demonstrate superior performance in the single-domain WoZ2.0 test set or the multi-domain MultiWoZ test set?",
      "prediction": "The model demonstrates superior performance in the multi-domain MultiWoZ test set.",
      "answer": "[{'answer': 'single-domain setting', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model (Mrksic et al., 2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ (Budzianowski et al., 2018).",
      "pages_used": [
        {
          "doc": "1701.06538.pdf",
          "page": 20,
          "matched": false,
          "score": 0.08900523560209424
        }
      ]
    },
    {
      "doc": "1912.10011.pdf",
      "question": "Which hierarchical model variant achieves a higher BLEU score of 17.5 compared to the Flat scenario (16.7)?",
      "prediction": "Hierarchical-k",
      "answer": "[{'answer': 'Hierarchical-k', 'type': 'extractive'}]",
      "evidence": "To evaluate the impact of our model components, we first compare scenarios Flat, Hierarchical-k, and Hierarchical-kv. As shown in Table TABREF25, we can see the lower results obtained by the Flat scenario compared to the other scenarios (e.g. BLEU $16.7$ vs. $17.5$ for resp. Flat and Hierarchical-k), suggesting the effectiveness of encoding the data-structure using a hierarchy.",
      "pages_used": [
        {
          "doc": "1912.10011.pdf",
          "page": 10,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1707.08559.pdf",
      "question": "What were the F-scores of the best performing model on the NALCS (English) and LMS (Traditional Chinese) test sets, as reported in the video highlight prediction paper using multimodal and multilingual audience chat reactions?",
      "prediction": "The F-scores of the best performing model on the NALCS (English) and LMS (Traditional Chinese) test sets were not provided in the video highlight prediction paper using multimodal and multilingual audience chat reactions.",
      "answer": "[{'answer': 'Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Test Results on the NALCS (English) and LMS (Traditional Chinese) datasets.",
      "pages_used": [
        {
          "doc": "1709.05413.pdf",
          "page": 13,
          "matched": false,
          "score": 0.03058103975535168
        }
      ]
    },
    {
      "doc": "1906.03538.pdf",
      "question": "What is the average token count of claims in the PERSPECTRUM dataset?",
      "prediction": "The average token count of claims in the PERSPECTRUM dataset is 10.",
      "answer": "[{'answer': 'Average claim length is 8.9 tokens.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: A summary of PERSPECTRUM statistics",
      "pages_used": [
        {
          "doc": "1906.03538.pdf",
          "page": 16,
          "matched": false,
          "score": 0.20512820512820512
        }
      ]
    },
    {
      "doc": "1908.11047.pdf",
      "question": "Does incorporating shallow syntax-aware embeddings lead to better performance on the sentiment classification task compared to using standard ELMo embeddings?",
      "prediction": "Yes, incorporating shallow syntax-aware embeddings leads to better performance on the sentiment classification task compared to using standard ELMo embeddings.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",
      "evidence": "Results are shown in Table TABREF12. Consistent with previous findings, cwrs offer large improvements across all tasks.",
      "pages_used": [
        {
          "doc": "1810.12885.pdf",
          "page": 14,
          "matched": false,
          "score": 0.2564102564102564
        }
      ]
    },
    {
      "doc": "1908.11047.pdf",
      "question": "Out of the 9 probe tasks, how many tasks show better performance with the mSynC model compared to baseline ELMo-transformer embeddings in the \"Shallow Syntax in Deep Water\" study?",
      "prediction": "5 tasks show better performance with the mSynC model compared to baseline ELMo-transformer embeddings in the \"Shallow Syntax in Deep Water\" study.",
      "answer": "[{'answer': 'performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks', 'type': 'extractive'}, {'answer': '3', 'type': 'abstractive'}]",
      "evidence": "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks.",
      "pages_used": [
        {
          "doc": "1904.10500.pdf",
          "page": 13,
          "matched": false,
          "score": 0.03847762442492681
        }
      ]
    },
    {
      "doc": "1908.11047.pdf",
      "question": "What specific set of black-box probes, as described in the evidence section of this paper, were used to analyze the role of shallow syntactic awareness in the enhanced ELMo architecture's embeddings for capturing linguistic properties?",
      "prediction": "The specific set of black-box probes used to analyze the role of shallow syntactic awareness in the enhanced ELMo architecture's embeddings for capturing linguistic properties were not provided in the given text.",
      "answer": "[{'answer': 'CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,\\nChunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection', 'type': 'abstractive'}, {'answer': 'Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.', 'type': 'extractive'}]",
      "evidence": " We further examine the contextual embeddings obtained from the enhanced architecture and a shallow syntactic context, using black-box probes from BIBREF1",
      "pages_used": [
        {
          "doc": "1811.02906.pdf",
          "page": 10,
          "matched": false,
          "score": 0.043795620437956206
        }
      ]
    },
    {
      "doc": "1908.11047.pdf",
      "question": "Based on the findings in *Shallow Syntax in Deep Water*, how does mSynC's performance on downstream NLP tasks, especially in chunk tag prediction, compare to that of ELMo-transformer, and what does this imply about the utility of shallow syntactic features?",
      "prediction": "mSynC's performance on downstream NLP tasks, particularly in chunk tag prediction, is comparable to that of ELMo-transformer, indicating that shallow syntactic features are still useful.",
      "answer": "[{'answer': 'only modest gains on three of the four downstream tasks', 'type': 'extractive'}, {'answer': ' the performance differences across all tasks are small enough ', 'type': 'extractive'}]",
      "evidence": "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. As we would expect, on the probe for predicting chunk tags, mSynC achieves 96.9 $F_1$ vs. 92.2 $F_1$ for ELMo-transformer, indicating that mSynC is indeed encoding shallow syntax. Overall, the results further confirm that explicit shallow syntax does not offer any benefits over ELMo-transformer.",
      "pages_used": [
        {
          "doc": "1909.09484.pdf",
          "page": 10,
          "matched": false,
          "score": 0.27
        }
      ]
    },
    {
      "doc": "2001.08051.pdf",
      "question": "How were the six predefined proficiency indicators scored in the 2017/2018 recordings of the TLT-school corpus for both spoken and written items, and what values did human experts assign to reflect proficiency levels?",
      "prediction": "The six predefined proficiency indicators were scored using a 5-point scale, with human experts assigning values to reflect proficiency levels.",
      "answer": "[{'answer': 'They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.', 'type': 'abstractive'}]",
      "evidence": "Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.23293172690763053
        }
      ]
    },
    {
      "doc": "2001.08051.pdf",
      "question": "What are the six linguistic proficiency indicators outlined in the TLT-school corpus that human experts used to assess the English and German utterances of non-native speaking students?",
      "prediction": "The six linguistic proficiency indicators outlined in the TLT-school corpus are: 1) Pronunciation, 2) Vocabulary, 3) Grammar, 4) Syntax, 5) Discourse, and 6) Fluency.",
      "answer": "[{'answer': '6 indicators:\\n- lexical richness\\n- pronunciation and fluency\\n- syntactical correctness\\n- fulfillment of delivery\\n- coherence and cohesion\\n- communicative, descriptive, narrative skills', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: List of the indicators used by human experts to evaluate specific linguistic competences.",
      "pages_used": [
        {
          "doc": "2002.02492.pdf",
          "page": 15,
          "matched": false,
          "score": 0.2903225806451613
        }
      ]
    },
    {
      "doc": "2001.08051.pdf",
      "question": "What are the Word Error Rates (WER) achieved by the adapted Automatic Speech Recognition (ASR) system on the non-native English and German children's speech evaluation data in the TLT-school corpus?",
      "prediction": "The adapted ASR system achieved Word Error Rates (WER) of 15.2% on the non-native English and 16.5% on the non-native German children's speech evaluation data in the TLT-school corpus.",
      "answer": "[{'answer': 'Accuracy not available: WER results are reported 42.6 German, 35.9 English', 'type': 'abstractive'}]",
      "evidence": "Table , extracted from BIBREF0, reports WERs obtained on evaluation data sets with a strongly adapted ASR, demonstrating the difficulty of the related speech recognition task for both languages.",
      "pages_used": [
        {
          "doc": "1909.08041.pdf",
          "page": 11,
          "matched": false,
          "score": 0.03296170625302957
        }
      ]
    },
    {
      "doc": "2001.08051.pdf",
      "question": "What metric is used to evaluate the performance of the adapted ASR system on non-native children's English and German speech?",
      "prediction": "The performance of the adapted ASR system on non-native children's English and German speech is evaluated using the word error rate (WER) metric.",
      "answer": "[{'answer': 'Speech recognition system is evaluated using WER metric.', 'type': 'abstractive'}]",
      "evidence": "Table , extracted from BIBREF0, reports WERs obtained on evaluation data sets with a strongly adapted ASR, demonstrating the difficulty of the related speech recognition task for both languages.",
      "pages_used": [
        {
          "doc": "1911.12579.pdf",
          "page": 17,
          "matched": false,
          "score": 0.27298050139275765
        }
      ]
    },
    {
      "doc": "2001.08051.pdf",
      "question": "How many utterances were manually transcribed in the TLT-school corpus, across both English and German languages, including train and evaluation sets?",
      "prediction": "The TLT-school corpus includes a total of 1,000 utterances that were manually transcribed, encompassing both English and German languages across the train and evaluation sets.",
      "answer": "[{'answer': 'Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned)', 'type': 'abstractive'}]",
      "evidence": "Speakers were assigned either to training or evaluation sets, with proportions of $\\frac{2}{3}$ and $\\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.29818181818181816
        }
      ]
    },
    {
      "doc": "1901.05280.pdf",
      "question": "What are the baseline models compared in this paper, which summarizes prior work on span and dependency SRL, including models like Fitzgerald2015?",
      "prediction": "The baseline models compared in this paper include models like Fitzgerald2015.",
      "answer": "[{'answer': '2008 Punyakanok et al. \\n2009 Zhao et al. + ME \\n2008 Toutanova et al. \\n2010 Bjorkelund et al.  \\n2015 FitzGerald et al. \\n2015 Zhou and Xu \\n2016 Roth and Lapata \\n2017 He et al. \\n2017 Marcheggiani et al.\\n2017 Marcheggiani and Titov \\n2018 Tan et al. \\n2018 He et al. \\n2018 Strubell et al. \\n2018 Cai et al. \\n2018 He et al. \\n2018 Li et al. \\n', 'type': 'abstractive'}]",
      "evidence": "Generally, the above work is summarized in Table TABREF2 . Considering motivation, our work is most closely related to the work of BIBREF14 Fitzgerald2015, which also tackles span and dependency SRL in a uniform fashion. The essential difference is that their model employs the syntactic features and takes pre-identified predicates as inputs, while our model puts syntax aside and jointly learns and predicts predicates and arguments.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.24731182795698925
        }
      ]
    },
    {
      "doc": "2004.03354.pdf",
      "question": "What are the eight biomedical NER tasks that were used to compare the performance of GreenBioBERT and BioBERT in the context of inexpensive domain adaptation?",
      "prediction": "The eight biomedical NER tasks used for comparison were: Disease, Symptom, Treatment, Procedure, Organ, Tissue, Substance, and Condition.",
      "answer": "[{'answer': 'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800', 'type': 'abstractive'}, {'answer': 'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Top: Examples of within-space and cross-space nearest neighbors (NNs) by cosine similarity in GreenBioBERT’s wordpiece embedding layer. Blue: Original wordpiece space. Green: Aligned Word2Vec space. Bottom: Biomedical NER test set precision / recall / F1 (%) measured with the CoNLL NER scorer. Boldface: Best model in row. Underlined: Best inexpensive model (without target-domain pretraining) in row.",
      "pages_used": [
        {
          "doc": "1906.03538.pdf",
          "page": 16,
          "matched": false,
          "score": 0.27586206896551724
        }
      ]
    },
    {
      "doc": "1909.03242.pdf",
      "question": "What specific metadata fields are provided, in addition to the claim and its label, in the example claim instance from the MultiFC dataset?",
      "prediction": "The example claim instance from the MultiFC dataset includes specific metadata fields such as the name of the author, the title of the paper, the year of publication, and the journal or conference where the paper was published.",
      "answer": "[{'answer': 'besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: An example of a claim instance. Entities are obtained via entity linking. Article and outlink texts, evidence search snippets and pages are not shown.",
      "pages_used": [
        {
          "doc": "1905.10810.pdf",
          "page": 5,
          "matched": false,
          "score": 0.22377622377622378
        }
      ]
    },
    {
      "doc": "1910.08987.pdf",
      "question": "What are the NMI values between the clustered tone contours and the ground truth tone categories for all syllables in both Mandarin and Cantonese, as reported in your phonemic tone clustering analysis?",
      "prediction": "The NMI values between the clustered tone contours and the ground truth tone categories for all syllables in both Mandarin and Cantonese, as reported in the phonemic tone clustering analysis, are not provided in the given text.",
      "answer": "[{'answer': 'NMI between cluster assignments and ground truth tones for all sylables is:\\nMandarin: 0.641\\nCantonese: 0.464', 'type': 'abstractive'}]",
      "evidence": "To test this hypothesis, we evaluate the model on only the first syllable of every word, which eliminates carry-over and declination effects (Table TABREF14). In both Mandarin and Cantonese, the clustering is more accurate when using only the first syllables, compared to using all of the syllables.",
      "pages_used": [
        {
          "doc": "1705.01265.pdf",
          "page": 7,
          "matched": false,
          "score": 0.0375234521575985
        }
      ]
    },
    {
      "doc": "1911.02086.pdf",
      "question": "Does the paper compare the execution time of their GDSConv-based model with other state-of-the-art keyword spotting models that use traditional acoustic feature extraction techniques?",
      "prediction": "No, the paper does not compare the execution time of their GDSConv-based model with other state-of-the-art keyword spotting models that use traditional acoustic feature extraction techniques.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "The base model composed of DSConv layers without grouping achieves the state-of-the-art accuracy of 96.6% on the Speech Commands test set. The low-parameter model with GDSConv achieves almost the same accuracy of 96.4% with only about half the parameters. ",
      "pages_used": [
        {
          "doc": "2002.02492.pdf",
          "page": 15,
          "matched": false,
          "score": 0.24584717607973422
        }
      ]
    },
    {
      "doc": "1803.09230.pdf",
      "question": "How much do the F1 scores improve when using the hybrid attention method and DCA compared to BiDAF and DCN on the SQuAD dataset?",
      "prediction": "The F1 scores improve by 5.18% when using the hybrid attention method and DCA compared to BiDAF and DCN on the SQuAD dataset.",
      "answer": "[{'answer': 'In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Effect of Character Embedding",
      "pages_used": [
        {
          "doc": "1809.00530.pdf",
          "page": 12,
          "matched": false,
          "score": 0.013112491373360938
        }
      ]
    },
    {
      "doc": "1911.13066.pdf",
      "question": "What is the accuracy score of the highest-performing McM model on the 12-class bilingual SMS dataset?",
      "prediction": "0.86",
      "answer": "[{'answer': 'the best performing model obtained an accuracy of 0.86', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3. Performance evaluation of variations of the proposed model and baseline. Showing highest scores in boldface.",
      "pages_used": [
        {
          "doc": "1911.13066.pdf",
          "page": 9,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1911.13066.pdf",
      "question": "What are the 12 class labels and their distribution (as given in %) for the bilingual Roman Urdu-English SMS feedback dataset discussed in the study on McM deep learning model for SMS classification?",
      "prediction": "The 12 class labels and their distribution (as given in %) for the bilingual Roman Urdu-English SMS feedback dataset discussed in the study on McM deep learning model for SMS classification are: Appreciation (43.1%), Satisfied (31.1%), Peripheral complaint (8.2%), Demanded",
      "answer": "[{'answer': 'Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1. Description of class label along with distribution of each class (in %) in the acquired dataset",
      "pages_used": [
        {
          "doc": "1911.13066.pdf",
          "page": 3,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1701.09123.pdf",
      "question": "What metrics (precision, recall, F1) are reported for evaluating the NER system's performance in the CoNLL 2003 English results?",
      "prediction": "The metrics reported for evaluating the NER system's performance in the CoNLL 2003 English results are precision, recall, and F1.",
      "answer": "[{'answer': 'Precision, Recall, F1', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: CoNLL 2003 English results.",
      "pages_used": [
        {
          "doc": "1701.09123.pdf",
          "page": 19,
          "matched": false,
          "score": 0.009494743981010512
        }
      ]
    },
    {
      "doc": "1701.09123.pdf",
      "question": "What are the datasets that were used for training, development, and out-of-domain evaluation in the multilingual named entity recognition experiments, including corpora such as MUC7, SONAR-1, and Ancora 2.0?",
      "prediction": "The datasets used for training, development, and out-of-domain evaluation in the multilingual named entity recognition experiments include CoNLL 2003, CoNLL 2002, CoNLL 2000, EFE 2000, De Morgen 200",
      "answer": "[{'answer': 'CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Datasets used for training, development and evaluation. MUC7: only three classes (LOC, ORG, PER) of the formal run are used for out-of-domain evaluation. As there are not standard partitions of SONAR-1 and Ancora 2.0, the full corpus was used for training and later evaluated in-out-of-domain settings.",
      "pages_used": [
        {
          "doc": "1701.09123.pdf",
          "page": 5,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2004.01878.pdf",
      "question": "What is the total number of documents in the dataset in the paper on modeling stock movements using a recurrent state transition model influenced by news events?",
      "prediction": "40",
      "answer": "[{'answer': '553,451 documents', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Statistics of the datasets.",
      "pages_used": [
        {
          "doc": "1812.10479.pdf",
          "page": 40,
          "matched": false,
          "score": 0.021206096752816435
        }
      ]
    },
    {
      "doc": "2003.04866.pdf",
      "question": "Which 12 languages, including both major and less-resourced ones, are listed summarizing the language datasets in the Multi-SimLex lexical resource?",
      "prediction": "The 12 languages listed are English, French, German, Spanish, Italian, Dutch, Russian, Portuguese, Chinese, Japanese, Korean, and Arabic.",
      "answer": "[{'answer': 'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese', 'type': 'abstractive'}, {'answer': 'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese', 'type': 'abstractive'}]",
      "evidence": "Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex.",
      "pages_used": [
        {
          "doc": "1903.00172.pdf",
          "page": 13,
          "matched": false,
          "score": 0.34355828220858897
        }
      ]
    },
    {
      "doc": "1904.07904.pdf",
      "question": "What is the exact EM and F1 score achieved by the proposed adversarial domain adaptation model to address ASR errors, as reported in the study on mitigating ASR errors in spoken question answering?",
      "prediction": "The exact EM and F1 score achieved by the proposed adversarial domain adaptation model to address ASR errors, as reported in the study on mitigating ASR errors in spoken question answering, is not provided in the given text.",
      "answer": "[{'answer': 'Best results authors obtain is EM 51.10 and F1 63.11', 'type': 'abstractive'}, {'answer': 'EM Score of 51.10', 'type': 'abstractive'}]",
      "evidence": "To better demonstrate the effectiveness of the proposed model, we compare with baselines and show the results in Table TABREF12 .",
      "pages_used": [
        {
          "doc": "1904.07904.pdf",
          "page": 6,
          "matched": false,
          "score": 0.2247191011235955
        }
      ]
    },
    {
      "doc": "1909.00175.pdf",
      "question": "What baseline models are compared for pun detection and location in terms of precision, recall, and F1-score against the proposed approach?",
      "prediction": "The baseline models compared for pun detection and location in terms of precision, recall, and F1-score against the proposed approach are the CRF model and the baseline model based on conditional random fields (CRF).",
      "answer": "[{'answer': 'They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Comparison results on two benchmark datasets. (P.: Precision, R.: Recall, F1: F1 score.)",
      "pages_used": [
        {
          "doc": "1909.00175.pdf",
          "page": 3,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1909.03135.pdf",
      "question": "What were the token counts for the English and Russian training corpora in the experiments comparing lemmatized and non-lemmatized inputs for word sense disambiguation?",
      "prediction": "The English training corpus had 1.5 million tokens, and the Russian training corpus had 1.2 million tokens.",
      "answer": "[{'answer': '2174000000, 989000000', 'type': 'abstractive'}, {'answer': '2174 million tokens for English and 989 million tokens for Russian', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Training corpora",
      "pages_used": [
        {
          "doc": "1809.06537.pdf",
          "page": 7,
          "matched": false,
          "score": 0.00455005055611729
        }
      ]
    },
    {
      "doc": "2001.05467.pdf",
      "question": "What baseline models, including LSTM, HRED, VHRED (with and without attention), and others reported in prior work, are compared to the AvgOut-based models in terms of F1 score and dialogue diversity/relevance?",
      "prediction": "The baseline models compared to the AvgOut-based models in terms of F1 score and dialogue diversity/relevance include LSTM, HRED, VHRED (with and without attention), and others reported in prior work.",
      "answer": "[{'answer': 'LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Automatic Evaluation Activity/Entity F1 results for baselines and our 3 models (attn means “with attention”). LSTM, HRED and VHRED are reported in Serban et al. (2017a), VHRED (attn) and Reranking-RL in Niu and Bansal (2018a), and the rest are produced by our work. All our four models have statistically significantly higher F1 values (p < 0.001) against VHRED (attn) and MMI.",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.2639593908629442
        }
      ]
    },
    {
      "doc": "2001.05467.pdf",
      "question": "What is the reported improvement in both diversity and relevance scores for the MinAvgOut, LFT, RL, and hybrid models compared to the base LSTM-RNN?",
      "prediction": "The reported improvement in both diversity and relevance scores for the MinAvgOut, LFT, RL, and hybrid models compared to the base LSTM-RNN is 6.83 and 4.77, respectively.",
      "answer": "[{'answer': 'on diversity 6.87 and on relevance 4.6 points higher', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Automatic Evaluation Activity/Entity F1 results for baselines and our 3 models (attn means “with attention”). LSTM, HRED and VHRED are reported in Serban et al. (2017a), VHRED (attn) and Reranking-RL in Niu and Bansal (2018a), and the rest are produced by our work. All our four models have statistically significantly higher F1 values (p < 0.001) against VHRED (attn) and MMI.",
      "pages_used": [
        {
          "doc": "2001.05467.pdf",
          "page": 5,
          "matched": false,
          "score": 0.041216486594637856
        }
      ]
    },
    {
      "doc": "1908.08345.pdf",
      "question": "What are the ROUGE-1, ROUGE-2, and ROUGE-L scores achieved by the best-performing abstractive model on the CNN/DailyMail, XSum, and NYT datasets, as reported in the \"Text Summarization with Pretrained Encoders\" paper?",
      "prediction": "The best-performing abstractive model achieved ROUGE-1, ROUGE-2, and ROUGE-L scores of 30.5, 20.5, and 10.5, respectively, on the CNN/DailyMail dataset. On the XSum dataset, the scores were",
      "answer": "[{'answer': 'Best results on unigram:\\nCNN/Daily Mail: Rogue F1 43.85\\nNYT: Rogue Recall 49.02\\nXSum: Rogue F1 38.81', 'type': 'abstractive'}, {'answer': 'Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55', 'type': 'abstractive'}]",
      "evidence": "The third block in Table TABREF23 highlights the performance of several abstractive models on the CNN/DailyMail dataset (see Section SECREF6 for an overview).",
      "pages_used": [
        {
          "doc": "2001.06888.pdf",
          "page": 13,
          "matched": false,
          "score": 0.205761316872428
        }
      ]
    },
    {
      "doc": "1606.00189.pdf",
      "question": "Which dataset, consisting of 1,312 error-annotated sentences with 30,144 tokens and using the M2 Scorer v3.2 for evaluation, did the authors employ for grammatical error correction in their neural network translation models study?",
      "prediction": "The authors used the CoNLL-2014 dataset for their study on grammatical error correction in neural network translation models.",
      "answer": "[{'answer': 'CoNLL 2014', 'type': 'extractive'}]",
      "evidence": ". The evaluation is performed similar to the CoNLL 2014 shared task setting using the the official test data of the CoNLL 2014 shared task with annotations from two annotators (without considering alternative annotations suggested by the participating teams). The test dataset consists of 1,312 error-annotated sentences with 30,144 tokens on the source side. We make use of the official scorer for the shared task, M INLINEFORM0 Scorer v3.2 BIBREF19 , for evaluation.",
      "pages_used": [
        {
          "doc": "1905.10810.pdf",
          "page": 5,
          "matched": false,
          "score": 0.2976190476190476
        }
      ]
    },
    {
      "doc": "1901.01010.pdf",
      "question": "Does the joint model combining text and visual features, as described in the paper's experimental results, outperform text-only and visual-only models on both Wikipedia articles and arXiv subsets?",
      "prediction": "Yes, the joint model combining text and visual features outperforms text-only and visual-only models on both Wikipedia articles and arXiv subsets.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",
      "evidence": "Experimental results show that our joint model outperforms the visual-only model in all cases, and the text-only model on Wikipedia and two subsets of arXiv.",
      "pages_used": [
        {
          "doc": "1901.01010.pdf",
          "page": 6,
          "matched": false,
          "score": 0.02522935779816514
        }
      ]
    },
    {
      "doc": "1901.01010.pdf",
      "question": "What are the exact accuracy results on the Wikipedia dataset, as well as the peer-reviewed Archive AI, Archive Computation and Language, and Archive Machine Learning papers, as reported in the document quality assessment paper?",
      "prediction": "The document does not provide the exact accuracy results on the Wikipedia dataset, as well as the peer-reviewed Archive AI, Archive Computation and Language, and Archive Machine Learning papers.",
      "answer": "[{'answer': '59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Experimental results. The best result for each dataset is indicated in bold, and marked with “†” if it is significantly higher than the second best result (based on a one-tailed Wilcoxon signed-rank test; p < 0.05). The results of Benchmark on Peer Review are from the original paper, where the standard deviation values were not reported.",
      "pages_used": [
        {
          "doc": "1906.03538.pdf",
          "page": 16,
          "matched": false,
          "score": 0.2111111111111111
        }
      ]
    },
    {
      "doc": "1611.04642.pdf",
      "question": "Which datasets are used to report the link prediction results for the Embedded Knowledge Graph Network (EKGN) model in the \"Link Prediction using Embedded Knowledge Graphs\" paper?",
      "prediction": "The paper reports the link prediction results for the EKGN model on the FB15k and FB15k-237 datasets.",
      "answer": "[{'answer': 'WN18 and FB15k', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: The knowledge base completion (link prediction) results on WN18 and FB15k.",
      "pages_used": [
        {
          "doc": "1910.11235.pdf",
          "page": 6,
          "matched": false,
          "score": 0.026565464895635674
        }
      ]
    },
    {
      "doc": "1806.07711.pdf",
      "question": "Based on the analysis of syntactic patterns in WordNet's glosses, how many distinct semantic roles are introduced?",
      "prediction": "The analysis of syntactic patterns in WordNet's glosses introduces 10 distinct semantic roles.",
      "answer": "[{'answer': '12', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Most common syntactic patterns for each semantic role.",
      "pages_used": [
        {
          "doc": "1810.12196.pdf",
          "page": 10,
          "matched": false,
          "score": 0.017817371937639197
        }
      ]
    },
    {
      "doc": "1810.12085.pdf",
      "question": "What specific HPI categories are outlined under the Annotation Instructions for the LSTM model's task of extractive summarization in this study?",
      "prediction": "The specific HPI categories outlined under the Annotation Instructions for the LSTM model's task of extractive summarization in this study are Demographics, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, and Other.",
      "answer": "[{'answer': 'Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.', 'type': 'abstractive'}, {'answer': 'Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1. HPI Categories and Annotation Instructions",
      "pages_used": [
        {
          "doc": "1810.12085.pdf",
          "page": 3,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1810.12085.pdf",
      "question": "Did the evaluation of the LSTM model on the 515 annotated history of present illness notes include a comparison with other existing extractive summarization techniques?",
      "prediction": "Yes, the evaluation of the LSTM model on the 515 annotated history of present illness notes included a comparison with other existing extractive summarization techniques.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "We evaluated our model on the 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set.",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 13,
          "matched": false,
          "score": 0.2054794520547945
        }
      ]
    },
    {
      "doc": "1909.03544.pdf",
      "question": "According to previous methods for POS tagging and lemmatization in Czech text processing, which were outperformed by the BERT and Flair-based approaches?",
      "prediction": "Previous methods for POS tagging and lemmatization in Czech text processing, which were outperformed by the BERT and Flair-based approaches, were the LSTM-based approaches.",
      "answer": "[{'answer': 'Table TABREF44, Table TABREF44, Table TABREF47, Table TABREF47', 'type': 'extractive'}]",
      "evidence": "The POS tagging and lemmatization results are presented in Table TABREF44.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.2575107296137339
        }
      ]
    },
    {
      "doc": "1805.04033.pdf",
      "question": "According to the ROUGE metrics, which baseline models are outperformed by the proposed regularization approach in the context of abstractive Chinese social media text summarization?",
      "prediction": "The baseline models that are outperformed by the proposed regularization approach in the context of abstractive Chinese social media text summarization are the baseline models.",
      "answer": "[{'answer': 'RNN-context, SRB, CopyNet, RNN-distract, DRGD', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3. Comparisons with the Existing Models in Terms of ROUGE Metrics",
      "pages_used": [
        {
          "doc": "1909.09484.pdf",
          "page": 10,
          "matched": false,
          "score": 0.17194570135746606
        }
      ]
    },
    {
      "doc": "2001.10161.pdf",
      "question": "How did participants in the evaluations on interactive fiction generation rate the neural model's performance in terms of coherence, genre resemblance, and overall quality compared to rule-based methods and human-made games across both mystery and fairy-tale genres?",
      "prediction": "The evaluations on interactive fiction generation showed that participants rated the neural model's performance as superior to rule-based methods and human-made games in terms of coherence, genre resemblance, and overall quality across both mystery and fairy-tale genres.",
      "answer": "[{'answer': 'the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all', 'type': 'extractive'}]",
      "evidence": "We conducted two sets of human participant evaluations by recruiting participants over Amazon Mechanical Turk. The first evaluation tests the knowledge graph construction phase, in which we measure perceived coherence and genre or theme resemblance of graphs extracted by different models. The second study compares full games—including description generation and game assembly, which can't easily be isolated from graph construction—generated by different methods. This study looks at how interesting the games were to the players in addition to overall coherence and genre resemblance. Both studies are performed across two genres: mystery and fairy-tales.",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.29518072289156627
        }
      ]
    },
    {
      "doc": "1906.10225.pdf",
      "question": "What specific evaluation metrics, including unlabeled INLINEFORM0 scores, are reported for assessing the performance of your compound probabilistic context-free grammar model compared to baseline models in unsupervised parsing experiments on English and Chinese?",
      "prediction": "The specific evaluation metrics reported for assessing the performance of the compound probabilistic context-free grammar model compared to baseline models in unsupervised parsing experiments on English and Chinese include unlabeled INLINEFORM0 scores.",
      "answer": "[{'answer': 'INLINEFORM0 scores', 'type': 'extractive'}, {'answer': 'Unlabeled sentence-level F1, perplexity, grammatically judgment performance', 'type': 'abstractive'}]",
      "evidence": "Table TABREF23 shows the unlabeled INLINEFORM0 scores for our models and various baselines.",
      "pages_used": [
        {
          "doc": "1906.10225.pdf",
          "page": 16,
          "matched": false,
          "score": 0.1956521739130435
        }
      ]
    },
    {
      "doc": "1906.11180.pdf",
      "question": "What are the precision scores for semantic typing and entity matching as reported in the paper, specifically on the S-Lite and R-Lite datasets compared to the baselines?",
      "prediction": "The paper does not provide the precision scores for semantic typing and entity matching on the S-Lite and R-Lite datasets compared to the baselines.",
      "answer": "[{'answer': '0.8320 on semantic typing, 0.7194 on entity matching', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3. Overall typing performance of our method and the baselines on S-Lite and R-Lite.",
      "pages_used": [
        {
          "doc": "1910.08210.pdf",
          "page": 14,
          "matched": false,
          "score": 0.017699115044247787
        }
      ]
    },
    {
      "doc": "1806.04330.pdf",
      "question": "Does the paper summarize any transformer-based models such as BERT or GPT for sentence pair modeling?",
      "prediction": "No, the paper does not summarize any transformer-based models such as BERT or GPT for sentence pair modeling.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 1: Summary of representative neural models for sentence pair modeling. The upper half contains sentence encoding models, and the lower half contains sentence pair interaction models.",
      "pages_used": [
        {
          "doc": "1902.10525.pdf",
          "page": 14,
          "matched": false,
          "score": 0.04395604395604396
        }
      ]
    },
    {
      "doc": "1912.01772.pdf",
      "question": "How are non-standard pronunciations and mispronunciations annotated in the transcriptions of the 142-hour Mapudungun medical conversations corpus provided for computational linguistic experiments?",
      "prediction": "Non-standard pronunciations and mispronunciations are annotated in the transcriptions of the 142-hour Mapudungun medical conversations corpus provided for computational linguistic experiments by highlighting them in red.",
      "answer": "[{'answer': 'Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.', 'type': 'abstractive'}]",
      "evidence": "In addition, the transcription includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. Foreign words, in this case Spanish words, are also labelled as such.",
      "pages_used": [
        {
          "doc": "1810.12885.pdf",
          "page": 14,
          "matched": false,
          "score": 0.23853211009174313
        }
      ]
    },
    {
      "doc": "1608.06757.pdf",
      "question": "Which NER systems were evaluated with micro-precision, recall, and F1 scores on datasets like CoNLL2003, KORE50, ACE2004, and MSNBC in the 'Robust Named Entity Recognition in Idiosyncratic Domains' paper?",
      "prediction": "The NER systems evaluated with micro-precision, recall, and F1 scores on datasets such as CoNLL2003, KORE50, ACE2004, and MSNBC in the 'Robust Named Entity Recognition in Idiosyncratic Domains' paper were Stanf, CR",
      "answer": "[{'answer': 'Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Comparison of annotators trained for common English news texts (micro-averaged scores on match per annotation span). The table shows micro-precision, recall and NER-style F1 for CoNLL2003, KORE50, ACE2004 and MSNBC datasets.",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.19047619047619047
        }
      ]
    },
    {
      "doc": "1808.09029.pdf",
      "question": "What recurrent neural network models are benchmarked against the Pyramidal Recurrent Unit to evaluate its performance on word-level language modeling?",
      "prediction": "The benchmarked recurrent neural network models against the Pyramidal Recurrent Unit for word-level language modeling include the Long Short-Term Memory (LSTM) network and the Gated Recurrent Unit (GRU) network.",
      "answer": "[{'answer': 'Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM', 'type': 'abstractive'}]",
      "evidence": "Table TABREF23 compares the performance of the PRU with state-of-the-art methods. ",
      "pages_used": [
        {
          "doc": "2001.05493.pdf",
          "page": 10,
          "matched": false,
          "score": 0.012237762237762238
        }
      ]
    },
    {
      "doc": "1910.12129.pdf",
      "question": "Was the baseline Transformer-based sequence-to-sequence model evaluated on the newly introduced video game data-to-text corpus, and what were its performance metrics, including BLEU, METEOR, ROUGE, CIDEr, and SER?",
      "prediction": "The baseline Transformer-based sequence-to-sequence model was not evaluated on the newly introduced video game data-to-text corpus.",
      "answer": "[{'answer': 'Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.', 'type': 'abstractive'}]",
      "evidence": "The NLG model we use to establish a baseline for this dataset is a standard Transformer-based BIBREF19 sequence-to-sequence model.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.18430034129692832
        }
      ]
    },
    {
      "doc": "2002.01207.pdf",
      "question": "What linguistic features are specified in the biLSTM-based Arabic diacritic recovery model for case ending reconstruction?",
      "prediction": "The biLSTM-based Arabic diacritic recovery model for case ending reconstruction specifies linguistic features such as word boundaries, word classes, and part-of-speech tags.",
      "answer": "[{'answer': 'POS, gender/number and stem POS', 'type': 'abstractive'}]",
      "evidence": "Table TABREF17 lists the features that we used for CE recovery.",
      "pages_used": [
        {
          "doc": "2002.01207.pdf",
          "page": 22,
          "matched": false,
          "score": 0.07734806629834254
        }
      ]
    },
    {
      "doc": "1606.05320.pdf",
      "question": "How much lower is the log likelihood for LSTMs compared to HMMs across datasets, both when they have similar numbers of parameters and when the LSTM has an increased number of parameters?",
      "prediction": "The log likelihood for LSTMs is 10% lower than HMMs across datasets, both when they have similar numbers of parameters and when the LSTM has an increased number of parameters.",
      "answer": "[{'answer': 'With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Predictive loglikelihood (LL) comparison, sorted by validation set performance.",
      "pages_used": [
        {
          "doc": "2004.03744.pdf",
          "page": 9,
          "matched": false,
          "score": 0.03005008347245409
        }
      ]
    },
    {
      "doc": "1909.00279.pdf",
      "question": "How do the perplexity and BLEU scores of the best-performing model, using +Anti OT and +Anti UT modifications, compare to the baseline unsupervised machine translation (UMT) model?",
      "prediction": "The perplexity and BLEU scores of the best-performing model, using +Anti OT and +Anti UT modifications, are lower than the baseline unsupervised machine translation (UMT) model.",
      "answer": "[{'answer': 'Perplexity of the best model is 65.58 compared to best baseline 105.79.\\nBleu of the best model is 6.57 compared to best baseline 5.50.', 'type': 'abstractive'}]",
      "evidence": "We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20.",
      "pages_used": [
        {
          "doc": "1909.00279.pdf",
          "page": 2,
          "matched": false,
          "score": 0.03555853740355586
        }
      ]
    },
    {
      "doc": "1811.12254.pdf",
      "question": "What is the total number of samples in the augmented dataset combining AD-specific picture descriptions with multi-task healthy speech data, as detailed in the study on heterogeneous data for Alzheimer's disease detection?",
      "prediction": "The total number of samples in the augmented dataset combining AD-specific picture descriptions with multi-task healthy speech data is 531.",
      "answer": "[{'answer': '609', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Speech datasets used. Note that HAPD, HAFP and FP only have samples from healthy subjects. Detailed description in App. 2.",
      "pages_used": [
        {
          "doc": "1811.12254.pdf",
          "page": 2,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1911.07555.pdf",
      "question": "Does the table, which presents LID accuracy results for South African languages, include the performance of a baseline model?",
      "prediction": "Yes, the table includes the performance of a baseline model.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 2: LID Accuracy Results. The models we executed ourselves are marked with *. The results that are not available from our own tests or the literature are indicated with ’—’.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.21782178217821782
        }
      ]
    },
    {
      "doc": "1911.07555.pdf",
      "question": "Did the results demonstrate that your hierarchical naive Bayesian and lexicon-based classifier achieved the highest accuracy across the South African and DSL datasets, outperforming other reported methods?",
      "prediction": "Yes, the results demonstrated that the hierarchical naive Bayesian and lexicon-based classifier achieved the highest accuracy across the South African and DSL datasets, outperforming other reported methods.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: LID Accuracy Results. The models we executed ourselves are marked with *. The results that are not available from our own tests or the literature are indicated with ’—’.",
      "pages_used": [
        {
          "doc": "2001.05970.pdf",
          "page": 6,
          "matched": false,
          "score": 0.05012531328320802
        }
      ]
    },
    {
      "doc": "1910.10288.pdf",
      "question": "How do the MOS naturalness scores, with 95% confidence intervals, compare for DCA and GMM-based attention mechanisms across the Lessac and LJ datasets, and do these results indicate a performance difference?",
      "prediction": "The MOS naturalness scores, with 95% confidence intervals, for DCA and GMM-based attention mechanisms across the Lessac and LJ datasets show a performance difference.",
      "answer": "[{'answer': 'About the same performance', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3. MOS naturalness results along with 95% confidence intervals for the Lessac and LJ datasets.",
      "pages_used": [
        {
          "doc": "1903.09722.pdf",
          "page": 8,
          "matched": false,
          "score": 0.018648018648018648
        }
      ]
    },
    {
      "doc": "1707.05236.pdf",
      "question": "What are the F0.5 scores on the FCE dataset and both annotations of CoNLL-14 when combining machine translation-based and syntactic pattern-based error generation?",
      "prediction": "The F0.5 scores on the FCE dataset and both annotations of CoNLL-14 when combining machine translation-based and syntactic pattern-based error generation are not provided in the given text.",
      "answer": "[{'answer': 'Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. ', 'type': 'abstractive'}]",
      "evidence": "The error detection results can be seen in Table TABREF4 . We use INLINEFORM0 as the main evaluation measure, which was established as the preferred measure for error correction and detection by the CoNLL-14 shared task BIBREF3 .",
      "pages_used": [
        {
          "doc": "1707.05236.pdf",
          "page": 6,
          "matched": false,
          "score": 0.019230769230769232
        }
      ]
    },
    {
      "doc": "1909.06937.pdf",
      "question": "What F1 scores did the CM-Net achieve for intent detection and slot filling on the CAIS dataset?",
      "prediction": "The CM-Net achieved F1 scores of 80.0 for intent detection and 80.0 for slot filling on the CAIS dataset.",
      "answer": "[{'answer': 'F1 scores of 86.16 on slot filling and 94.56 on intent detection', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 6: Results on our CAIS dataset, where “†” indicates our implementation of the S-LSTM.",
      "pages_used": [
        {
          "doc": "1909.08089.pdf",
          "page": 12,
          "matched": false,
          "score": 0.027944111776447105
        }
      ]
    },
    {
      "doc": "1909.06937.pdf",
      "question": "How many utterances are present in the CAIS dataset as specified in the study on Collaborative Memory Networks (CM-Net)?",
      "prediction": "The study on Collaborative Memory Networks (CM-Net) mentions that there are 32 utterances present in the CAIS dataset.",
      "answer": "[{'answer': '10,001 utterances', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Dataset statistics.",
      "pages_used": [
        {
          "doc": "1707.00110.pdf",
          "page": 8,
          "matched": false,
          "score": 0.007537688442211055
        }
      ]
    },
    {
      "doc": "1611.02550.pdf",
      "question": "What is the improvement in average precision of their best-performing model over previous results on the word discrimination task?",
      "prediction": "The improvement in average precision of their best-performing model over previous results on the word discrimination task is 0.156.",
      "answer": "[{'answer': 'Their best average precision tops previous best result by 0.202', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations.",
      "pages_used": [
        {
          "doc": "1611.02550.pdf",
          "page": 5,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1911.08673.pdf",
      "question": "What is the comparative performance of the proposed global greedy parser versus BIAF and STACKPTR in terms of both training convergence time and decoding speed?",
      "prediction": "The proposed global greedy parser has a similar training convergence time as BIAF, while STACKPTR is much slower. In terms of decoding speed, the proposed global greedy parser is the fastest, followed by BIAF, and STACKPTR is the slowest.",
      "answer": "[{'answer': 'Proposed vs best baseline:\\nDecoding: 8541 vs 8532 tokens/sec\\nTraining: 8h vs 8h', 'type': 'abstractive'}]",
      "evidence": "The comparison in Table TABREF24 shows that in terms of convergence time, our model is basically the same speed as BIAF, while STACKPTR is much slower. For decoding, our model is the fastest, followed by BIAF. STACKPTR is unexpectedly the slowest.",
      "pages_used": [
        {
          "doc": "1911.08673.pdf",
          "page": 7,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1704.08960.pdf",
      "question": "What external datasets are provided for pretraining the neural word segmentation model in this study?",
      "prediction": "The study does not mention any external datasets used for pretraining the neural word segmentation model.",
      "answer": "[{'answer': \"Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily\", 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Statistics of external data.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.22429906542056074
        }
      ]
    },
    {
      "doc": "1909.06162.pdf",
      "question": "What are the precision, recall, and F1-score values of the best-performing teams in the sentence-level classification (SLC) and fragment-level classification (FLC) tasks as reported in the MIC-CIS system's performance comparison?",
      "prediction": "The precision, recall, and F1-score values of the best-performing teams in the SLC and FLC tasks as reported in the MIC-CIS system's performance comparison are not provided in the given text.",
      "answer": "[{'answer': 'For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Comparison of our system (MIC-CIS) with top-5 participants: Scores on Test set for SLC and FLC",
      "pages_used": [
        {
          "doc": "1901.01010.pdf",
          "page": 9,
          "matched": false,
          "score": 0.03275109170305677
        }
      ]
    },
    {
      "doc": "1909.06162.pdf",
      "question": "What extracted linguistic features, combined with BERT, had the most significant influence on performance in the sentence-level classification (SLC) task in the MIC-CIS system's evaluation for fine-grained propaganda detection?",
      "prediction": "The combination of BERT and the choice-aware passage embedding had the most significant influence on performance in the SLC task in the MIC-CIS system's evaluation for fine-grained propaganda detection.",
      "answer": "[{'answer': 'Linguistic', 'type': 'abstractive'}, {'answer': 'BERT', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: SLC: Scores on Dev (internal) of Fold1 and Dev (external) using different classifiers and features.",
      "pages_used": [
        {
          "doc": "1901.02257.pdf",
          "page": 4,
          "matched": false,
          "score": 0.013368983957219251
        }
      ]
    },
    {
      "doc": "1909.06162.pdf",
      "question": "What was the F1 score difference between the best performing ensemble and the top individual classifier for Sentence and Fragment Level Classification (SLC and FLC) in the MIC-CIS system, according to the reported results?",
      "prediction": "The F1 score difference between the best performing ensemble and the top individual classifier for SLC and FLC in the MIC-CIS system was 0.1.",
      "answer": "[{'answer': 'The best ensemble topped the best single model by 0.029 in F1 score on dev (external).', 'type': 'abstractive'}, {'answer': 'They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: SLC: Scores on Dev (internal) of Fold1 and Dev (external) using different classifiers and features.",
      "pages_used": [
        {
          "doc": "2002.01207.pdf",
          "page": 22,
          "matched": false,
          "score": 0.0673076923076923
        }
      ]
    },
    {
      "doc": "1909.06162.pdf",
      "question": "Which neural architecture achieved the highest performance for sentence-level (SLC) propaganda detection on the internal Fold1 and external Dev sets?",
      "prediction": "The neural architecture that achieved the highest performance for sentence-level (SLC) propaganda detection on the internal Fold1 and external Dev sets is the one that uses structured neural representations.",
      "answer": "[{'answer': 'BERT', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: SLC: Scores on Dev (internal) of Fold1 and Dev (external) using different classifiers and features.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.20363636363636364
        }
      ]
    },
    {
      "doc": "1909.06162.pdf",
      "question": "Which participating teams achieved higher scores than the MIC-CIS system in both the SLC and FLC tasks?",
      "prediction": "The participating teams that achieved higher scores than the MIC-CIS system in both the SLC and FLC tasks are not specified in the given information.",
      "answer": "[{'answer': 'For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\\nFor FLC task: newspeak and Antiganda teams had better results.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Comparison of our system (MIC-CIS) with top-5 participants: Scores on Test set for SLC and FLC",
      "pages_used": [
        {
          "doc": "1906.10225.pdf",
          "page": 16,
          "matched": false,
          "score": 0.27884615384615385
        }
      ]
    },
    {
      "doc": "1804.07789.pdf",
      "question": "Which datasets, including any non-English ones, are discussed in the performance comparison of models for generating one-line biographical descriptions from structured tables?",
      "prediction": "The performance comparison of models for generating one-line biographical descriptions from structured tables discusses the following datasets: DBpedia, Freebase, and YAGO.",
      "answer": "[{'answer': 'English WIKIBIO, French WIKIBIO , German WIKIBIO ', 'type': 'abstractive'}, {'answer': 'WikiBio dataset,  introduce two new biography datasets, one in French and one in German', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Comparison of different models on the English WIKIBIO dataset",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.25
        }
      ]
    },
    {
      "doc": "2002.02070.pdf",
      "question": "Which classifier achieved the highest F1 micro and F1 macro scores during 4-fold cross-validation on the shuffled data set, and what explanation is given for its superior performance?",
      "prediction": "The classifier that achieved the highest F1 micro and F1 macro scores during 4-fold cross-validation on the shuffled data set is the one that uses a cycled reinforcement learning approach. The explanation for its superior performance is that it is able to learn from the feedback provided by the reinforcement learning process, which helps it to",
      "answer": "[{'answer': 'Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject', 'type': 'extractive'}, {'answer': 'Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.', 'type': 'abstractive'}]",
      "evidence": "In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.2616033755274262
        }
      ]
    },
    {
      "doc": "2002.02070.pdf",
      "question": "Based on the study, which machine learning classifiers were evaluated on classifying dealership interaction data?",
      "prediction": "The study evaluated the performance of support vector machines (SVMs) and logistic regression classifiers on classifying dealership interaction data.",
      "answer": "[{'answer': 'KNN\\nRF\\nSVM\\nMLP', 'type': 'abstractive'}, {'answer': ' K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Evaluation metrics for all classifiers.",
      "pages_used": [
        {
          "doc": "1707.08559.pdf",
          "page": 7,
          "matched": false,
          "score": 0.020486555697823303
        }
      ]
    },
    {
      "doc": "1605.08675.pdf",
      "question": "Does the RAFAEL system paper provide a comparative analysis of question-answering accuracy between DeepER, traditional NER methods (like Nerf and Liner2), and hybrid entity recognition strategies?",
      "prediction": "Yes, the RAFAEL system paper provides a comparative analysis of question-answering accuracy between DeepER, traditional NER methods (like Nerf and Liner2), and hybrid entity recognition strategies.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 3. Question answering accuracy of RAFAEL with different entity recognition strategies: quantities only (Quant), traditional NER (Nerf, Liner2 ), deep entity recognition (DeepER) and their combination (Hybrid).",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 20,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1809.09194.pdf",
      "question": "What models are evaluated in the paper's experimental results when comparing the extended Stochastic Answer Network (SAN) to baseline and state-of-the-art methods on SQuAD 2.0?",
      "prediction": "The paper evaluates the extended Stochastic Answer Network (SAN) model against baseline and state-of-the-art methods on SQuAD 2.0.",
      "answer": "[{'answer': 'SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo', 'type': 'abstractive'}, {'answer': 'BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo', 'type': 'abstractive'}]",
      "evidence": "Table TABREF21 reports comparison results in literature published .",
      "pages_used": [
        {
          "doc": "1904.07904.pdf",
          "page": 6,
          "matched": false,
          "score": 0.22330097087378642
        }
      ]
    },
    {
      "doc": "1903.09588.pdf",
      "question": "What is the F1 score improvement of the BERT-pair models, specifically BERT-pair-NLI-B for aspect category detection and BERT-pair-QA-B for aspect category polarity (in 4-way, 3-way, and binary settings) on the SemEval-2014 dataset, compared to prior state-of-the-art results?",
      "prediction": "The F1 score improvement of the BERT-pair models, specifically BERT-pair-NLI-B for aspect category detection and BERT-pair-QA-B for aspect category polarity (in 4-way, 3-way, and binary settings) on the SemEval-2014 dataset, compared to",
      "answer": "[{'answer': 'On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.\\nOn subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.', 'type': 'abstractive'}]",
      "evidence": "Results on SemEval-2014 are presented in Table TABREF35 and Table TABREF36 . We find that BERT-single has achieved better results on these two subtasks, and BERT-pair has achieved further improvements over BERT-single. The BERT-pair-NLI-B model achieves the best performance for aspect category detection. For aspect category polarity, BERT-pair-QA-B performs best on all 4-way, 3-way, and binary settings.",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.18823529411764706
        }
      ]
    },
    {
      "doc": "1909.00015.pdf",
      "question": "What are the BLEU score differences between the baseline Transformer, 1.5-entmax, and α-entmax methods on the DE-EN, JA-EN, RO-EN, and EN-DE datasets as reported in the adaptively sparse Transformer paper?",
      "prediction": "The BLEU score differences between the baseline Transformer, 1.5-entmax, and α-entmax methods on the DE-EN, JA-EN, RO-EN, and EN-DE datasets as reported in the adaptively sparse Transformer paper are not provided in the given text.",
      "answer": "[{'answer': 'On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The α-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Machine translation tokenized BLEU test results on IWSLT 2017 DE EN, KFTT JA EN, WMT 2016 RO EN and WMT 2014 EN DE, respectively.",
      "pages_used": [
        {
          "doc": "1909.00105.pdf",
          "page": 12,
          "matched": false,
          "score": 0.20100502512562815
        }
      ]
    },
    {
      "doc": "1712.05999.pdf",
      "question": "What were the meta-data features, such as Followers, Friends, and URLs, that showed significant differences in distribution between fake news and non-fake news tweets, according to the Kolmogorov-Smirnov test results?",
      "prediction": "The meta-data features that showed significant differences in distribution between fake news and non-fake news tweets, according to the Kolmogorov-Smirnov test results, were Followers, Friends, and URLs.",
      "answer": "[{'answer': 'Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: For each one of the selected features, the table shows the difference between the set of tweets containing fake news and those non containing them, and the associated p-value (applying a KolmogorovSmirnov test). The null hypothesis is that both distributions are equal (two sided). Results are ordered by decreasing p-value.",
      "pages_used": [
        {
          "doc": "1712.05999.pdf",
          "page": 4,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1910.08210.pdf",
      "question": "What are the final win rates of the proposed txt2$\\pi$, CNN, and FiLM models on the evaluation set with new environment dynamics for the simplest RTFM variant?",
      "prediction": "The final win rates of the proposed txt2$\\pi$, CNN, and FiLM models on the evaluation set with new environment dynamics for the simplest RTFM variant are 83 ± 21, 25 ± 1, and 49 ± 2, respectively.",
      "answer": "[{'answer': 'Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Final win rate on simplest variant of RTFM. The models are trained on one set of dynamics (e.g. training set) and evaluated on another set of dynamics (e.g. evaluation set). “Train” and “Eval” show final win rates on training and eval environments.",
      "pages_used": [
        {
          "doc": "1910.08210.pdf",
          "page": 6,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1810.12196.pdf",
      "question": "What tasks were the models evaluated on in ReviewQA's test set, specifically relating to aspect detection, rating predictions, and opinion analysis?",
      "prediction": "The models were evaluated on tasks such as aspect detection, rating predictions, and opinion analysis in ReviewQA's test set.",
      "answer": "[{'answer': \"ReviewQA's test set\", 'type': 'extractive'}, {'answer': 'Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review', 'type': 'abstractive'}]",
      "evidence": "Table TABREF19 displays the performance of the 4 baselines on the ReviewQA's test set. These results are the performance achieved by our own implementation of these 4 models.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 17,
          "matched": false,
          "score": 0.3017241379310345
        }
      ]
    },
    {
      "doc": "1908.07816.pdf",
      "question": "What is the test set perplexity of the proposed MEED model, and how does it compare to the next-best baseline?",
      "prediction": "The test set perplexity of the proposed MEED model is 11.4, which is lower than the next-best baseline.",
      "answer": "[{'answer': 'Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets.",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.24390243902439024
        }
      ]
    },
    {
      "doc": "1809.02286.pdf",
      "question": "Which baseline models, including tree-structured models and non-tree models, were used for comparison when evaluating the performance of the SATA Tree-LSTM on sentence classification tasks?",
      "prediction": "The baseline models used for comparison when evaluating the performance of the SATA Tree-LSTM on sentence classification tasks included tree-structured models and non-tree models.",
      "answer": "[{'answer': 'Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks', 'type': 'abstractive'}, {'answer': 'Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \\nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: The comparison of various models on different sentence classification tasks. We report the test accuracy of each model in percentage. Our SATA Tree-LSTM shows superior or competitive performance on all tasks, compared to previous treestructured models as well as other sophisticated models. ?: Latent tree-structured models. †: Models which are pre-trained with large external corpora.",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.3240223463687151
        }
      ]
    },
    {
      "doc": "1701.02877.pdf",
      "question": "Which NER datasets are used to analyze model generalization in diverse domains such as newswire, broadcast conversation, Web content, and social media?",
      "prediction": "The NER datasets used to analyze model generalization in diverse domains such as newswire, broadcast conversation, Web content, and social media are CoNLL-2003, CoNLL-2004, CoNLL-2005, and CoNLL-2007",
      "answer": "[{'answer': 'MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC', 'type': 'abstractive'}]",
      "evidence": "Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details).",
      "pages_used": [
        {
          "doc": "1911.12579.pdf",
          "page": 21,
          "matched": false,
          "score": 0.19851116625310175
        }
      ]
    },
    {
      "doc": "1910.06748.pdf",
      "question": "What are the language labels included in the Twitter corpus distribution in the paper's language identification system evaluation?",
      "prediction": "The language labels included in the Twitter corpus distribution for the paper's language identification system evaluation are Action, Object, Attribute, and Value.",
      "answer": "[{'answer': 'EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2. Twitter corpus distribution by language label.",
      "pages_used": [
        {
          "doc": "1911.01680.pdf",
          "page": 6,
          "matched": false,
          "score": 0.029079159935379646
        }
      ]
    },
    {
      "doc": "1708.09609.pdf",
      "question": "According to the study on domain adaptation for identifying products in online cybercrime marketplaces, which four cybercrime forums are analyzed to demonstrate within-forum and cross-forum performance?",
      "prediction": "The four cybercrime forums analyzed in the study are Darkode, Hack Forums, Blackhat, and Nulled.",
      "answer": "[{'answer': 'Darkode,  Hack Forums, Blackhat and Nulled.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Test set results at the NP level in within-forum and cross-forum settings for a variety of different systems. Using either Brown clusters or gazetteers gives mixed results on cross-forum performance: only one of the improvements (†) is statistically significant with p < 0.05 according to a bootstrap resampling test. Gazetteers are unavailable for Blackhat and Nulled since we have no training data for those forums.",
      "pages_used": [
        {
          "doc": "1708.09609.pdf",
          "page": 7,
          "matched": false,
          "score": 0.031753336401288544
        }
      ]
    },
    {
      "doc": "1809.06537.pdf",
      "question": "Which baseline models, including both neural classifiers and off-the-shelf RC models, are re-implemented and fine-tuned for comparison in the experiments conducted on the real-world civil case dataset in this paper?",
      "prediction": "The baseline models, including both neural classifiers and off-the-shelf RC models, that are re-implemented and fine-tuned for comparison in the experiments conducted on the real-world civil case dataset in this paper are not specified in the provided information.",
      "answer": "[{'answer': 'SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ', 'type': 'extractive'}, {'answer': 'SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard', 'type': 'extractive'}]",
      "evidence": "For comparison, we adopt and re-implement three kinds of baselines as follows:\n\nWe implement an SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4 and select the best feature set on the development set.\n\nWe implement and fine-tune a series of neural text classifiers, including attention-based method BIBREF3 and other methods we deem important. CNN BIBREF18 and GRU BIBREF27 , BIBREF21 take as input the concatenation of fact description and plea. Similarly, CNN/GRU+law refers to using the concatenation of fact description, plea and law articles as inputs.\n\nWe implement and train some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard. In our initial experiments, these models take fact description as passage and plea as query. Further, Law articles are added to the fact description as a part of the reading materials, which is a simple way to consider them as well.",
      "pages_used": [
        {
          "doc": "1703.06492.pdf",
          "page": 8,
          "matched": false,
          "score": 0.04384485666104553
        }
      ]
    },
    {
      "doc": "1910.11769.pdf",
      "question": "Based on the benchmark results of the DENS paper, which algorithm and model combination achieved the lowest average micro-F1 score across the 5-fold cross-validation for the emotion analysis task?",
      "prediction": "The algorithm and model combination that achieved the lowest average micro-F1 score across the 5-fold cross-validation for the emotion analysis task, based on the benchmark results of the DENS paper, is the one with the lowest score.",
      "answer": "[{'answer': 'Depeche + SVM', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Benchmark results (averaged 5-fold cross validation)",
      "pages_used": [
        {
          "doc": "2003.04642.pdf",
          "page": 11,
          "matched": false,
          "score": 0.014783526927138331
        }
      ]
    },
    {
      "doc": "1705.08142.pdf",
      "question": "Do Sluice networks outperform single-task models and hard parameter sharing in both in-domain and out-of-domain settings for chunking with POS tagging as an auxiliary task in the OntoNotes 5.0 experiments?",
      "prediction": "Yes, Sluice networks outperform single-task models and hard parameter sharing in both in-domain and out-of-domain settings for chunking with POS tagging as an auxiliary task in the OntoNotes 5.0 experiments.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 3: Accuracy scores on in-domain and out-of-domain test sets for chunking (main task) with POS tagging as auxiliary task for different target domains for baselines and Sluice networks. Out-of-domain results for each target domain are averages across the 6 remaining source domains. Average error reduction over single-task performance is 12.8% for in-domain; 8.9% for out-of-domain. In-domain error reduction over hard parameter sharing is 14.8%.",
      "pages_used": [
        {
          "doc": "1903.00172.pdf",
          "page": 13,
          "matched": false,
          "score": 0.30158730158730157
        }
      ]
    },
    {
      "doc": "1811.01088.pdf",
      "question": "Does the paper compare the performance of models trained with STILTs to BERT and ELMo on the same tasks from the GLUE benchmark?",
      "prediction": "Yes, the paper compares the performance of models trained with STILTs to BERT and ELMo on the same tasks from the GLUE benchmark.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 1: GLUE results with and without STILTs, fine-tuning on full training data of each target task. Bold marks the best within each section. Strikethrough indicates cases where the intermediate task is the same as the target task—we substitute the baseline result for that cell. A.Ex is the average excluding MNLI and QQP because of the overlap with intermediate tasks. See text for discussion of WNLI results. Test results on STILTs uses the supplementary training regime for each task based on the performance on the development set, corresponding to the numbers shown in Best of Each. The aggregated GLUE scores differ from the public leaderboard because we report performance on QNLIv1.",
      "pages_used": [
        {
          "doc": "1811.01088.pdf",
          "page": 4,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2002.10361.pdf",
      "question": "What is the total number of documents in the multilingual Twitter corpus for hate speech detection, covering English, Italian, Polish, Portuguese, and Spanish datasets?",
      "prediction": "The total number of documents in the multilingual Twitter corpus for hate speech detection, covering English, Italian, Polish, Portuguese, and Spanish datasets, is 10.",
      "answer": "[{'answer': 'It contains 106,350 documents', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Statistical summary of multilingual corpora across English, Italian, Polish, Portuguese and Spanish. We present number of users (Users), documents (Docs), and average tokens per document (Tokens) in the corpus, plus the label distribution (HS Ratio, percent of documents labeled positive for hate speech).",
      "pages_used": [
        {
          "doc": "1902.09393.pdf",
          "page": 11,
          "matched": false,
          "score": 0.0248565965583174
        }
      ]
    },
    {
      "doc": "1705.01265.pdf",
      "question": "Which hyperparameters, including the number of clusters, word embedding dimensions, and others, were adjusted in the systematic evaluation of clustered word embeddings across named entity recognition and sentiment classification tasks, as described in the paper?",
      "prediction": "The paper does not provide specific information about the hyperparameters that were adjusted during the systematic evaluation of clustered word embeddings across named entity recognition and sentiment classification tasks.",
      "answer": "[{'answer': 'number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding', 'type': 'abstractive'}, {'answer': 'different number of clusters, different embeddings', 'type': 'extractive'}]",
      "evidence": "We cluster the embeddings with INLINEFORM0 -Means.",
      "pages_used": [
        {
          "doc": "1705.01265.pdf",
          "page": 7,
          "matched": false,
          "score": 0.02336448598130841
        }
      ]
    },
    {
      "doc": "2001.05493.pdf",
      "question": "What specific psycholinguistic and basic linguistic features, including punctuation and emoticons, are utilized as meta-data to support aggression detection in both code-mixed and uni-lingual English texts as described in this paper's unified system?",
      "prediction": "The specific psycholinguistic and basic linguistic features, including punctuation and emoticons, utilized as meta-data to support aggression detection in both code-mixed and uni-lingual English texts are not provided in the given information.",
      "answer": "[{'answer': 'Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features', 'type': 'abstractive'}]",
      "evidence": "Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term \"NLP Features\" to represent it in the entire paper.",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.25112107623318386
        }
      ]
    },
    {
      "doc": "1909.08859.pdf",
      "question": "How does the Procedural Reasoning Network (PRN) model perform in terms of accuracy in single-task and multi-task training on the visual reasoning tasks in the RecipeQA dataset compared to the previous best models?",
      "prediction": "The PRN model outperforms the previous best models in terms of accuracy in both single-task and multi-task training on the visual reasoning tasks in the RecipeQA dataset.",
      "answer": "[{'answer': 'Average accuracy of proposed model vs best prevous result:\\nSingle-task Training: 57.57 vs 55.06\\nMulti-task Training: 50.17 vs 50.59', 'type': 'abstractive'}]",
      "evidence": "Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.35856573705179284
        }
      ]
    },
    {
      "doc": "1611.04798.pdf",
      "question": "Does the paper report results for English-to-German translation in a simulated under-resourced scenario using their proposed multilingual NMT framework?",
      "prediction": "No",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 1: Results of the English→German systems in a simulated under-resourced scenario.",
      "pages_used": [
        {
          "doc": "1603.00968.pdf",
          "page": 7,
          "matched": false,
          "score": 0.03088803088803089
        }
      ]
    },
    {
      "doc": "1610.00879.pdf",
      "question": "What is the baseline human evaluation method used to compare against the classifier performance on Dataset-H?",
      "prediction": "The baseline human evaluation method used to compare against the classifier performance on Dataset-H is the TextGen benchmark.",
      "answer": "[{'answer': 'Human evaluators', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: Performance of human evaluators and our classifiers (trained on all features), for Dataset-H as the test set",
      "pages_used": [
        {
          "doc": "1910.11235.pdf",
          "page": 6,
          "matched": false,
          "score": 0.03208556149732621
        }
      ]
    },
    {
      "doc": "1610.00879.pdf",
      "question": "What stylistic and structural features are outlined in the paper for detecting drunk-texting behavior?",
      "prediction": "The paper does not provide information on the stylistic and structural features for detecting drunk-texting behavior.",
      "answer": "[{'answer': 'LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) \\n and Sentiment Ratio', 'type': 'abstractive'}, {'answer': 'LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction",
      "pages_used": [
        {
          "doc": "1610.00879.pdf",
          "page": 6,
          "matched": false,
          "score": 0.027855153203342618
        }
      ]
    },
    {
      "doc": "1904.09678.pdf",
      "question": "What metrics are used to evaluate the performance of UniSent in comparison to manually crafted lexica in Czech, German, French, Macedonian, and Spanish?",
      "prediction": "The performance of UniSent is evaluated using BLEU, METEOR, and ROUGE metrics in comparison to manually crafted lexica in Czech, German, French, Macedonian, and Spanish.",
      "answer": "[{'answer': 'Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Comparison of manually created lexicon performance with UniSent in Czech, German, French, Macedonians, and Spanish. We report accuracy and the macro-F1 (averaged F1 over positive and negative classes). The baseline is constantly considering the majority label. The last two columns indicate the performance of UniSent after drift weighting.",
      "pages_used": [
        {
          "doc": "1810.12196.pdf",
          "page": 10,
          "matched": false,
          "score": 0.026373626373626374
        }
      ]
    },
    {
      "doc": "1912.00864.pdf",
      "question": "By how much do the BLEU-4 and ROUGE-L scores of the NAGM model outperform the best conventional models, Trans and CLSTM, on both the Oshiete-goo and nfL6 datasets, specifically in terms of exact numerical differences?",
      "prediction": "The BLEU-4 and ROUGE-L scores of the NAGM model outperform the best conventional models, Trans and CLSTM, by 1.5% and 1.0% on the Oshiete-goo dataset, and by 1.0% and 0.5% on the",
      "answer": "[{'answer': \"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. \", 'type': 'abstractive'}]",
      "evidence": "Finally, NAGM is consistently superior to the conventional attentive encoder-decoders regardless of the metric. Its ROUGE-L and BLEU-4 scores are much higher than those of CLSTM. ",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.27074235807860264
        }
      ]
    },
    {
      "doc": "1904.01608.pdf",
      "question": "What are the distinct citation intent labels identified in both the ACL-ARC and SciCite datasets, and how do they differ in terms of their application and coverage?",
      "prediction": "The citation intent labels identified in both the ACL-ARC and SciCite datasets include \"method\", \"result\", \"background\", \"related work\", and \"conclusion\". The \"method\" label refers to the techniques used in the research, \"result\" refers to the findings, \"background\" refers to the context",
      "answer": "[{'answer': 'Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Characteristics of SciCite compared with ACL-ARC dataset by Jurgens et al. (2018)",
      "pages_used": [
        {
          "doc": "1810.12196.pdf",
          "page": 10,
          "matched": false,
          "score": 0.046908315565031986
        }
      ]
    },
    {
      "doc": "1812.10479.pdf",
      "question": "According to the results of the study investigating the effects of combining news and price data for short-term stock volatility prediction, which stock market sector achieved the highest accuracy in volatility forecasting?",
      "prediction": "The document image does not provide information about the stock market sector that achieved the highest accuracy in volatility forecasting.",
      "answer": "[{'answer': 'Energy with accuracy of 0.538', 'type': 'abstractive'}, {'answer': 'Energy', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 8: Sector-level performance comparison.",
      "pages_used": [
        {
          "doc": "1909.09587.pdf",
          "page": 8,
          "matched": false,
          "score": 0.046052631578947366
        }
      ]
    },
    {
      "doc": "1901.08079.pdf",
      "question": "Does the paper focus on datasets from the medical domain, without including general-purpose English datasets?",
      "prediction": "No",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 1: Description of training and test datasets.",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.18834080717488788
        }
      ]
    },
    {
      "doc": "1906.10551.pdf",
      "question": "Which 12 authorship verification methods were evaluated and classified based on the AV characteristics proposed in Section SECREF3, particularly in relation to their performance on challenging cases like informal chat conversations and cross-topic verification?",
      "prediction": "The 12 authorship verification methods evaluated and classified based on the AV characteristics proposed in Section SECREF3 include: 1) Textual Analysis, 2) Semantic Analysis, 3) Syntactic Analysis, 4) Lexical Analysis, 5) Stylistic Analysis, 6) Th",
      "answer": "[{'answer': 'MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD', 'type': 'abstractive'}]",
      "evidence": "The methods are listed in Table TABREF33 together with their classifications regarding the AV characteristics, which we proposed in Section SECREF3 .",
      "pages_used": [
        {
          "doc": "1909.00105.pdf",
          "page": 12,
          "matched": false,
          "score": 0.2436548223350254
        }
      ]
    },
    {
      "doc": "1910.00825.pdf",
      "question": "How does SPNet's performance on ROUGE-1 and CIC compare to the best baseline method when evaluated on the MultiWOZ-based abstractive dialog summarization task?",
      "prediction": "SPNet outperforms the best baseline method on both ROUGE-1 and CIC metrics.",
      "answer": "[{'answer': 'SPNet vs best baseline:\\nROUGE-1: 90.97 vs 90.68\\nCIC: 70.45 vs 70.25', 'type': 'abstractive'}]",
      "evidence": "We show all the models' results in Table TABREF24",
      "pages_used": [
        {
          "doc": "1910.00825.pdf",
          "page": 11,
          "matched": false,
          "score": 0.014388489208633094
        }
      ]
    },
    {
      "doc": "1909.08089.pdf",
      "question": "What is the performance difference in ROUGE-1, ROUGE-L, and METEOR scores between the best proposed model and the previous top-performing model on both the arXiv and PubMed datasets in the paper \"Extractive Summarization of Long Documents by Combining Global and Local Context\"?",
      "prediction": "The paper does not provide specific information about the performance difference in ROUGE-1, ROUGE-L, and METEOR scores between the best proposed model and the previous top-performing model on both the arXiv and PubMed datasets.",
      "answer": "[{'answer': 'Best proposed model result vs best previous result:\\nArxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)\\nPubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)', 'type': 'abstractive'}, {'answer': 'On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.\\n', 'type': 'abstractive'}]",
      "evidence": "The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively.",
      "pages_used": [
        {
          "doc": "2004.03354.pdf",
          "page": 8,
          "matched": false,
          "score": 0.02122820318423048
        }
      ]
    },
    {
      "doc": "1611.00514.pdf",
      "question": "What are the reported EER, Cmindet, and Cdet values for the Intelligent Voice system on the NIST 2016 Speaker Recognition Evaluation development set?",
      "prediction": "The reported EER, Cmindet, and Cdet values for the Intelligent Voice system on the NIST 2016 Speaker Recognition Evaluation development set are 10.0%, 10.0%, and 10.0%, respectively.",
      "answer": "[{'answer': 'EER 16.04, Cmindet 0.6012, Cdet 0.6107', 'type': 'abstractive'}]",
      "evidence": "In this section we present the results obtained on the protocol provided by NIST on the development set which is supposed to mirror that of evaluation set. The results are shown in Table TABREF26 .",
      "pages_used": [
        {
          "doc": "1911.12579.pdf",
          "page": 17,
          "matched": false,
          "score": 0.19138755980861244
        }
      ]
    },
    {
      "doc": "1908.05434.pdf",
      "question": "What are the Mean Absolute Error (MAE), macro-averaged MAE (MAEM), binary classification accuracy, and weighted binary classification accuracy values for the proposed Ordinal Regression Neural Network (ORNN) compared to the previous best state-of-the-art model for the Trafficking-10K dataset, for the task of detecting sex trafficking in escort ads?",
      "prediction": "The provided information does not include the specific values for the MAE, MAEM, binary classification accuracy, and weighted binary classification accuracy for the proposed ORNN compared to the previous best state-of-the-art model for the Trafficking-10K dataset.",
      "answer": "[{'answer': 'Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)', 'type': 'abstractive'}]",
      "evidence": "We report the mean metrics from the CV in Table TABREF14 .",
      "pages_used": [
        {
          "doc": "1909.00512.pdf",
          "page": 11,
          "matched": false,
          "score": 0.03271028037383177
        }
      ]
    },
    {
      "doc": "1911.02711.pdf",
      "question": "What is the accuracy difference between using a user-written (golden) summary and an auto-generated summary in the hierarchically-refined review-centric attention model?",
      "prediction": "The accuracy difference between using a user-written (golden) summary and an auto-generated summary in the hierarchically-refined review-centric attention model is 1.",
      "answer": "[{'answer': '2.7 accuracy points', 'type': 'abstractive'}]",
      "evidence": "Table TABREF34 and Table TABREF35 show the final results. Our model outperforms all the baseline models and the top-performing models with both generated summary and golden summary, for all the three datasets.",
      "pages_used": [
        {
          "doc": "1904.07904.pdf",
          "page": 6,
          "matched": false,
          "score": 0.33793103448275863
        }
      ]
    },
    {
      "doc": "2002.06424.pdf",
      "question": "\"How many shared layers were utilized in the final model configuration for both NER and RE tasks in the ADE dataset?\"",
      "prediction": "The final model configuration for both NER and RE tasks in the ADE dataset utilized 3 shared layers.",
      "answer": "[{'answer': '1', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.",
      "pages_used": [
        {
          "doc": "1909.09484.pdf",
          "page": 10,
          "matched": false,
          "score": 0.2835249042145594
        }
      ]
    },
    {
      "doc": "2002.06424.pdf",
      "question": "How many additional task-specific RNN layers are introduced for Named Entity Recognition (NER) and Relation Extraction (RE) in both the ADE and CoNLL04 datasets in the proposed architecture?",
      "prediction": "The proposed architecture introduces two additional task-specific RNN layers for Named Entity Recognition (NER) and Relation Extraction (RE) in both the ADE and CoNLL04 datasets.",
      "answer": "[{'answer': '2 for the ADE dataset and 3 for the CoNLL04 dataset', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.2681992337164751
        }
      ]
    },
    {
      "doc": "1812.01704.pdf",
      "question": "What were the three real-world datasets, including the subverted versions, that were used to evaluate the impact of sentiment detection on toxicity detection in this study?",
      "prediction": "The three real-world datasets used in this study were Kaggle, Wikipedia, and Reddit, including their subverted versions.",
      "answer": "[{'answer': 'Kaggle\\nSubversive Kaggle\\nWikipedia\\nSubversive Wikipedia\\nReddit\\nSubversive Reddit ', 'type': 'abstractive'}]",
      "evidence": "n every experiment, we used a random 70% of messages in the corpus as training data, another 20% as validation data, and the final 10% as testing data. The average results of the three tests are given in Table TABREF40 .",
      "pages_used": [
        {
          "doc": "1812.01704.pdf",
          "page": 8,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1909.07734.pdf",
      "question": "Which team achieved the top micro-F1 score of 81.5% on the Friends dataset for spoken dialogues in the EmotionX 2019 Challenge?",
      "prediction": "The team that achieved the top micro-F1 score of 81.5% on the Friends dataset for spoken dialogues in the EmotionX 2019 Challenge was the team from the University of Edinburgh.",
      "answer": "[{'answer': 'IDEA', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 6: F-scores for Friends (%)",
      "pages_used": [
        {
          "doc": "1808.03430.pdf",
          "page": 6,
          "matched": false,
          "score": 0.022573363431151242
        }
      ]
    },
    {
      "doc": "1906.05474.pdf",
      "question": "What evaluation metrics were used to assess the performance of BERT and ELMo across the ten datasets in the BLUE benchmark?",
      "prediction": "The evaluation metrics used to assess the performance of BERT and ELMo across the ten datasets in the BLUE benchmark were not provided in the given text.",
      "answer": "[{'answer': 'BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: BLUE tasks",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.15625
        }
      ]
    },
    {
      "doc": "1906.05474.pdf",
      "question": "What are the specific tasks in the BLUE benchmark in the paper evaluating BERT and ELMo on ten biomedical and clinical datasets, used to assess model performance across various NLP challenges?",
      "prediction": "The specific tasks in the BLUE benchmark include entity recognition, relation extraction, and text classification.",
      "answer": "[{'answer': 'Inference task\\nThe aim of the inference task is to predict whether the premise sentence entails or contradicts the hypothesis sentence, Document multilabel classification\\nThe multilabel classification task predicts multiple labels from the texts., Relation extraction\\nThe aim of the relation extraction task is to predict relations and their types between the two entities mentioned in the sentences., Named entity recognition\\nThe aim of the named entity recognition task is to predict mention spans given in the text , Sentence similarity\\nThe sentence similarity task is to predict similarity scores based on sentence pairs', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: BLUE tasks",
      "pages_used": [
        {
          "doc": "1705.01214.pdf",
          "page": 31,
          "matched": false,
          "score": 0.013675213675213675
        }
      ]
    },
    {
      "doc": "1712.00991.pdf",
      "question": "Which summarization algorithms were evaluated using ROUGE unigram f1 scores for peer feedback summarization in the study on supervisor assessments and peer feedback in performance appraisals?",
      "prediction": "The study evaluated the summarization algorithms using ROUGE unigram f1 scores for peer feedback summarization.",
      "answer": "[{'answer': 'LSA, TextRank, LexRank and ILP-based summary.', 'type': 'abstractive'}, {'answer': 'LSA, TextRank, LexRank', 'type': 'abstractive'}]",
      "evidence": "Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries.",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.2724014336917563
        }
      ]
    },
    {
      "doc": "1712.00991.pdf",
      "question": "What evaluation metrics are computed to assess the multi-label classification task of matching supervisor assessments to predefined performance perspectives in this paper?",
      "prediction": "The paper does not provide specific evaluation metrics for the multi-label classification task of matching supervisor assessments to predefined performance perspectives.",
      "answer": "[{'answer': 'Precision, Recall, F-measure, accuracy', 'type': 'extractive'}, {'answer': 'Precision, Recall and F-measure', 'type': 'extractive'}]",
      "evidence": "Precision, Recall and F-measure for this multi-label classification are computed using a strategy similar to the one described in BIBREF21 . ",
      "pages_used": [
        {
          "doc": "1712.00991.pdf",
          "page": 14,
          "matched": false,
          "score": 0.01598721023181455
        }
      ]
    },
    {
      "doc": "1712.00991.pdf",
      "question": "Which specific machine learning classifiers, including both SciKit Learn implementations and custom-developed approaches, were tested using 5-fold cross-validation in the supervisor assessment text mining experiments?",
      "prediction": "The specific machine learning classifiers tested using 5-fold cross-validation in the supervisor assessment text mining experiments included Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, and a custom-developed pattern-based approach.",
      "answer": "[{'answer': 'Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based', 'type': 'abstractive'}, {'answer': 'Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach', 'type': 'abstractive'}]",
      "evidence": "Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation.",
      "pages_used": [
        {
          "doc": "1712.00991.pdf",
          "page": 5,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2003.01769.pdf",
      "question": "How much intelligibility improvement is achieved for the AECNN-T and AECNN-T-SM models when incorporating mimic loss for speech enhancement, based on experiments with CHiME-4 data?",
      "prediction": "The AECNN-T and AECNN-T-SM models achieve an intelligibility improvement of 1.5% and 2.0% respectively when incorporating mimic loss for speech enhancement, based on experiments with CHiME-4 data.",
      "answer": "[{'answer': 'Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9', 'type': 'abstractive'}]",
      "evidence": "In addition to the setting without any parallel data, we show results given parallel data. In Table TABREF10 we demonstrate that training the AECNN framework with mimic loss improves intelligibility over both the model trained with only time-domain loss (AECNN-T), as well as the model trained with both time-domain and spectral-domain losses (AECNN-T-SM). We only see a small improvement in the SI-SDR, likely due to the fact that the mimic loss technique is designed to improve the recognizablity of the results. In fact, seeing any improvement in SI-SDR at all is a surprising result.",
      "pages_used": [
        {
          "doc": "1909.09484.pdf",
          "page": 10,
          "matched": false,
          "score": 0.20382165605095542
        }
      ]
    },
    {
      "doc": "2001.06286.pdf",
      "question": "Which models are listed as having previously held the state-of-the-art results on Dutch downstream tasks that RobBERT has now surpassed?",
      "prediction": "The models listed as having previously held the state-of-the-art results on Dutch downstream tasks that RobBERT has now surpassed are BERT, RoBERTa, and ALBERT.",
      "answer": "[{'answer': 'BERTje BIBREF8, an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19., mBERT', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Results of RobBERT fine-tuned on several downstream tasks compared to the state of the art on the tasks. For accuracy, we also report the 95% confidence intervals. (Results annotated with * from van der Burgh and Verberne (2019), ** = from de Vries et al. (2019), *** from Allein et al. (2020))",
      "pages_used": [
        {
          "doc": "1904.07904.pdf",
          "page": 6,
          "matched": false,
          "score": 0.2857142857142857
        }
      ]
    },
    {
      "doc": "2001.00137.pdf",
      "question": "By how much does Stacked DeBERT outperform baseline models in F1-score in the Twitter Sentiment Classification task using Kaggle's Sentiment140 Corpus, and what is the average improvement in the intent classification task across the Chatbot NLU corpus?",
      "prediction": "Stacked DeBERT outperforms baseline models in F1-score by 6% to 8% in the Twitter Sentiment Classification task using Kaggle's Sentiment140 Corpus. The average improvement in the intent classification task across the Chatbot NLU corpus is not provided in the given text.",
      "answer": "[{'answer': 'In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average', 'type': 'abstractive'}]",
      "evidence": "Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. ",
      "pages_used": [
        {
          "doc": "2001.00137.pdf",
          "page": 17,
          "matched": false,
          "score": 0.05253940455341506
        }
      ]
    },
    {
      "doc": "1902.00330.pdf",
      "question": "What is the exact micro F1 performance improvement of the proposed model over the highest performing baseline on the ACE2004 and CWEB datasets, as well as the overall average?",
      "prediction": "The exact micro F1 performance improvement of the proposed model over the highest performing baseline on the ACE2004 and CWEB datasets, as well as the overall average, is not provided in the given text.",
      "answer": "[{'answer': 'Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1.",
      "pages_used": [
        {
          "doc": "2004.01980.pdf",
          "page": 12,
          "matched": false,
          "score": 0.026981450252951095
        }
      ]
    },
    {
      "doc": "1901.04899.pdf",
      "question": "Does the paper present a comparison of multiple approaches for utterance-level intent recognition in autonomous vehicle scenarios?",
      "prediction": "Yes, the paper presents a comparison of multiple approaches for utterance-level intent recognition in autonomous vehicle scenarios.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "The slot extraction and intent keywords extraction results are given in Table TABREF1 and Table TABREF2 , respectively. Table TABREF3 summarizes the results of various approaches we investigated for utterance-level intent understanding. Table TABREF4 shows the intent-wise detection results for our AMIE scenarios with the best performing utterance-level intent recognizer.",
      "pages_used": [
        {
          "doc": "1901.04899.pdf",
          "page": 2,
          "matched": false,
          "score": 0.09852670349907919
        }
      ]
    },
    {
      "doc": "1909.01247.pdf",
      "question": "What stylistic domains are represented in the annotated sentences from the Romanian newspaper corpus?",
      "prediction": "The annotated sentences from the Romanian newspaper corpus represent the domains of news and opinion.",
      "answer": "[{'answer': 'current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Stylistic domains and examples (bold marks annotated entities)",
      "pages_used": [
        {
          "doc": "1901.05280.pdf",
          "page": 9,
          "matched": false,
          "score": 0.03103448275862069
        }
      ]
    },
    {
      "doc": "1911.10049.pdf",
      "question": "How much larger are the training datasets used to create the new ELMo embeddings for the seven less-resourced languages, compared to those used in the CoNLL 2017 Shared Task, which sampled data from sources like Wikipedia dump and Common Crawl?",
      "prediction": "The training datasets used to create the new ELMo embeddings for the seven less-resourced languages are 10 times larger than those used in the CoNLL 2017 Shared Task.",
      "answer": "[{'answer': 'By 14 times.', 'type': 'abstractive'}, {'answer': 'up to 1.95 times larger', 'type': 'abstractive'}]",
      "evidence": "They used 20-million-words data randomly sampled from the raw text released by the CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings BIBREF8, which is a combination of Wikipedia dump and common crawl. ",
      "pages_used": [
        {
          "doc": "2002.01984.pdf",
          "page": 22,
          "matched": false,
          "score": 0.03936039360393604
        }
      ]
    },
    {
      "doc": "1911.10049.pdf",
      "question": "What is the reported improvement in F1 score between ELMo and FastText embeddings for the Estonian NER task in the study on seven less-resourced languages?",
      "prediction": "The reported improvement in F1 score between ELMo and FastText embeddings for the Estonian NER task in the study on seven less-resourced languages is 0.02.",
      "answer": "[{'answer': '5 percent points.', 'type': 'abstractive'}, {'answer': '0.05 F1', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: The results of NER evaluation task, averaged over 5 training and evaluation runs. The scores are average F1 score of the three named entity classes. The columns show FastText, ELMo, and the difference between them (∆(E − FT )).",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.21428571428571427
        }
      ]
    },
    {
      "doc": "1912.01214.pdf",
      "question": "What language pairs, including the six zero-shot translation directions using Arabic, Spanish, and Russian from the MultiUN dataset, are evaluated through cross-lingual pre-training in the experiments outlined in this paper?",
      "prediction": "The experiments in this paper evaluate cross-lingual pre-training on the following language pairs: English-Arabic, English-Spanish, English-Russian, Arabic-Spanish, Arabic-Russian, and Spanish-Russian.",
      "answer": "[{'answer': 'De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru', 'type': 'abstractive'}, {'answer': 'French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation', 'type': 'extractive'}]",
      "evidence": "For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. ",
      "pages_used": [
        {
          "doc": "1701.00185.pdf",
          "page": 26,
          "matched": false,
          "score": 0.037142857142857144
        }
      ]
    },
    {
      "doc": "1712.03547.pdf",
      "question": "How does the performance of the proposed KG embedding method using entity co-occurrence statistics compare to baseline methods in terms of maintaining comparable KG task performance while significantly improving interpretability?",
      "prediction": "The proposed KG embedding method using entity co-occurrence statistics outperforms baseline methods in terms of maintaining comparable KG task performance while significantly improving interpretability.",
      "answer": "[{'answer': 'Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Results on test data. The proposed method significantly improves interpretability while maintaining comparable performance on KG tasks (Section 4.3).",
      "pages_used": [
        {
          "doc": "1906.10225.pdf",
          "page": 16,
          "matched": false,
          "score": 0.2773722627737226
        }
      ]
    },
    {
      "doc": "1603.00968.pdf",
      "question": "What baseline models are compared to MGNC-CNN in the experiments involving the use of word2vec, Glove, and syntactic embeddings?",
      "prediction": "The baseline models compared to MGNC-CNN in the experiments involving the use of word2vec, Glove, and syntactic embeddings are MGNC and CNN.",
      "answer": "[{'answer': 'MC-CNN\\nMVCNN\\nCNN', 'type': 'abstractive'}]",
      "evidence": "We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 .",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.27705627705627706
        }
      ]
    },
    {
      "doc": "1603.00968.pdf",
      "question": "By how much does the MGNC-CNN model outperform the baseline models across different datasets like Subj, SST-1, SST-2, TREC, and Irony?",
      "prediction": "The MGNC-CNN model outperforms the baseline models by 1.53% across different datasets like Subj, SST-1, SST-2, TREC, and Irony.",
      "answer": "[{'answer': 'In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. \\nIn case of Irony the difference is about 2.0. \\n', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Results mean (min, max) achieved with each method. w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. Note that we experiment with using two and three sets of embeddings jointly, e.g., w2v+Syn+Glv indicates that we use all three of these.",
      "pages_used": [
        {
          "doc": "1603.00968.pdf",
          "page": 4,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1603.00968.pdf",
      "question": "What baseline and alternative architectures, including those that utilize single and concatenated word embeddings, are compared to MGNC-CNN according to the study?",
      "prediction": "The study compares MGNC-CNN to baseline architectures such as CNN and LSTM, as well as alternative architectures that utilize single and concatenated word embeddings.",
      "answer": "[{'answer': 'standard CNN, C-CNN, MVCNN ', 'type': 'extractive'}]",
      "evidence": "We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. ",
      "pages_used": [
        {
          "doc": "1901.05280.pdf",
          "page": 9,
          "matched": false,
          "score": 0.03316749585406302
        }
      ]
    },
    {
      "doc": "1905.12260.pdf",
      "question": "How does the exclusion of pixel data impact the crosslingual semantic similarity scores for the ImageVec model compared to previous methods across all six subtasks?",
      "prediction": "The exclusion of pixel data leads to significantly lower crosslingual semantic similarity scores for the ImageVec model compared to previous methods across all six subtasks.",
      "answer": "[{'answer': 'performance is significantly degraded without pixel data', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Crosslingual semantic similarity scores (Spearman’s ρ) across six subtasks for ImageVec (our method) and previous work. Coverage is in brackets. The last column indicates the combined score across all subtasks. Best scores on each subtask are bolded.",
      "pages_used": [
        {
          "doc": "1905.12260.pdf",
          "page": 7,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2002.06675.pdf",
      "question": "What is the total duration of transcribed speech data available for the eight speakers in the speech corpus used to develop the Ainu language ASR system?",
      "prediction": "The total duration of transcribed speech data available for the eight speakers in the speech corpus used to develop the Ainu language ASR system is 10 hours.",
      "answer": "[{'answer': 'Transcribed data is available for duration of 38h 54m 38s for 8 speakers.', 'type': 'abstractive'}]",
      "evidence": "The corpus we have prepared for ASR in this study is composed of text and speech. Table 1 shows the number of episodes and the total speech duration for each speaker.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.26720647773279355
        }
      ]
    },
    {
      "doc": "1912.06670.pdf",
      "question": "Based on the Common Voice dataset, is the distribution of audio hours across the released languages described as balanced?",
      "prediction": "No",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "The data presented in Table (TABREF12) shows the currently available data. Each of the released languages is available for individual download as a compressed directory from the Mozilla Common Voice website. ",
      "pages_used": [
        {
          "doc": "1810.12885.pdf",
          "page": 14,
          "matched": false,
          "score": 0.19540229885057472
        }
      ]
    },
    {
      "doc": "2003.03106.pdf",
      "question": "What are the F1-scores of BERT for detection, relaxed classification, and strict classification tasks in the anonymisation of Spanish clinical text, and how do they compare to HUBES-PHI and MEDDOCAN as reported in the results?",
      "prediction": "The F1-scores for BERT in the anonymisation of Spanish clinical text are not provided in the given information. However, it is mentioned that BERT outperforms HUBES-PHI and MEDDOCAN in terms of F1-scores for detection, relaxed classification, and strict classification tasks.",
      "answer": "[{'answer': 'F1 scores are:\\nHUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)\\nMedoccan: Detection(0.972), Classification (0.967)', 'type': 'abstractive'}, {'answer': 'BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table ', 'type': 'extractive'}]",
      "evidence": "To finish with this experiment set, Table also shows the strict classification precision, recall and F1-score for the compared systems.",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.19463087248322147
        }
      ]
    },
    {
      "doc": "2003.12738.pdf",
      "question": "Which variant of the Variational Transformer (GVT with global latent variables or SVT with sequential latent variables) achieves better performance in terms of perplexity, diversity, embeddings similarity, and human evaluation based on the experimental results presented in the paper?",
      "prediction": "The Variational Transformer with global latent variables (GVT) achieves better performance in terms of perplexity, diversity, embeddings similarity, and human evaluation based on the experimental results presented in the paper.",
      "answer": "[{'answer': 'PPL: SVT\\nDiversity: GVT\\nEmbeddings Similarity: SVT\\nHuman Evaluation: SVT', 'type': 'abstractive'}]",
      "evidence": "Compare to baseline models, the GVT achieves relatively lower reconstruction PPL, which suggests that the global latent variable contains rich latent information (e.g., topic) for response generation. Meanwhile, the sequential latent variables of the SVT encode fine-grained latent information and further improve the reconstruction PPL.",
      "pages_used": [
        {
          "doc": "1701.06538.pdf",
          "page": 20,
          "matched": false,
          "score": 0.06467661691542288
        }
      ]
    },
    {
      "doc": "1902.09393.pdf",
      "question": "Which baseline models are referenced on the ListOps dataset as comparisons, including those from Nangia and Bowman (2018), to evaluate the new recursive model's performance?",
      "prediction": "The baseline models referenced on the ListOps dataset for comparison with the new recursive model's performance include those from Nangia and Bowman (2018), as well as the models from the authors of the paper.",
      "answer": "[{'answer': 'The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Accuracy on the ListOps dataset. All models have 128 dimensions. Results for models with * are taken from Nangia and Bowman (2018).",
      "pages_used": [
        {
          "doc": "2002.01207.pdf",
          "page": 22,
          "matched": false,
          "score": 0.096045197740113
        }
      ]
    },
    {
      "doc": "1810.05241.pdf",
      "question": "\"According to 'One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases,' what is the approximate size of the StackEx dataset, in terms of the number of questions, used for keyphrase generation?\"",
      "prediction": "The StackEx dataset used for keyphrase generation contains approximately 10,000 questions.",
      "answer": "[{'answer': 'around 330k questions', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Statistics of datasets we use in this work. Avg# and Var# indicate the mean and variance of numbers of target phrases per data point, respectively.",
      "pages_used": [
        {
          "doc": "2001.06888.pdf",
          "page": 13,
          "matched": false,
          "score": 0.32085561497326204
        }
      ]
    },
    {
      "doc": "1810.05241.pdf",
      "question": "What baseline models are listed for comparison against the proposed recurrent generative model on the present-keyphrase portion of the KP20k dataset?",
      "prediction": "The baseline models listed for comparison against the proposed recurrent generative model on the present-keyphrase portion of the KP20k dataset are CatSeq and CatSeqD.",
      "answer": "[{'answer': 'CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)', 'type': 'abstractive'}, {'answer': 'CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*', 'type': 'extractive'}]",
      "evidence": "We report our model's performance on the present-keyphrase portion of the KP20k dataset in Table TABREF35 .",
      "pages_used": [
        {
          "doc": "1810.05241.pdf",
          "page": 15,
          "matched": false,
          "score": 0.022819885900570498
        }
      ]
    },
    {
      "doc": "2003.11645.pdf",
      "question": "What hyperparameters are evaluated in the Word2Vec study, and how do they impact tasks such as Named Entity Recognition (NER), Sentiment Analysis (SA), and vector quality?",
      "prediction": "The hyperparameters evaluated in the Word2Vec study include the dimensionality of the word vectors, the window size, and the minimum frequency threshold. These hyperparameters impact tasks such as Named Entity Recognition (NER), Sentiment Analysis (SA), and vector quality by affecting the model's ability to capture meaningful relationships between words and",
      "answer": "[{'answer': 'Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.', 'type': 'abstractive'}, {'answer': 'Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Hyper-parameter choices",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 17,
          "matched": false,
          "score": 0.3076923076923077
        }
      ]
    },
    {
      "doc": "2003.11645.pdf",
      "question": "Does the study on Word2Vec hyper-parameters present choices relevant to both the skip-gram and CBOW models?",
      "prediction": "Yes, the study on Word2Vec hyper-parameters presents choices relevant to both the skip-gram and CBOW models.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 1: Hyper-parameter choices",
      "pages_used": [
        {
          "doc": "1811.12254.pdf",
          "page": 8,
          "matched": false,
          "score": 0.0195971692977681
        }
      ]
    },
    {
      "doc": "1704.00939.pdf",
      "question": "How did word-representations and basic pre-processing influence the final test set performance in the SemEval-2017 Task 5 (subtask 2) according to the reported results?",
      "prediction": "The reported results did not provide specific information on how word-representations and basic pre-processing influenced the final test set performance in the SemEval-2017 Task 5 (subtask 2).",
      "answer": "[{'answer': 'beneficial impact of word-representations and basic pre-processing', 'type': 'extractive'}]",
      "evidence": "In Table TABREF17 we show model's performances on the challenge training data, in a 5-fold cross-validation setting.\n\nFurther, the final performances obtained with our approach on the challenge test set are reported in Table TABREF18 . Consistently with the cross-validation performances shown earlier, we observe the beneficial impact of word-representations and basic pre-processing.",
      "pages_used": [
        {
          "doc": "1906.10225.pdf",
          "page": 16,
          "matched": false,
          "score": 0.22406639004149378
        }
      ]
    },
    {
      "doc": "1909.13375.pdf",
      "question": "What is the difference in Exact Match (EM) and F1 metrics between the LARGE-SQUAD and MTMSNlarge models for single-span, number-type, and date-type questions from the DROP development set?",
      "prediction": "The difference in Exact Match (EM) and F1 metrics between the LARGE-SQUAD and MTMSNlarge models for single-span, number-type, and date-type questions from the DROP development set is not provided in the given information.",
      "answer": "[{'answer': 'For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \\nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2. Performance of different models on DROP’s development set in terms of Exact Match (EM) and F1.",
      "pages_used": [
        {
          "doc": "1906.10225.pdf",
          "page": 16,
          "matched": false,
          "score": 0.29310344827586204
        }
      ]
    },
    {
      "doc": "1909.13375.pdf",
      "question": "What are the precise EM and F1 scores achieved by the proposed multi-span sequence tagging model on DROP's development and test sets?",
      "prediction": "The precise EM and F1 scores achieved by the proposed multi-span sequence tagging model on DROP's development and test sets are not provided in the given text.",
      "answer": "[{'answer': 'The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev', 'type': 'abstractive'}]",
      "evidence": "Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions.",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.2236024844720497
        }
      ]
    },
    {
      "doc": "1610.07809.pdf",
      "question": "What are the three incremental levels of document preprocessing implemented in the study to evaluate the robustness of keyphrase extraction models over noisy scientific articles, as described in the SemEval-2010 dataset analysis?",
      "prediction": "The three incremental levels of document preprocessing implemented in the study are: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document.",
      "answer": "[{'answer': 'raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document', 'type': 'extractive'}, {'answer': 'Level 1, Level 2 and Level 3.', 'type': 'abstractive'}]",
      "evidence": "Three incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document. In doing so, we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text.",
      "pages_used": [
        {
          "doc": "1610.07809.pdf",
          "page": 1,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1909.09484.pdf",
      "question": "What improvements in the BPRA, APRA, and BLEU metrics does the proposed generative dialog policy (GDP) model achieve over the baseline models on the DSTC2 and Maluuba task-oriented dialogue datasets?",
      "prediction": "The proposed generative dialog policy (GDP) model achieves improvements in the BPRA, APRA, and BLEU metrics over the baseline models on the DSTC2 and Maluuba task-oriented dialogue datasets.",
      "answer": "[{'answer': 'most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)\\nGDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)\\nGDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: The performance of baselines and proposed model on DSTC2 and Maluuba dataset. T imefull is the time spent on training the whole model, T imeDP is the time spent on training the dialogue policy maker.",
      "pages_used": [
        {
          "doc": "1909.09484.pdf",
          "page": 10,
          "matched": false,
          "score": 0.30935251798561153
        }
      ]
    },
    {
      "doc": "1912.13109.pdf",
      "question": "How many rows of labeled text messages are present in the dataset used to train the classifier for categorizing Hinglish social media content into abusive, hate-inducing, and not offensive categories?",
      "prediction": "The dataset used to train the classifier for categorizing Hinglish social media content into abusive, hate-inducing, and not offensive categories contains 3189 rows of labeled text messages.",
      "answer": "[{'answer': '3189 rows of text messages', 'type': 'extractive'}, {'answer': 'Resulting dataset was 7934 messages for train and 700 messages for test.', 'type': 'abstractive'}]",
      "evidence": "Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295.",
      "pages_used": [
        {
          "doc": "1912.13109.pdf",
          "page": 2,
          "matched": false,
          "score": 0.035416666666666666
        }
      ]
    },
    {
      "doc": "1909.01383.pdf",
      "question": "What is the average BLEU score improvement reported when using the DocRepair model for refining 4-sentence fragment translations in the English-Russian translation task?",
      "prediction": "The average BLEU score improvement reported when using the DocRepair model for refining 4-sentence fragment translations in the English-Russian translation task is 1.5.",
      "answer": "[{'answer': 'On average 0.64 ', 'type': 'abstractive'}]",
      "evidence": "The BLEU scores are provided in Table TABREF24 (we evaluate translations of 4-sentence fragments).",
      "pages_used": [
        {
          "doc": "1906.03538.pdf",
          "page": 16,
          "matched": false,
          "score": 0.2597402597402597
        }
      ]
    },
    {
      "doc": "1908.06264.pdf",
      "question": "What baseline models are listed in the validation results for the Friends dataset used to compare with Emotion BERT in the task of dialogue emotion prediction?",
      "prediction": "The baseline models listed in the validation results for the Friends dataset used to compare with Emotion BERT in the task of dialogue emotion prediction are LSTM-DQN, LSTM-DQN+ADM, DRRN, and Go-Explore.",
      "answer": "[{'answer': 'BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN', 'type': 'abstractive'}, {'answer': 'bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe', 'type': 'extractive'}]",
      "evidence": "The experiment results of validation on Friends are shown in Table TABREF19. ",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 17,
          "matched": false,
          "score": 0.2803738317757009
        }
      ]
    },
    {
      "doc": "1908.06264.pdf",
      "question": "In the EmotionX-IDEA study, which BERT model configurations were used for validation on the Friends dataset?",
      "prediction": "The BERT model configurations used for validation on the Friends dataset in the EmotionX-IDEA study were BERT-base and BERT-large.",
      "answer": "[{'answer': 'BERT-base, BERT-large, BERT-uncased, BERT-cased', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 6: Validation Results (Friends)",
      "pages_used": [
        {
          "doc": "1901.01010.pdf",
          "page": 9,
          "matched": false,
          "score": 0.021151586368977675
        }
      ]
    },
    {
      "doc": "1910.13215.pdf",
      "question": "What evaluation metric was calculated on the validation set to select the best model for inference in the cascaded multimodal speech translation system developed for the IWSLT 2019 evaluation?",
      "prediction": "The evaluation metric calculated on the validation set to select the best model for inference in the cascaded multimodal speech translation system developed for the IWSLT 2019 evaluation was BLEU.",
      "answer": "[{'answer': 'BLEU scores', 'type': 'abstractive'}]",
      "evidence": "After the training finishes, we evaluate all the checkpoints on the validation set and compute the real BIBREF30 scores, based on which we select the best model for inference on the test set.",
      "pages_used": [
        {
          "doc": "2002.00652.pdf",
          "page": 8,
          "matched": false,
          "score": 0.03167898627243928
        }
      ]
    },
    {
      "doc": "1910.07481.pdf",
      "question": "According to the results, which translation direction—English-French or French-English—demonstrated better performance based on both BLEU and TER metrics?",
      "prediction": "The English-French translation direction demonstrated better performance based on both BLEU and TER metrics.",
      "answer": "[{'answer': 'French-English', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: Results obtained for the English-French and French-English translation tasks, scored on three test sets using BLEU and TER metrics. p-values are denoted by * and correspond to the following values: ∗< .05, ∗∗< .01, ∗∗∗< .001.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.3717948717948718
        }
      ]
    },
    {
      "doc": "1809.00540.pdf",
      "question": "Does the clustering algorithm involve the use of graphical models in comparison to CluStream for multilingual datasets?",
      "prediction": "No, the clustering algorithm does not involve the use of graphical models in comparison to CluStream for multilingual datasets.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 2: Clustering results on the labeled dataset. We compare our algorithm (with and without timestamps) with the online micro-clustering routine of Aggarwal and Yu (2006) (denoted by CluStream). The F1 values are for the precision (P) and recall (R) in the following columns. See Table 3 for a legend of the different models. Best result for each language is in bold.",
      "pages_used": [
        {
          "doc": "1809.00540.pdf",
          "page": 6,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1809.00540.pdf",
      "question": "What evaluation metrics are used to compare the clustering performance, including precision, recall, and F1 scores, of the proposed algorithm against CluStream?",
      "prediction": "The evaluation metrics used to compare the clustering performance include precision (P), recall (R), and F1 scores.",
      "answer": "[{'answer': 'F1, precision, recall, accuracy', 'type': 'abstractive'}, {'answer': 'Precision, recall, F1, accuracy', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Clustering results on the labeled dataset. We compare our algorithm (with and without timestamps) with the online micro-clustering routine of Aggarwal and Yu (2006) (denoted by CluStream). The F1 values are for the precision (P) and recall (R) in the following columns. See Table 3 for a legend of the different models. Best result for each language is in bold.",
      "pages_used": [
        {
          "doc": "1809.00540.pdf",
          "page": 6,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1609.00559.pdf",
      "question": "What word embedding methods are compared against the proposed vector-res and vector-faith methods in this paper's evaluation of improving correlation with human judgments using semantic similarity integration?",
      "prediction": "The paper compares the proposed vector-res and vector-faith methods against the word embedding methods of ELMo and FastText.",
      "answer": "[{'answer': 'Skip–gram, CBOW', 'type': 'extractive'}, {'answer': 'integrated vector-res, vector-faith, Skip–gram, CBOW', 'type': 'extractive'}]",
      "evidence": "chiu2016how evaluated both the the Skip–gram and CBOW models over the PMC corpus and PubMed.",
      "pages_used": [
        {
          "doc": "1903.09722.pdf",
          "page": 8,
          "matched": false,
          "score": 0.027842227378190254
        }
      ]
    },
    {
      "doc": "2004.03788.pdf",
      "question": "What is the percentage improvement of the GTRS model over both SVM and Pawlak rough set models in detecting satirical news tweets, as presented in the experimental results?",
      "prediction": "The GTRS model achieved an accuracy of 82.71%, which is a 5.71% improvement over the SVM model's accuracy of 78%, and a 7.71% improvement over the Pawlak rough set model's accuracy of 75%.",
      "answer": "[{'answer': 'Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 7. Experimental results",
      "pages_used": [
        {
          "doc": "2004.03788.pdf",
          "page": 10,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1804.08050.pdf",
      "question": "Based on the experimental results, by how many percentage points does the proposed multi-head decoder method improve the Character Error Rate compared to the best-performing multi-head attention model?",
      "prediction": "The proposed multi-head decoder method improves the Character Error Rate by 1.5 percentage points compared to the best-performing multi-head attention model.",
      "answer": "[{'answer': 'Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Experimental results.",
      "pages_used": [
        {
          "doc": "1908.11546.pdf",
          "page": 7,
          "matched": false,
          "score": 0.012578616352201259
        }
      ]
    },
    {
      "doc": "1804.08050.pdf",
      "question": "In the experimental conditions described, what is the total size, in utterances, of the Corpus of Spontaneous Japanese used for evaluating the multi-head decoder model?",
      "prediction": "The total size of the Corpus of Spontaneous Japanese used for evaluating the multi-head decoder model is 100 utterances.",
      "answer": "[{'answer': '449050', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Experimental conditions.",
      "pages_used": [
        {
          "doc": "1909.00105.pdf",
          "page": 12,
          "matched": false,
          "score": 0.35051546391752575
        }
      ]
    },
    {
      "doc": "1910.11204.pdf",
      "question": "What were the F1 scores of the RelAwe with DepPath&RelPath model in the closed and open settings for the Chinese SRL task on the CoNLL-2009 dataset?",
      "prediction": "The F1 scores of the RelAwe with DepPath&RelPath model in the closed and open settings for the Chinese SRL task on the CoNLL-2009 dataset were 82.0 and 78.0, respectively.",
      "answer": "[{'answer': 'In closed setting 84.22 F1 and in open 87.35 F1.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF46 shows that our Open model achieves more than 3 points of f1-score than the state-of-the-art result, and RelAwe with DepPath&RelPath achieves the best in both Closed and Open settings.",
      "pages_used": [
        {
          "doc": "1906.10225.pdf",
          "page": 16,
          "matched": false,
          "score": 0.2413793103448276
        }
      ]
    },
    {
      "doc": "1910.11204.pdf",
      "question": "Which two strong baseline methods are compared to the syntax-enhanced self-attention model in the SRL results for the Chinese CoNLL-2009 dataset?",
      "prediction": "The two strong baseline methods compared to the syntax-enhanced self-attention model in the SRL results for the Chinese CoNLL-2009 dataset are the GRU-based encoder-decoder approach with attention and the drawing and recognizing Chinese characters with recurrent neural network.",
      "answer": "[{'answer': 'Marcheggiani and Titov (2017) and Cai et al. (2018)', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 7: SRL results on the Chinese test set. We choose the best settings for each configuration of our model.",
      "pages_used": [
        {
          "doc": "1902.10525.pdf",
          "page": 14,
          "matched": false,
          "score": 0.023310023310023312
        }
      ]
    },
    {
      "doc": "2003.07723.pdf",
      "question": "Does the paper include macro F1 for the BERT model’s performance on the test set for classifying emotions in the annotated poetry dataset?",
      "prediction": "No",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels.",
      "pages_used": [
        {
          "doc": "1806.07711.pdf",
          "page": 9,
          "matched": false,
          "score": 0.03338391502276176
        }
      ]
    },
    {
      "doc": "1710.09340.pdf",
      "question": "What are the UAS (Unlabeled Attachment Score) and LAS (Labeled Attachment Score) achieved by the proposed non-local transition-based parser compared to the strongest state-of-the-art greedy and overall parsers on the PT-SD dataset?",
      "prediction": "The proposed non-local transition-based parser achieved UAS and LAS scores of 80.5 and 75.5, respectively, on the PT-SD dataset.",
      "answer": "[{'answer': 'Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF12 compares our novel system with other state-of-the-art transition-based dependency parsers on the PT-SD. Greedy parsers are in the first block, beam-search and dynamic programming parsers in the second block. The third block shows the best result on this benchmark, obtained with constituent parsing with generative re-ranking and conversion to dependencies. Despite being the only non-projective parser tested on a practically projective dataset, our parser achieves the highest score among greedy transition-based models (even above those trained with a dynamic oracle).\n\nWe even slightly outperform the arc-swift system of Qi2017, with the same model architecture, implementation and training setup, but based on the projective arc-eager transition-based parser instead.",
      "pages_used": [
        {
          "doc": "1812.10479.pdf",
          "page": 35,
          "matched": false,
          "score": 0.03773584905660377
        }
      ]
    },
    {
      "doc": "1809.09795.pdf",
      "question": "What are the 7 benchmark datasets that were used to evaluate sarcasm and irony detection across Tweets, Reddit posts, and online debates in the study on deep contextualized word representations?",
      "prediction": "The 7 benchmark datasets used in the study on deep contextualized word representations for evaluating sarcasm and irony detection across different platforms are: SARC, IY, SARC Reddit, IY Reddit, SARC Debates, IY Debates, and SARC Reddit Debates.",
      "answer": "[{'answer': 'SemEval 2018 Task 3, BIBREF20, BIBREF4, SARC 2.0, SARC 2.0 pol, Sarcasm Corpus V1 (SC-V1), Sarcasm Corpus V2 (SC-V2)', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Benchmark datasets: Tweets, Reddit posts and online debates for sarcasm and irony detection.",
      "pages_used": [
        {
          "doc": "1711.02013.pdf",
          "page": 14,
          "matched": false,
          "score": 0.03323262839879154
        }
      ]
    },
    {
      "doc": "2002.11402.pdf",
      "question": "What are the differences in recall scores between the proposed BERT-Multilingual, Bi-GRU, CRF-based model and traditional NER systems such as Stanford, SpaCy, and Flair when evaluated using both Traditional NERs and Wikipedia titles as references?",
      "prediction": "The differences in recall scores between the proposed BERT-Multilingual, Bi-GRU, CRF-based model and traditional NER systems such as Stanford, SpaCy, and Flair when evaluated using both Traditional NERs and Wikipedia titles as references are not provided in the given text.",
      "answer": "[{'answer': 'Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference",
      "pages_used": [
        {
          "doc": "1909.07734.pdf",
          "page": 6,
          "matched": false,
          "score": 0.008820286659316428
        }
      ]
    },
    {
      "doc": "2002.11402.pdf",
      "question": "What are the F1 score and recall values reported when comparing your BERT-CRF model's performance on detecting case-less n-grams against traditional Named Entity Recognition systems?",
      "prediction": "The F1 score and recall values reported when comparing the BERT-CRF model's performance on detecting case-less n-grams against traditional Named Entity Recognition systems are not provided in the given text.",
      "answer": "[{'answer': 'F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference",
      "pages_used": [
        {
          "doc": "1903.09722.pdf",
          "page": 8,
          "matched": false,
          "score": 0.015584415584415584
        }
      ]
    },
    {
      "doc": "1905.10810.pdf",
      "question": "What is the accuracy difference between the interpretable system leveraging edit distance and cosine similarity with semantic vectors, and the LSTM model enhanced with ELMo embeddings for spelling correction on the PlEWi dataset?",
      "prediction": "The accuracy difference between the interpretable system leveraging edit distance and cosine similarity with semantic vectors, and the LSTM model enhanced with ELMo embeddings for spelling correction on the PlEWi dataset is 0.05.",
      "answer": "[{'answer': 'Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.', 'type': 'abstractive'}]",
      "evidence": "The experimental results are presented in Table TABREF4 .",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 16,
          "matched": false,
          "score": 0.24096385542168675
        }
      ]
    },
    {
      "doc": "1807.07961.pdf",
      "question": "Are the experimental datasets used in the paper's proposed emoji-based sentiment analysis exclusively from English-language tweets?",
      "prediction": "Yes, the experimental datasets used in the paper's proposed emoji-based sentiment analysis are exclusively from English-language tweets.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 1: Tweet examples with emojis. The sentiment ground truth is given in the second column. The examples show that inconsistent sentiments exist between emojis and texts.",
      "pages_used": [
        {
          "doc": "1901.05280.pdf",
          "page": 9,
          "matched": false,
          "score": 0.024390243902439025
        }
      ]
    },
    {
      "doc": "1809.10644.pdf",
      "question": "What are the F1 scores for the SR, HATE, and HAR datasets achieved by the proposed model, as reported in the paper on predictive embeddings for hate speech detection?",
      "prediction": "The paper does not provide the F1 scores for the SR, HATE, and HAR datasets achieved by the proposed model.",
      "answer": "[{'answer': 'Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: F1 Results3",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.16
        }
      ]
    },
    {
      "doc": "1607.06025.pdf",
      "question": "What is the highest classification accuracy reported for a model trained on the combined original and best generated dataset, with the generated datasets filtered at a threshold of 0.6?",
      "prediction": "82.0",
      "answer": "[{'answer': '82.0%', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: The performance of classifiers trained on the original and generated datasets. The classifiers were tested on original test set. The generated datasets were generated by the models from Table 3. The generated datasets were filtered with threshold 0.6.",
      "pages_used": [
        {
          "doc": "1607.06025.pdf",
          "page": 15,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1910.14497.pdf",
      "question": "What are the three bias evaluation metrics that show reduction when applying the probabilistic bias mitigation method, as indicated by the cosine distance measurements in the experimental results?",
      "prediction": "The three bias evaluation metrics that show reduction when applying the probabilistic bias mitigation method, as indicated by the cosine distance measurements in the experimental results, are not provided in the given text.",
      "answer": "[{'answer': 'RIPA, Neighborhood Metric, WEAT', 'type': 'abstractive'}]",
      "evidence": "Geometric bias mitigation uses the cosine distances between words to both measure and remove gender bias BIBREF0.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.21739130434782608
        }
      ]
    },
    {
      "doc": "1809.02279.pdf",
      "question": "What is the state-of-the-art accuracy achieved by the proposed CAS-LSTM architecture on the SNLI benchmark, and how does it leverage the fusion of hidden and memory cell states to outperform existing models?",
      "prediction": "The state-of-the-art accuracy achieved by the proposed CAS-LSTM architecture on the SNLI benchmark is not provided in the given text. However, it is mentioned that the CAS-LSTM architecture leverages the fusion of hidden and memory cell states to outperform existing models.",
      "answer": "[{'answer': 'In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5', 'type': 'extractive'}, {'answer': 'accuracy of 87.0%', 'type': 'extractive'}]",
      "evidence": "In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0% with relatively fewer parameters.",
      "pages_used": [
        {
          "doc": "1809.02279.pdf",
          "page": 16,
          "matched": false,
          "score": 0.016129032258064516
        }
      ]
    },
    {
      "doc": "1701.05574.pdf",
      "question": "What system achieved the best F-score in sarcasm detection after incorporating cognitive features from eye-tracking data with traditional features?",
      "prediction": "The system that achieved the best F-score in sarcasm detection after incorporating cognitive features from eye-tracking data with traditional features is the Multi Instance Logistic Regression (MILR) system.",
      "answer": "[{'answer': 'Gaze Sarcasm using Multi Instance Logistic Regression.', 'type': 'abstractive'}, {'answer': 'the MILR classifier', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Classification results for different feature combinations. P→ Precision, R→Recall, F→ F˙score, Kappa→ Kappa statistics show agreement with the gold labels. Subscripts 1 and -1 correspond to sarcasm and non-sarcasm classes respectively.",
      "pages_used": [
        {
          "doc": "1701.05574.pdf",
          "page": 7,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1701.05574.pdf",
      "question": "What specific eye-movement-based cognitive features, including fixation and saccade metrics, are listed as contributing to the sarcasm detection model in this paper?",
      "prediction": "The specific eye-movement-based cognitive features, including fixation and saccade metrics, that are listed as contributing to the sarcasm detection model in this paper are not provided in the given text.",
      "answer": "[{'answer': 'Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half\\nto first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze\\ngraph (ED),  Fixation Duration at Left/Source\\n(F1H, F1S),  Fixation Duration at Right/Target\\n(F2H, F2S),  Forward Saccade Word Count of\\nSource (PSH, PSS),  Forward SaccadeWord Count of Destination\\n(PSDH, PSDS), Regressive Saccade Word Count of\\nSource (RSH, RSS),  Regressive Saccade Word Count of\\nDestination (RSDH, RSDS)', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: The complete set of features used in our system.",
      "pages_used": [
        {
          "doc": "1701.05574.pdf",
          "page": 11,
          "matched": false,
          "score": 0.022675736961451247
        }
      ]
    },
    {
      "doc": "2001.08868.pdf",
      "question": "How does Go-Explore's phase 1 exploration compare to DQN++ and DRQN++ in terms of interaction efficiency and trajectory optimality when solving the CoinCollector text-based game?",
      "prediction": "Go-Explore's phase 1 exploration is more efficient in terms of interaction and trajectory optimality compared to DQN++ and DRQN++ when solving the CoinCollector text-based game.",
      "answer": "[{'answer': ' On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model', 'type': 'extractive'}, {'answer': 'On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.\\nOn Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.', 'type': 'abstractive'}]",
      "evidence": "Results ::: CoinCollector\nIn this setting, we compare the number of actions played in the environment (frames) and the score achieved by the agent (i.e. +1 reward if the coin is collected). In Go-Explore we also count the actions used to restore the environment to a selected cell, i.e. to bring the agent to the state represented in the selected cell. This allows a one-to-one comparison of the exploration efficiency between Go-Explore and algorithms that use a count-based reward in text-based games. Importantly, BIBREF8 showed that DQN and DRQN, without such counting rewards, could never find a successful trajectory in hard games such as the ones used in our experiments. Figure FIGREF17 shows the number of interactions with the environment (frames) versus the maximum score obtained, averaged over 10 games of the same difficulty. As shown by BIBREF8, DRQN++ finds a trajectory with the maximum score faster than to DQN++. On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment. Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively.",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 16,
          "matched": false,
          "score": 0.3673469387755102
        }
      ]
    },
    {
      "doc": "1802.06024.pdf",
      "question": "What are the F1 score improvements of the LiLi model over the \"Single\" model on the Freebase and WordNet datasets, specifically for known, unknown, and all relation types?",
      "prediction": "The F1 score improvements of the LiLi model over the \"Single\" model on the Freebase and WordNet datasets, specifically for known, unknown, and all relation types, are not provided in the given text.",
      "answer": "[{'answer': 'In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. \\n', 'type': 'abstractive'}]",
      "evidence": "Baselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines.\n\nSingle: Version of LiLi where we train a single prediction model INLINEFORM0 for all test relations.\n\nSep: We do not transfer (past learned) weights for initializing INLINEFORM0 , i.e., we disable LL.\n\nF-th): Here, we use a fixed prediction threshold 0.5 instead of relation-specific threshold INLINEFORM0 .\n\nBG: The missing or connecting links (when the user does not respond) are filled with “@-RelatedTo-@\" blindly, no guessing mechanism.\n\nw/o PTS: LiLi does not ask for additional clues via past task selection for skillset improvement.",
      "pages_used": [
        {
          "doc": "1709.05413.pdf",
          "page": 13,
          "matched": false,
          "score": 0.03496503496503497
        }
      ]
    },
    {
      "doc": "1810.03459.pdf",
      "question": "What are the specific BABEL languages used for training the multilingual sequence-to-sequence model and the target languages for transfer learning experiments as described in the paper?",
      "prediction": "The specific BABEL languages used for training the multilingual sequence-to-sequence model are English, French, German, Italian, and Spanish. The target languages for transfer learning experiments are Arabic, Chinese, and Russian.",
      "answer": "[{'answer': 'Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF14 presents the details of the languages used in this work for training and evaluation.",
      "pages_used": [
        {
          "doc": "2003.04642.pdf",
          "page": 18,
          "matched": false,
          "score": 0.018436326391259816
        }
      ]
    },
    {
      "doc": "2002.08899.pdf",
      "question": "How do the authors simulate Wernicke’s and Broca’s aphasia in the LLA-LSTM model by damaging neural modules in the paper?",
      "prediction": "The authors simulate Wernicke’s and Broca’s aphasia in the LLA-LSTM model by damaging neural modules in the paper.",
      "answer": "[{'answer': 'Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Results for artificial Wernicke’s and Broca’s aphasia induced in the LLA-LSTM model. Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information. The inputs that we present are arbitrarily chosen, subject to the constraints listed in the text. Mean precision (Prec.) results on the test sets are also provided to demonstrate corpus-level results. An ellipses represents the repetition of the preceding word at least 1000 times.",
      "pages_used": [
        {
          "doc": "2002.08899.pdf",
          "page": 4,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1902.09666.pdf",
      "question": "How many tweets are annotated in the OLID dataset, which focuses on the distribution of label combinations for categorizing offensive content by type and target?",
      "prediction": "The OLID dataset contains 10,000 annotated tweets.",
      "answer": "[{'answer': '14,100 tweets', 'type': 'abstractive'}, {'answer': 'Dataset contains total of 14100 annotations.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Distribution of label combinations in OLID.",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.25892857142857145
        }
      ]
    },
    {
      "doc": "1902.09666.pdf",
      "question": "How many tweets are annotated at each hierarchical level (A, B, and C) in the OLID dataset in the paper on identifying the type and target of offensive posts in social media?",
      "prediction": "The OLID dataset contains 100 annotated tweets at each hierarchical level (A, B, and C).",
      "answer": "[{'answer': 'Level A: 14100 Tweets\\nLevel B: 4640 Tweets\\nLevel C: 4089 Tweets', 'type': 'abstractive'}]",
      "evidence": " The breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15 .",
      "pages_used": [
        {
          "doc": "1908.06083.pdf",
          "page": 12,
          "matched": false,
          "score": 0.02197802197802198
        }
      ]
    },
    {
      "doc": "1910.11235.pdf",
      "question": "Which models are compared to the proposed method, where BLEU scores and road exam results are reported for the EMNLP2017 WMT News dataset?",
      "prediction": "The proposed method is compared to the following models: the baseline model, the model with a single attention mechanism, and the model with a multi-head attention mechanism.",
      "answer": "[{'answer': 'TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Results on EMNLP2017 WMT News dataset. The 95 % confidence intervals from multiple trials are reported.",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.25663716814159293
        }
      ]
    },
    {
      "doc": "1905.07464.pdf",
      "question": "What were the number of drug labels and sentences in Test Set 1 and Test Set 2 used to evaluate the performance of your system in the TAC 2018 track for drug-drug interaction extraction from structured product labels?",
      "prediction": "The number of drug labels and sentences in Test Set 1 and Test Set 2 used to evaluate the performance of the system in the TAC 2018 track for drug-drug interaction extraction from structured product labels were not provided in the given text.",
      "answer": "[{'answer': 'Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences', 'type': 'abstractive'}]",
      "evidence": "Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.3115264797507788
        }
      ]
    },
    {
      "doc": "1806.11432.pdf",
      "question": "Does the GAN model using the Diehl-Martinez-Kamalu (DMK) loss function in this paper show state-of-the-art performance when predicting Airbnb booking success from listing descriptions, based on development accuracy and cross-entropy loss?",
      "prediction": "Yes, the GAN model using the Diehl-Martinez-Kamalu (DMK) loss function in this paper shows state-of-the-art performance when predicting Airbnb booking success from listing descriptions, based on development accuracy and cross-entropy loss.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "That said, these results, though they do show a marginal increase in dev accuracy and a decrease in CE loss, suggest that perhaps listing description is not too predictive of occupancy rate given our parameterizations. ",
      "pages_used": [
        {
          "doc": "1711.02013.pdf",
          "page": 15,
          "matched": false,
          "score": 0.03138373751783167
        }
      ]
    },
    {
      "doc": "1806.11432.pdf",
      "question": "What GloVe-based baseline configurations, including ensembling and non-ensembling approaches, are compared in the study on using GANs to generate Airbnb listing descriptions with the DMK loss function?",
      "prediction": "The study compares GloVe-based baseline configurations that include ensembling and non-ensembling approaches.",
      "answer": "[{'answer': 'GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Results of RNN/LSTM",
      "pages_used": [
        {
          "doc": "1809.02286.pdf",
          "page": 9,
          "matched": false,
          "score": 0.015748031496062992
        }
      ]
    },
    {
      "doc": "1909.00361.pdf",
      "question": "What are the exact numbers of questions in the CMRC 2018 and DRCD Chinese machine reading comprehension datasets, as listed in the evaluations of this paper?",
      "prediction": "The CMRC 2018 dataset has 1,000 questions, and the DRCD Chinese machine reading comprehension dataset has 1,000 questions.",
      "answer": "[{'answer': 'Evaluation datasets used:\\nCMRC 2018 - 18939 questions, 10 answers\\nDRCD - 33953 questions, 5 answers\\nNIST MT02/03/04/05/06/08 Chinese-English - Not specified\\n\\nSource language train data:\\nSQuAD - Not specified', 'type': 'abstractive'}]",
      "evidence": "We evaluate our approaches on two public Chinese span-extraction machine reading comprehension datasets: CMRC 2018 (simplified Chinese) BIBREF8 and DRCD (traditional Chinese) BIBREF9. The statistics of the two datasets are listed in Table TABREF29.",
      "pages_used": [
        {
          "doc": "1911.12579.pdf",
          "page": 17,
          "matched": false,
          "score": 0.2132564841498559
        }
      ]
    },
    {
      "doc": "2002.01984.pdf",
      "question": "What was the highest MRR score achieved by the UNCC system for Factoid questions in Batch 3 of the BioASQ Task-7B, Phase-B competition?",
      "prediction": "The highest MRR score achieved by the UNCC system for Factoid questions in Batch 3 of the BioASQ Task-7B, Phase-B competition was 0.5115.",
      "answer": "[{'answer': '0.5115', 'type': 'abstractive'}, {'answer': '0.6103', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Factoid Questions. In Batch 3 we obtained the highest score. Also the relative distance between our best system and the top performing system shrunk between Batch 4 and 5.",
      "pages_used": [
        {
          "doc": "2002.01984.pdf",
          "page": 15,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2002.05058.pdf",
      "question": "How does the proposed comparative evaluator's Pearson correlation with human judgment in story generation and dialogue response tasks, at both the sample and model levels, compare to the next best automated evaluation metric?",
      "prediction": "The proposed comparative evaluator's Pearson correlation with human judgment is better than the next best automated evaluation metric at both the sample and model levels.",
      "answer": "[{'answer': 'Pearson correlation to human judgement - proposed vs next best metric\\nSample level comparison:\\n- Story generation: 0.387 vs 0.148\\n- Dialogue: 0.472 vs 0.341\\nModel level comparison:\\n- Story generation:  0.631 vs 0.302\\n- Dialogue: 0.783 vs 0.553', 'type': 'abstractive'}]",
      "evidence": "The experimental results are summarized in Table 1. We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity.",
      "pages_used": [
        {
          "doc": "2002.05058.pdf",
          "page": 5,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1910.06036.pdf",
      "question": "What are the specific improvements and slight degradations in automatic evaluation metrics for the proposed model compared to the baselines, especially for the Bleu1 scores on the Zhou split and DuSplit datasets?",
      "prediction": "The specific improvements and slight degradations in automatic evaluation metrics for the proposed model compared to the baselines, especially for the Bleu1 scores on the Zhou split and DuSplit datasets, are not provided in the given text.",
      "answer": "[{'answer': 'Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1', 'type': 'abstractive'}]",
      "evidence": "Table TABREF30 shows automatic evaluation results for our model and baselines (copied from their papers).",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.2988505747126437
        }
      ]
    },
    {
      "doc": "1605.07683.pdf",
      "question": "How many dialogs are in the training, validation, and test sets for Task 6, derived from the 2nd Dialog State Tracking Challenge, as reported in the Learning End-to-End Goal-Oriented Dialog paper?",
      "prediction": "The Learning End-to-End Goal-Oriented Dialog paper does not provide information about the number of dialogs in the training, validation, and test sets for Task 6, derived from the 2nd Dialog State Tracking Challenge.",
      "answer": "[{'answer': '1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Data used in this paper. Tasks 1-5 were generated using our simulator and share the same KB. Task 6 was converted from the 2nd Dialog State Tracking Challenge (Henderson et al., 2014a). Concierge is made of chats extracted from a real online concierge service. (∗) Tasks 1-5 have two test sets, one using the vocabulary of the training set and the other using out-of-vocabulary words.",
      "pages_used": [
        {
          "doc": "1710.06700.pdf",
          "page": 4,
          "matched": false,
          "score": 0.0708955223880597
        }
      ]
    },
    {
      "doc": "1709.10367.pdf",
      "question": "Does the empirical evaluation of the S-EFE model, which includes datasets like U.S. Senate speeches, ArXiv papers, and grocery purchases, involve only English-language content?",
      "prediction": "Yes, the empirical evaluation of the S-EFE model involves only English-language content.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "Data. We apply the sefe on three datasets: ArXiv papers, U.S. Senate speeches, and purchases on supermarket grocery shopping data. We describe these datasets below, and we provide a summary of the datasets in Table TABREF17 .",
      "pages_used": [
        {
          "doc": "1905.10810.pdf",
          "page": 5,
          "matched": false,
          "score": 0.2541436464088398
        }
      ]
    },
    {
      "doc": "1809.03449.pdf",
      "question": "Does the paper introducing the Knowledge Aided Reader (KAR) suggest that humans' robustness to noise in comprehension tasks stems from their ability to use general knowledge, such as the semantic connections extracted via WordNet?",
      "prediction": "Yes",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "To verify the effectiveness of general knowledge, we first study the relationship between the amount of general knowledge and the performance of KAR. As shown in Table TABREF13 , by increasing INLINEFORM0 from 0 to 5 in the data enrichment method, the amount of general knowledge rises monotonically, but the performance of KAR first rises until INLINEFORM1 reaches 3 and then drops down.",
      "pages_used": [
        {
          "doc": "1906.03538.pdf",
          "page": 16,
          "matched": false,
          "score": 0.21359223300970873
        }
      ]
    },
    {
      "doc": "1912.08960.pdf",
      "question": "Which specific ShapeWorldICE datasets, including OneShape, MultiShapes, TwoShapes, Count, and Ratio, are described in the paper for evaluating model performance?",
      "prediction": "The paper describes the OneShape, MultiShapes, TwoShapes, Count, and Ratio datasets from the ShapeWorldICE for evaluating model performance.",
      "answer": "[{'answer': 'Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE', 'type': 'abstractive'}, {'answer': 'ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio', 'type': 'abstractive'}]",
      "evidence": "We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper.",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.2196078431372549
        }
      ]
    },
    {
      "doc": "1911.03310.pdf",
      "question": "How does the data demonstrate that mBERT's language-neutral component leads to better word-alignment performance than FastAlign, even when using large parallel corpora, and what role does explicit projection learning play in this outcome?",
      "prediction": "The data shows that mBERT's language-neutral component leads to better word-alignment performance than FastAlign, even when using large parallel corpora, because it incorporates explicit projection learning.",
      "answer": "[{'answer': 'Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.', 'type': 'extractive'}, {'answer': 'explicit projection had a negligible effect on the performance', 'type': 'extractive'}]",
      "evidence": "Following BIBREF3, we hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-specific component is similar across all sentences in the language.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.2755905511811024
        }
      ]
    },
    {
      "doc": "1909.00430.pdf",
      "question": "Does the system trained using expectation regularization (XR) with sentence-level (S) and noisy sentence-level (N) data outperform the fully supervised neural system trained solely with aspect-level data (A) in terms of accuracy and Macro-F1 score for Aspect-based Sentiment Classification?",
      "prediction": "Yes, the system trained using expectation regularization (XR) with sentence-level (S) and noisy sentence-level (N) data outperforms the fully supervised neural system trained solely with aspect-level data (A) in terms of accuracy and Macro-F1 score for Aspect-based Sentiment Classification.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).",
      "pages_used": [
        {
          "doc": "1909.00430.pdf",
          "page": 9,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1909.00430.pdf",
      "question": "What are the reported accuracy values for the BiLSTM-XR and BiLSTM-XR-Dev Estimation models, trained using only XR loss on the SemEval-15 and SemEval-16 datasets, in the study on aspect-based sentiment classification using Sentence-level, Noisy sentence-level, and Aspect-level data?",
      "prediction": "The reported accuracy values for the BiLSTM-XR and BiLSTM-XR-Dev Estimation models, trained using only XR loss on the SemEval-15 and SemEval-16 datasets, are 83.31 ± 0.77 and 83.44 ±",
      "answer": "[{'answer': 'BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\\n', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).",
      "pages_used": [
        {
          "doc": "1909.00430.pdf",
          "page": 9,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1909.13695.pdf",
      "question": "What specific speaker verification systems, including adaptations and fine-tunings of VoxCeleb-based models, are evaluated for their % EER on BULATS and Linguaskill test sets?",
      "prediction": "The specific speaker verification systems, including adaptations and fine-tunings of VoxCeleb-based models, that are evaluated for their % EER on BULATS and Linguaskill test sets are not provided in the image.",
      "answer": "[{'answer': 'BULATS i-vector/PLDA\\nBULATS x-vector/PLDA\\nVoxCeleb x-vector/PLDA\\nPLDA adaptation (X1)\\n Extractor fine-tuning (X2) ', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2. % EER performance of VoxCeleb-based systems on BULATS and Linguaskill test sets.",
      "pages_used": [
        {
          "doc": "1810.12885.pdf",
          "page": 14,
          "matched": false,
          "score": 0.21666666666666667
        }
      ]
    },
    {
      "doc": "1904.10503.pdf",
      "question": "Which models are directly compared with the authors' ELMo and Wikidata-based approach for fine-grained entity recognition using the Wiki(gold) dataset?",
      "prediction": "The models directly compared with the authors' ELMo and Wikidata-based approach for fine-grained entity recognition using the Wiki(gold) dataset are LSTM-DQN, LSTM-DQN+ADM, DRRN, and Go-Explore.",
      "answer": "[{'answer': 'Akbik et al. (2018), Link et al. (2012)', 'type': 'abstractive'}, {'answer': 'They compare to Akbik et al. (2018) and Link et al. (2012).', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Comparison with existing models.",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 17,
          "matched": false,
          "score": 0.23255813953488372
        }
      ]
    },
    {
      "doc": "1804.05918.pdf",
      "question": "Based on the results reported for the paragraph-level neural network model trained on the PDTB corpus, which explicit discourse relations demonstrate the highest and lowest classification performance?",
      "prediction": "The explicit discourse relations that show the highest and lowest classification performance are not specified in the given text.",
      "answer": "[{'answer': 'explicit discourse relations', 'type': 'extractive'}, {'answer': 'Best: Expansion (Exp). Worst: Comparison (Comp).', 'type': 'abstractive'}]",
      "evidence": "the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ).",
      "pages_used": [
        {
          "doc": "1804.05918.pdf",
          "page": 4,
          "matched": false,
          "score": 0.048846675712347354
        }
      ]
    },
    {
      "doc": "1905.06566.pdf",
      "question": "Does the paper provide a comparison between hierarchical models like HiBERT and non-hierarchical models such as BERT, evaluated as baselines on the CNNDM dataset?",
      "prediction": "No, the paper does not provide a comparison between hierarchical models like HiBERT and non-hierarchical models such as BERT, evaluated as baselines on the CNNDM dataset.",
      "answer": "[{'answer': 'There were hierarchical and non-hierarchical baselines; BERT was one of those baselines', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Results of various models on the CNNDM test set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R2), and ROUGE-L (R-L).",
      "pages_used": [
        {
          "doc": "1910.06592.pdf",
          "page": 6,
          "matched": false,
          "score": 0.02268283144309738
        }
      ]
    },
    {
      "doc": "1811.02906.pdf",
      "question": "What are the percentage improvements in F1 score and accuracy for the top-performing transfer learning strategy in Task 1 in the paper on BiLSTM-CNN neural networks and background knowledge transfer for offensive language detection on German Twitter data?",
      "prediction": "The paper does not provide specific details about the percentage improvements in F1 score and accuracy for the top-performing transfer learning strategy in Task 1.",
      "answer": "[{'answer': 'In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Transfer learning performance (Task 1)",
      "pages_used": [
        {
          "doc": "1811.02906.pdf",
          "page": 10,
          "matched": false,
          "score": 0.02197802197802198
        }
      ]
    },
    {
      "doc": "1910.03814.pdf",
      "question": "What are the F-score, AUC, and mean accuracy values comparing the unimodal LSTM model and the best multimodal model (FCM) for hate speech detection on the MMHS150K dataset?",
      "prediction": "The F-score, AUC, and mean accuracy values comparing the unimodal LSTM model and the best multimodal model (FCM) for hate speech detection on the MMHS150K dataset are not provided in the given text.",
      "answer": "[{'answer': 'Unimodal LSTM vs Best Multimodal (FCM)\\n- F score: 0.703 vs 0.704\\n- AUC: 0.732 vs 0.734 \\n- Mean Accuracy: 68.3 vs 68.4 ', 'type': 'abstractive'}]",
      "evidence": "Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available.",
      "pages_used": [
        {
          "doc": "2003.03044.pdf",
          "page": 4,
          "matched": false,
          "score": 0.03581267217630854
        }
      ]
    },
    {
      "doc": "1808.03430.pdf",
      "question": "What recall scores for positions 1, 2, and 5 were reported for the introduced method when evaluated with 10 candidates in the comparison of different models?",
      "prediction": "The recall scores for positions 1, 2, and 5 were not provided in the document.",
      "answer": "[{'answer': 'Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Comparison of different models.",
      "pages_used": [
        {
          "doc": "1808.03430.pdf",
          "page": 6,
          "matched": false,
          "score": 0.026490066225165563
        }
      ]
    },
    {
      "doc": "1910.00912.pdf",
      "question": "Apart from entity tagging, what combined metrics for intent and entities are reported in HERMIT NLU's evaluation results?",
      "prediction": "HERMIT NLU's evaluation results report combined metrics for intent and entities such as F1 score, precision, and recall.",
      "answer": "[{'answer': 'We also report the metrics in BIBREF7 for consistency, we report the span F1,  Exact Match (EM) accuracy of the entire sequence of labels, metric that combines intent and entities', 'type': 'extractive'}]",
      "evidence": "Following BIBREF7, we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices (Table TABREF23). Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 17,
          "matched": false,
          "score": 0.25296442687747034
        }
      ]
    },
    {
      "doc": "2003.04642.pdf",
      "question": "Which MRC gold standards published between 2016 and 2019, meeting the citation-based thresholds and aligned with the problem definition in this study, were analyzed for features like lexical ambiguity, factual correctness, and the presence of lexical cues as part of their answer selection styles?",
      "prediction": "The MRC gold standards that were analyzed for features such as lexical ambiguity, factual correctness, and the presence of lexical cues as part of their answer selection styles are those published between 2016 and 2019, meeting the citation-based thresholds and aligned with the problem definition in this study.",
      "answer": "[{'answer': 'fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\\\\ year) \\\\times 20$ citations', 'type': 'extractive'}, {'answer': 'MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.', 'type': 'abstractive'}]",
      "evidence": "Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\\ year) \\times 20$ citations, and bucket them according to the answer selection styles as described in Section SECREF4",
      "pages_used": [
        {
          "doc": "2003.04642.pdf",
          "page": 4,
          "matched": false,
          "score": 0.035266974291364536
        }
      ]
    },
    {
      "doc": "1911.12579.pdf",
      "question": "According to the corpus statistics presented in the Sindhi word embeddings study, how many unique tokens were retained in the final cleaned vocabulary after preprocessing?",
      "prediction": "The final cleaned vocabulary after preprocessing retained 10,000 unique tokens.",
      "answer": "[{'answer': '908456 unique words are available in collected corpus.', 'type': 'abstractive'}]",
      "evidence": "The large corpus acquired from multiple resources is rich in vocabulary. We present the complete statistics of collected corpus (see Table TABREF52) with number of sentences, words and unique tokens.",
      "pages_used": [
        {
          "doc": "1911.12579.pdf",
          "page": 21,
          "matched": false,
          "score": 0.33217993079584773
        }
      ]
    },
    {
      "doc": "2003.03044.pdf",
      "question": "How many distinct clinical phenotypes were manually annotated in the dataset of Discharge Summaries and Nursing Progress Notes, focusing on the risk of recurrent ICU readmissions?",
      "prediction": "The dataset of Discharge Summaries and Nursing Progress Notes was manually annotated for 1,600 distinct clinical phenotypes focusing on the risk of recurrent ICU readmissions.",
      "answer": "[{'answer': '15 clinical patient phenotypes', 'type': 'extractive'}, {'answer': 'Thirteen different phenotypes are present in the dataset.', 'type': 'abstractive'}]",
      "evidence": "We have created a dataset of discharge summaries and nursing notes, all in the English language, with a focus on frequently readmitted patients, labeled with 15 clinical patient phenotypes believed to be associated with risk of recurrent Intensive Care Unit (ICU) readmission per our domain experts (co-authors LAC, PAT, DAG) as well as the literature. BIBREF10 BIBREF11 BIBREF12",
      "pages_used": [
        {
          "doc": "1809.05752.pdf",
          "page": 2,
          "matched": false,
          "score": 0.021867115222876366
        }
      ]
    },
    {
      "doc": "1707.00110.pdf",
      "question": "How do the BLEU scores vary based on different values of K and sequence lengths, and how do these scores compare to the baseline models with and without attention?",
      "prediction": "The BLEU scores vary based on different values of K and sequence lengths, and these scores compare to the baseline models with and without attention.",
      "answer": "[{'answer': 'Ranges from 44.22 to 100.00 depending on K and the sequence length.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: BLEU scores and computation times with varyingK and sequence length compared to baseline models with and without attention.",
      "pages_used": [
        {
          "doc": "1707.00110.pdf",
          "page": 8,
          "matched": false,
          "score": 0.035555555555555556
        }
      ]
    },
    {
      "doc": "1904.03288.pdf",
      "question": "What baseline models are listed for comparison in the Jasper paper when evaluating performance on the Hub5'00 conversational dataset?",
      "prediction": "The baseline models listed for comparison in the Jasper paper when evaluating performance on the Hub5'00 conversational dataset are the baseline models.",
      "answer": "[{'answer': 'LF-MMI Attention\\nSeq2Seq \\nRNN-T \\nChar E2E LF-MMI \\nPhone E2E LF-MMI \\nCTC + Gram-CTC', 'type': 'abstractive'}]",
      "evidence": " We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31 .",
      "pages_used": [
        {
          "doc": "1805.03710.pdf",
          "page": 6,
          "matched": false,
          "score": 0.00948429164196799
        }
      ]
    },
    {
      "doc": "1904.03288.pdf",
      "question": "What are the WER scores achieved by Jasper's best model on the WSJ nov93/nov92 subsets and the Hub5'00 dataset’s SWB and CHM subsets, and how do these results compare to the current state-of-the-art?",
      "prediction": "The WER scores achieved by Jasper's best model on the WSJ nov93/nov92 subsets and the Hub5'00 dataset's SWB and CHM subsets are not provided in the given text.",
      "answer": "[{'answer': 'In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.\\nIn case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. ', 'type': 'abstractive'}, {'answer': \"On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.\\nOn Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.\", 'type': 'abstractive'}]",
      "evidence": "We trained a smaller Jasper 10x3 model with SGD with momentum optimizer for 400 epochs on a combined WSJ dataset (80 hours): LDC93S6A (WSJ0) and LDC94S13A (WSJ1). The results are provided in Table TABREF29 .",
      "pages_used": [
        {
          "doc": "2002.01984.pdf",
          "page": 18,
          "matched": false,
          "score": 0.03610832497492478
        }
      ]
    },
    {
      "doc": "1810.06743.pdf",
      "question": "From which 31 languages are evaluated for token-level recall in the proposed conversion from Universal Dependencies tags to UniMorph tags?",
      "prediction": "The 31 languages evaluated for token-level recall in the proposed conversion from Universal Dependencies tags to UniMorph tags are not specified in the given text.",
      "answer": "[{'answer': 'Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur', 'type': 'abstractive'}, {'answer': 'We apply this conversion to the 31 languages, Arabic, Hindi, Lithuanian, Persian, and Russian. , Dutch, Spanish', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.",
      "pages_used": [
        {
          "doc": "1811.01088.pdf",
          "page": 12,
          "matched": false,
          "score": 0.022392457277548614
        }
      ]
    },
    {
      "doc": "1908.06267.pdf",
      "question": "In the ablation study results for the non-hierarchical MPAD model, performed on the Reuters, Polarity, and IMDB datasets, which modification resulted in the smallest performance change compared to the baseline?",
      "prediction": "The modification that resulted in the smallest performance change compared to the baseline in the ablation study results for the non-hierarchical MPAD model, performed on the Reuters, Polarity, and IMDB datasets, was the removal of the hierarchical structure.",
      "answer": "[{'answer': 'Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.', 'type': 'abstractive'}]",
      "evidence": "Results and ablations ::: Ablation studies\nTo understand the impact of some hyperparameters on performance, we conducted additional experiments on the Reuters, Polarity, and IMDB datasets, with the non-hierarchical version of MPAD. Results are shown in Table TABREF29.",
      "pages_used": [
        {
          "doc": "1709.05413.pdf",
          "page": 13,
          "matched": false,
          "score": 0.023980815347721823
        }
      ]
    },
    {
      "doc": "1908.06267.pdf",
      "question": "How does increasing the number of message passing iterations and removing the master node affect the classification performance of the non-hierarchical MPAD on the Reuters, Polarity, and IMDB datasets?",
      "prediction": "Increasing the number of message passing iterations and removing the master node decreases the classification performance of the non-hierarchical MPAD on the Reuters, Polarity, and IMDB datasets.",
      "answer": "[{'answer': 'Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations', 'type': 'abstractive'}, {'answer': 'Removing the master node deteriorates performance across all datasets', 'type': 'extractive'}]",
      "evidence": "Results and ablations ::: Ablation studies\nTo understand the impact of some hyperparameters on performance, we conducted additional experiments on the Reuters, Polarity, and IMDB datasets, with the non-hierarchical version of MPAD. Results are shown in Table TABREF29.\n\nNumber of MP iterations. First, we varied the number of message passing iterations from 1 to 4. We can clearly see in Table TABREF29 that having more iterations improves performance. We attribute this to the fact that we are reading out at each iteration from 1 to $T$ (see Eq. DISPLAY_FORM18), which enables the final graph representation to encode a mixture of low-level and high-level features. Indeed, in initial experiments involving readout at $t$=$T$ only, setting $T\\ge 2$ was always decreasing performance, despite the GRU-based updates (Eq. DISPLAY_FORM14). These results were consistent with that of BIBREF53 and BIBREF9, who both are reading out only at $t$=$T$ too. We hypothesize that node features at $T\\ge 2$ are too diffuse to be entirely relied upon during readout. More precisely, initially at $t$=0, node representations capture information about words, at $t$=1, about their 1-hop neighborhood (bigrams), at $t$=2, about compositions of bigrams, etc. Thus, pretty quickly, node features become general and diffuse. In such cases, considering also the lower-level, more precise features of the earlier iterations when reading out may be necessary.",
      "pages_used": [
        {
          "doc": "1908.06267.pdf",
          "page": 5,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1612.05270.pdf",
      "question": "Based on the dataset breakdown, how many annotated data points are provided for SemEval 2015, SemEval 2016, TASS 2015, and SENTIPOLC 2014 in the study?",
      "prediction": "The study provides 7,521 annotated data points for SemEval 2015, 6,000 for SemEval 2016, 7,218 for TASS 2015, and 4,501 for SENTIPOLC",
      "answer": "[{'answer': \"Total number of annotated data:\\nSemeval'15: 10712\\nSemeval'16: 28632\\nTass'15: 69000\\nSentipol'14: 6428\", 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Datasets details from each competition tested in this work",
      "pages_used": [
        {
          "doc": "1612.05270.pdf",
          "page": 6,
          "matched": false,
          "score": 0.04506657562307955
        }
      ]
    },
    {
      "doc": "1612.05270.pdf",
      "question": "In the paper analyzing B4MSA for multilingual sentiment classification, which languages, outside of SemEval, TASS, and SENTIPOLC, did B4MSA demonstrate superior performance compared to the reported results?",
      "prediction": "B4MSA demonstrated superior performance compared to the reported results for the languages English, German, and Spanish, in addition to SemEval, TASS, and SENTIPOLC.",
      "answer": "[{'answer': 'Arabic, German, Portuguese, Russian, Swedish', 'type': 'abstractive'}]",
      "evidence": "Table TABREF24 supports the idea of choosing B4MSA as a bootstrapping sentiment classifier because, in the overall, B4MSA reaches superior performances regardless of the language.",
      "pages_used": [
        {
          "doc": "2003.03044.pdf",
          "page": 4,
          "matched": false,
          "score": 0.028416779431664412
        }
      ]
    },
    {
      "doc": "1908.06379.pdf",
      "question": "What are the state-of-the-art dependency parsing metrics (UAS/LAS) and constituent parsing F1 scores achieved on the PTB and CTB datasets in the paper on Concurrent Parsing of Constituency and Dependency?",
      "prediction": "The paper on Concurrent Parsing of Constituency and Dependency does not provide the state-of-the-art dependency parsing metrics (UAS/LAS) and constituent parsing F1 scores achieved on the PTB and CTB datasets.",
      "answer": "[{'answer': '. On PTB, our model achieves 93.90 F1 score of constituent parsing and 95.91 UAS and 93.86 LAS of dependency parsing., On CTB, our model achieves a new state-of-the-art result on both constituent and dependency parsing.', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Dependency parsing on PTB and CTB.",
      "pages_used": [
        {
          "doc": "1908.06379.pdf",
          "page": 2,
          "matched": false,
          "score": 0.017307026652821047
        }
      ]
    },
    {
      "doc": "1801.05147.pdf",
      "question": "What are the F1 scores achieved by the proposed LSTM-CRF model for Chinese NER on the DL-PS dataset (dialogue domain) and the EC-MT/EC-UQ datasets (e-commerce domain)?",
      "prediction": "The F1 scores achieved by the proposed LSTM-CRF model for Chinese NER on the DL-PS dataset (dialogue domain) and the EC-MT/EC-UQ datasets (e-commerce domain) are not provided in the given text.",
      "answer": "[{'answer': 'F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ', 'type': 'abstractive'}, {'answer': 'F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Main results on the DL-PS data.",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.1792452830188679
        }
      ]
    },
    {
      "doc": "2001.05970.pdf",
      "question": "Based on the linear regression results, what is the exact correlation between the prevalence of #MeToo social media activity and official reports of sexual harassment?",
      "prediction": "The exact correlation between the prevalence of #MeToo social media activity and official reports of sexual harassment is not provided in the given text.",
      "answer": "[{'answer': '0.9098 correlation', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Linear regression results.",
      "pages_used": [
        {
          "doc": "1909.09587.pdf",
          "page": 8,
          "matched": false,
          "score": 0.061224489795918366
        }
      ]
    },
    {
      "doc": "1911.03597.pdf",
      "question": "How do the experiments demonstrate the performance of the transformer-based multilingual paraphrasing model in terms of relevance and fluency, compared to the pivoting method?",
      "prediction": "The experiments show that the transformer-based multilingual paraphrasing model surpasses the pivoting method in terms of relevance and fluency.",
      "answer": "[{'answer': 'our method outperforms the baseline in both relevance and fluency significantly.', 'type': 'extractive'}]",
      "evidence": "First we compare our models with the conventional pivoting method, i.e., round-trip translation. As shown in Figure FIGREF15 (a)(b), either the bilingual or the multilingual model is better than the baseline in terms of relevance and diversity in most cases. In other words, with the same generation diversity (measured by both Distinct-2 and Self-BLEU), our models can generate paraphrase with more semantically similarity to the input sentence.",
      "pages_used": [
        {
          "doc": "1911.03597.pdf",
          "page": 1,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1704.05907.pdf",
      "question": "What accuracy is reported for the multi-view network (MVN) model on the Stanford Sentiment Treebank 5-class classification task?",
      "prediction": "The accuracy reported for the multi-view network (MVN) model on the Stanford Sentiment Treebank 5-class classification task is 86.5%.",
      "answer": "[{'answer': '51.5', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Accuracies on the Stanford Sentiment Treebank 5-class classification task; except for the MVN, all results are drawn from (Lei et al., 2015).",
      "pages_used": [
        {
          "doc": "1704.05907.pdf",
          "page": 6,
          "matched": false,
          "score": 0.030634573304157548
        }
      ]
    },
    {
      "doc": "1907.09369.pdf",
      "question": "Are the results in the \"Emotion Detection in Text\" paper focused exclusively on datasets consisting of English text, including the new dataset tested on?",
      "prediction": "No",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 2: Results of final classification in Wang et al.",
      "pages_used": [
        {
          "doc": "2002.00652.pdf",
          "page": 8,
          "matched": false,
          "score": 0.009673518742442563
        }
      ]
    },
    {
      "doc": "2004.04721.pdf",
      "question": "What languages were evaluated to measure the cross-lingual transfer learning performance on the XNLI development set, highlighting the impact of translation artifacts?",
      "prediction": "The languages evaluated to measure the cross-lingual transfer learning performance on the XNLI development set, highlighting the impact of translation artifacts, were Polish and English.",
      "answer": "[{'answer': 'English\\nFrench\\nSpanish\\nGerman\\nGreek\\nBulgarian\\nRussian\\nTurkish\\nArabic\\nVietnamese\\nThai\\nChinese\\nHindi\\nSwahili\\nUrdu\\nFinnish', 'type': 'abstractive'}, {'answer': 'English, Spanish, Finnish', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: XNLI dev results (acc). BT-XX and MT-XX consistently outperform ORIG in all cases.",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.2983425414364641
        }
      ]
    },
    {
      "doc": "1808.09920.pdf",
      "question": "What performance metric is used to evaluate the accuracy of models on the WIKIHOP closed test and public validation sets in the Entity-GCN paper?",
      "prediction": "The accuracy of models on the WIKIHOP closed test and public validation sets is evaluated using the F1 score.",
      "answer": "[{'answer': 'Accuracy', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Accuracy of different models on WIKIHOP closed test set and public validation set. Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pretrained one (ELMo – without fine-tunning it) and applying R-GCN to reason among entities in the text. * with coreference for unmasked dataset and without coreference for the masked one.",
      "pages_used": [
        {
          "doc": "1808.09920.pdf",
          "page": 6,
          "matched": false,
          "score": 0.04154140475539765
        }
      ]
    },
    {
      "doc": "1808.09920.pdf",
      "question": "What are the testing accuracies reported for the Entity-GCN model (with and without coreference) and the ensemble of 5 models on the WIKIHOP closed test set, in the paper that uses R-GCN for reasoning among entities without fine-tuning a language model?",
      "prediction": "The testing accuracies reported for the Entity-GCN model (with and without coreference) and the ensemble of 5 models on the WIKIHOP closed test set are 71.2 and 71.6, respectively.",
      "answer": "[{'answer': 'During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Accuracy of different models on WIKIHOP closed test set and public validation set. Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pretrained one (ELMo – without fine-tunning it) and applying R-GCN to reason among entities in the text. * with coreference for unmasked dataset and without coreference for the masked one.",
      "pages_used": [
        {
          "doc": "1808.09920.pdf",
          "page": 6,
          "matched": false,
          "score": 0.04154140475539765
        }
      ]
    },
    {
      "doc": "1911.04952.pdf",
      "question": "What are the twenty distinct lyrical topics identified through the LDA topic model from the corpus of 124,288 metal song lyrics?",
      "prediction": "The twenty distinct lyrical topics identified through the LDA topic model from the corpus of 124,288 metal song lyrics are: 1. War and Conflict, 2. Death and Suffering, 3. Love and Romance, 4. Power and Authority, 5. Nature",
      "answer": "[{'answer': 'Table TABREF10 displays the twenty resulting topics', 'type': 'extractive'}]",
      "evidence": "Table TABREF10 displays the twenty resulting topics found within the text corpus using LDA.",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.34579439252336447
        }
      ]
    },
    {
      "doc": "1911.01799.pdf",
      "question": "What is the EER(%) performance of the i-vector and x-vector systems trained on VoxCeleb and evaluated using the CN-Celeb evaluation dataset?",
      "prediction": "The EER(%) performance of the i-vector and x-vector systems trained on VoxCeleb and evaluated using the CN-Celeb evaluation dataset is 13%.",
      "answer": "[{'answer': 'ERR of 19.05 with i-vectors and 15.52 with x-vectors', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.16666666666666666
        }
      ]
    },
    {
      "doc": "1911.01799.pdf",
      "question": "What are the 11 genres listed in the CN-Celeb speaker recognition dataset for evaluating speaker recognition in unconstrained conditions?",
      "prediction": "The 11 genres listed in the CN-Celeb speaker recognition dataset for evaluating speaker recognition in unconstrained conditions are: CNN, Florida, Johnson, Ochocinco, Evelyn, Lozada, Johnson, Ochocinco, Evelyn, Lozada, Johnson, Ochocinco",
      "answer": "[{'answer': 'genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1. The distribution over genres.",
      "pages_used": [
        {
          "doc": "1810.12885.pdf",
          "page": 14,
          "matched": false,
          "score": 0.3380281690140845
        }
      ]
    },
    {
      "doc": "1911.01799.pdf",
      "question": "Based on the results presented in the CN-Celeb paper, which model—i-vector or x-vector—showed better performance in terms of EER (%) when evaluated on the CN-Celeb dataset?",
      "prediction": "The x-vector model showed better performance in terms of EER (%) when evaluated on the CN-Celeb dataset.",
      "answer": "[{'answer': 'x-vector', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.",
      "pages_used": [
        {
          "doc": "1810.12085.pdf",
          "page": 4,
          "matched": false,
          "score": 0.039634146341463415
        }
      ]
    },
    {
      "doc": "1911.01799.pdf",
      "question": "*According to the CN-Celeb paper, what is the percentage increase in EER for both the i-vector and x-vector systems when evaluated on CN-Celeb compared to VoxCeleb?*",
      "prediction": "The percentage increase in EER for both the i-vector and x-vector systems when evaluated on CN-Celeb compared to VoxCeleb is 10%.",
      "answer": "[{'answer': 'For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.",
      "pages_used": [
        {
          "doc": "1909.09587.pdf",
          "page": 8,
          "matched": false,
          "score": 0.04477611940298507
        }
      ]
    },
    {
      "doc": "2002.02492.pdf",
      "question": "How effective are the consistent variants of nucleus and top-k sampling in reducing the non-termination ratio compared to baseline decoding algorithms?",
      "prediction": "The consistent variants of nucleus and top-k sampling are effective in reducing the non-termination ratio compared to baseline decoding algorithms.",
      "answer": "[{'answer': 'It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF44 shows that consistent nucleus and top-$k$ sampling (§SECREF28) resulted in only terminating sequences, except for a few cases that we attribute to the finite limit $L$ used to measure the non-termination ratio. The example continuations in Table TABREF46 show that the sampling tends to preserve language modeling quality on prefixes that led to termination with the baseline (first row). On prefixes that led to non-termination with the baseline (second & third rows), the quality tends to improve since the continuation now terminates. Since the model's non-$\\left<\\text{eos}\\right>$ token probabilities at each step are only modified by a multiplicative constant, the sampling process can still enter a repetitive cycle (e.g. when the constant is close to 1), though the cycle is guaranteed to eventually terminate.",
      "pages_used": [
        {
          "doc": "2002.02492.pdf",
          "page": 15,
          "matched": false,
          "score": 0.3002680965147453
        }
      ]
    },
    {
      "doc": "1901.02262.pdf",
      "question": "Which models are listed as baselines in this paper for comparing Masque's performance on the MS MARCO V2 leaderboard?",
      "prediction": "The baselines for comparing Masque's performance on the MS MARCO V2 leaderboard are the models listed in the table, which include \"Our system\", \"Rashwan et al. 2015\", \"Farasa\", \"Microsoft ATKS\", \"MADAMIRA\", and \"Belinkov",
      "answer": "[{'answer': 'BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Performance of our and competing models on the MS MARCO V2 leaderboard (4 March 2019). aSeo et al. (2017); bYan et al. (2019); cShao (unpublished), a variant of Tan et al. (2018); dLi (unpublished), a model using Devlin et al. (2018) and See et al. (2017); eQian (unpublished); fWu et al. (2018). Whether the competing models are ensemble models or not is unreported.",
      "pages_used": [
        {
          "doc": "2002.01207.pdf",
          "page": 22,
          "matched": false,
          "score": 0.08374384236453201
        }
      ]
    },
    {
      "doc": "1901.02262.pdf",
      "question": "How does the Masque model perform on the NarrativeQA test set in terms of BLEU-1, BLEU-4, METEOR, and ROUGE-L scores?",
      "prediction": "The Masque model achieves the following scores on the NarrativeQA test set: BLEU-1 = 54.11, BLEU-4 = 30.43, METEOR = 26.13, and ROUGE-L = 59.87.",
      "answer": "[{'answer': 'Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set.",
      "pages_used": [
        {
          "doc": "1901.02262.pdf",
          "page": 8,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1809.00530.pdf",
      "question": "Based on the dataset summary, how many labeled data points are there for the datasets Book, Electronics, Beauty, Music, IMDB, Yelp, Cell Phone, and Baby?",
      "prediction": "There are 8 labeled data points for the datasets Book, Electronics, Beauty, Music, IMDB, Yelp, Cell Phone, and Baby.",
      "answer": "[{'answer': '719313', 'type': 'abstractive'}, {'answer': 'Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Summary of datasets.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 17,
          "matched": false,
          "score": 0.18085106382978725
        }
      ]
    },
    {
      "doc": "1809.00530.pdf",
      "question": "Which specific domains were selected as the source and target for sentiment classification in this research?",
      "prediction": "The specific domains selected for sentiment classification in this research are natural language problems and formal language solutions.",
      "answer": "[{'answer': 'Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen', 'type': 'abstractive'}, {'answer': 'we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Summary of datasets.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 17,
          "matched": false,
          "score": 0.18085106382978725
        }
      ]
    },
    {
      "doc": "1908.10084.pdf",
      "question": "Which sentence embedding methods are evaluated alongside SBERT and SRoBERTa based on Spearman rank correlation for the STS tasks in the paper?",
      "prediction": "The paper evaluates the performance of SBERT and SRoBERTa alongside average GloVe embeddings, average BERT embeddings, BERT CLS-vector, InferSent - Glove, and Universal Sentence Encoder for the STS tasks based on Spearman rank correlation.",
      "answer": "[{'answer': 'GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent', 'type': 'abstractive'}, {'answer': 'Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks. Performance is reported by convention as ρ × 100. STS12-STS16: SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.",
      "pages_used": [
        {
          "doc": "1908.10084.pdf",
          "page": 4,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1908.05828.pdf",
      "question": "According to the \"Dataset statistics\" section of the Nepali NER study, how many sentences are present in both the training and testing sets of the dataset?",
      "prediction": "1000",
      "answer": "[{'answer': '3606', 'type': 'abstractive'}, {'answer': '6946', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Dataset statistics",
      "pages_used": [
        {
          "doc": "1909.00105.pdf",
          "page": 12,
          "matched": false,
          "score": 0.2608695652173913
        }
      ]
    },
    {
      "doc": "1908.05828.pdf",
      "question": "Which baseline models are used to compare the proposed grapheme-level neural-based Nepali NER to other models for performance evaluation?",
      "prediction": "The baseline models used for performance evaluation include the LSTM-CRF model and the Bi-LSTM-CRF model.",
      "answer": "[{'answer': 'CNN modelBIBREF0, Stanford CRF modelBIBREF21', 'type': 'extractive'}, {'answer': 'Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec', 'type': 'abstractive'}]",
      "evidence": "First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21.",
      "pages_used": [
        {
          "doc": "1811.02906.pdf",
          "page": 10,
          "matched": false,
          "score": 0.04710144927536232
        }
      ]
    },
    {
      "doc": "1908.05828.pdf",
      "question": "What are the total number of sentences and entities in the ILPRL and OurNepali datasets in the neural-based Nepali NER study?",
      "prediction": "The ILPRL and OurNepali datasets contain a total of 100 sentences and 100 entities in the neural-based Nepali NER study.",
      "answer": "[{'answer': 'Dataset contains 3606 total sentences and 79087 total entities.', 'type': 'abstractive'}, {'answer': 'ILPRL contains 548 sentences, OurNepali contains 3606 sentences', 'type': 'abstractive'}]",
      "evidence": "The statistics of both the dataset is presented in table TABREF23.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 16,
          "matched": false,
          "score": 0.27230046948356806
        }
      ]
    },
    {
      "doc": "1908.05828.pdf",
      "question": "How many distinct entity types are listed for the OurNepali and ILPRL datasets in the \"Named Entity Recognition for Nepali Language\" paper?",
      "prediction": "The \"Named Entity Recognition for Nepali Language\" paper lists 10 distinct entity types for the OurNepali and ILPRL datasets.",
      "answer": "[{'answer': 'OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities', 'type': 'abstractive'}, {'answer': 'three', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Dataset statistics",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.21
        }
      ]
    },
    {
      "doc": "1908.05828.pdf",
      "question": "In the dataset statistics section of the paper on neural-based Nepali NER, how many sentences and entities are present in the newly created dataset?",
      "prediction": "The newly created dataset contains 10,000 sentences and 100,000 entities.",
      "answer": "[{'answer': '3606 sentences', 'type': 'abstractive'}, {'answer': 'Dataset contains 3606 total sentences and 79087 total entities.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Dataset statistics",
      "pages_used": [
        {
          "doc": "1908.05828.pdf",
          "page": 10,
          "matched": false,
          "score": 0.005698005698005698
        }
      ]
    },
    {
      "doc": "1908.05828.pdf",
      "question": "What is the performance comparison between the grapheme-level and character-level BiLSTM+CNN models on the OurNepali and ILPRL test datasets as reported in the Nepali NER paper?",
      "prediction": "The grapheme-level BiLSTM+CNN model outperformed the character-level BiLSTM+CNN model on both the OurNepali and ILPRL test datasets.",
      "answer": "[{'answer': 'On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement', 'type': 'abstractive'}, {'answer': 'BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: Comparison of different variation of our models",
      "pages_used": [
        {
          "doc": "1909.00105.pdf",
          "page": 12,
          "matched": false,
          "score": 0.36363636363636365
        }
      ]
    },
    {
      "doc": "1711.02013.pdf",
      "question": "What performance metrics are reported to evaluate the PRPN model's variants on word/character-level language modeling tasks after the removal of different model components?",
      "prediction": "The performance metrics reported to evaluate the PRPN model's variants on word/character-level language modeling tasks after the removal of different model components are perplexity and BLEU score.",
      "answer": "[{'answer': 'BPC, Perplexity', 'type': 'abstractive'}]",
      "evidence": "In Table TABREF40 , we show the value of test perplexity for different variants of PRPN, each variant remove part of the model. ",
      "pages_used": [
        {
          "doc": "2003.08385.pdf",
          "page": 12,
          "matched": false,
          "score": 0.03949447077409163
        }
      ]
    },
    {
      "doc": "1703.06492.pdf",
      "question": "In the evaluation results, for which VQA task did the method achieve state-of-the-art accuracy of 60.34%, especially using basic questions and VGG?",
      "prediction": "The method achieved state-of-the-art accuracy of 60.34% in the counting-type VQA task, especially using basic questions and VGG.",
      "answer": "[{'answer': 'in open-ended task esp. for counting-type questions ', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4. Evaluation results on VQA dataset [1]. ”-” indicates the results are not available, and the Ours+VGG(1) and Ours+VGG(2) are the results by using different thresholds. Note that our VGGNet is same as CoAtt+VGG.",
      "pages_used": [
        {
          "doc": "1703.06492.pdf",
          "page": 6,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1909.09587.pdf",
      "question": "What are the exact EM and F1 scores for multi-BERT when fine-tuned on English and tested on Chinese, as compared to QANet trained on Chinese?",
      "prediction": "The exact EM and F1 scores for multi-BERT when fine-tuned on English and tested on Chinese, as compared to QANet trained on Chinese, are not provided in the given text.",
      "answer": "[{'answer': 'Table TABREF6, Table TABREF8', 'type': 'extractive'}, {'answer': 'when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En', 'type': 'extractive'}]",
      "evidence": "Table TABREF6 shows the result of different models trained on either Chinese or English and tested on Chinese. In row (f), multi-BERT is fine-tuned on English but tested on Chinese, which achieves competitive performance compared with QANet trained on Chinese. We also find that multi-BERT trained on English has relatively lower EM compared with the model with comparable F1 scores. ",
      "pages_used": [
        {
          "doc": "1905.10810.pdf",
          "page": 5,
          "matched": false,
          "score": 0.2914572864321608
        }
      ]
    },
    {
      "doc": "1909.09587.pdf",
      "question": "What source-target language pairs were fine-tuned and tested for zero-shot cross-lingual reading comprehension tasks in this paper's multi-BERT experiments?",
      "prediction": "The paper's multi-BERT experiments fine-tuned and tested on English-German and English-Spanish source-target language pairs for zero-shot cross-lingual reading comprehension tasks.",
      "answer": "[{'answer': 'En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean', 'type': 'abstractive'}, {'answer': 'English , Chinese', 'type': 'extractive'}, {'answer': 'English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: EM/F1 score of multi-BERTs fine-tuned on different training sets and tested on different languages (En: English, Fr: French, Zh: Chinese, Jp: Japanese, Kr: Korean, xx-yy: translated from xx to yy). The text in bold means training data language is the same as testing data language.",
      "pages_used": [
        {
          "doc": "1909.09587.pdf",
          "page": 8,
          "matched": false,
          "score": 0.03982300884955752
        }
      ]
    },
    {
      "doc": "1705.01214.pdf",
      "question": "What performance metrics were used to evaluate the classifiers in the first version of the training set for your proposed hybrid architecture in the finance domain?",
      "prediction": "The performance metrics used to evaluate the classifiers in the first version of the training set for the proposed hybrid architecture in the finance domain were accuracy and discriminator error rate.",
      "answer": "[{'answer': 'precision, recall, F1 and accuracy', 'type': 'abstractive'}, {'answer': 'Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 15: Evaluation of different classifiers in the first version of the training set",
      "pages_used": [
        {
          "doc": "1607.06025.pdf",
          "page": 14,
          "matched": false,
          "score": 0.04032258064516129
        }
      ]
    },
    {
      "doc": "1911.08976.pdf",
      "question": "What three methods did Red Dragon AI employ for explanation regeneration in their TextGraphs-13 Shared Task submission, and how are they ranked by MAP scoring?",
      "prediction": "Red Dragon AI used three methods for explanation regeneration: CRFSuite, Senna, and Stanford. According to the MAP scoring, they are ranked as follows: CRFSuite, Senna, and Stanford.",
      "answer": "[{'answer': 'Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: MAP scoring of new methods. The timings are in seconds for the whole dev-set, and the BERT Re-ranking figure includes the initial Iterated TF-IDF step.",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.19931271477663232
        }
      ]
    },
    {
      "doc": "1712.03556.pdf",
      "question": "What are the specific improvements in exact match (EM) and F1 scores of the Stochastic Answer Network (SAN) compared to baseline models, and how does SAN rank in F1 performance on the SQuAD test set?",
      "prediction": "The document does not provide specific improvements in exact match (EM) and F1 scores of the Stochastic Answer Network (SAN) compared to baseline models, nor does it mention how SAN ranks in F1 performance on the SQuAD test set.",
      "answer": "[{'answer': 'Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Test performance on SQuAD. Results are sorted by Test F1.",
      "pages_used": [
        {
          "doc": "1704.05907.pdf",
          "page": 6,
          "matched": false,
          "score": 0.022222222222222223
        }
      ]
    },
    {
      "doc": "1911.00069.pdf",
      "question": "What are the exact document counts for the in-house and ACE05 datasets in the training, development, and test sets in the paper on neural cross-lingual relation extraction using bilingual word embedding mapping?",
      "prediction": "The exact document counts for the in-house and ACE05 datasets in the training, development, and test sets in the paper on neural cross-lingual relation extraction using bilingual word embedding mapping are not provided in the image.",
      "answer": "[{'answer': 'In-house dataset consists of  3716 documents \\nACE05 dataset consists of  1635 documents', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets.",
      "pages_used": [
        {
          "doc": "1909.00015.pdf",
          "page": 11,
          "matched": false,
          "score": 0.037383177570093455
        }
      ]
    },
    {
      "doc": "1701.00185.pdf",
      "question": "What are the ACC and NMI performance results of the STC^2 framework for the SearchSnippets, StackOverflow, and Biomedical datasets?",
      "prediction": "The ACC and NMI performance results of the STC^2 framework for the SearchSnippets, StackOverflow, and Biomedical datasets are not provided in the given text.",
      "answer": "[{'answer': 'On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",
      "pages_used": [
        {
          "doc": "1701.00185.pdf",
          "page": 21,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1701.00185.pdf",
      "question": "By what percentage did the STC^2 model outperform other non-biased clustering methods in terms of ACC and NMI on both the SearchSnippets and Biomedical datasets?",
      "prediction": "The STC^2 model outperformed other non-biased clustering methods in terms of ACC and NMI on both the SearchSnippets and Biomedical datasets by 20.01% and 1.25%, respectively.",
      "answer": "[{'answer': 'on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",
      "pages_used": [
        {
          "doc": "1701.00185.pdf",
          "page": 21,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1911.02821.pdf",
      "question": "What are the four datasets reported for evaluating the improvements brought by the Multi-source Word-aligned Attention (MWA) method in this paper?",
      "prediction": "The four datasets reported for evaluating the improvements brought by the Multi-source Word-aligned Attention (MWA) method in this paper are: DeepER, DeepER+MWA, DeepER+MWA+M, and DeepER+MWA+M+M.",
      "answer": "[{'answer': 'weibo-100k, Ontonotes, LCQMC and XNLI', 'type': 'abstractive'}]",
      "evidence": "Table TABREF14 shows the experiment measuring improvements from the MWA attention on test sets of four datasets.",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.28085106382978725
        }
      ]
    },
    {
      "doc": "1903.00172.pdf",
      "question": "What are the two question-answer datasets used to evaluate NeurON’s tuple extraction performance?",
      "prediction": "The two question-answer datasets used to evaluate NeurON’s tuple extraction performance are Twitter and DBpedia.",
      "answer": "[{'answer': 'AmazonQA and ConciergeQA datasets', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Precision (P), Recall (R), and Relative Coverage (RC) results on ConciergeQA.",
      "pages_used": [
        {
          "doc": "2001.06888.pdf",
          "page": 13,
          "matched": false,
          "score": 0.27932960893854747
        }
      ]
    },
    {
      "doc": "1903.00172.pdf",
      "question": "Which two real-world datasets are listed in the NeurON paper for evaluating the system's performance in Open Information Extraction from question-answer pairs?",
      "prediction": "The two real-world datasets listed in the NeurON paper for evaluating the system's performance in Open Information Extraction from question-answer pairs are the HotpotQA dataset and the HotpotQA dataset.",
      "answer": "[{'answer': 'ConciergeQA and AmazonQA', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Various types of training instances.",
      "pages_used": [
        {
          "doc": "1903.00172.pdf",
          "page": 13,
          "matched": false,
          "score": 0.288
        }
      ]
    },
    {
      "doc": "1909.11687.pdf",
      "question": "What specific compression techniques, including NoKD and PKD, were compared in the evaluation of mixed-vocabulary training models in the \"Extremely Small BERT Models from Mixed-Vocabulary Training\" paper?",
      "prediction": "The evaluation of mixed-vocabulary training models in the \"Extremely Small BERT Models from Mixed-Vocabulary Training\" paper compared the compression techniques of NoKD and PKD.",
      "answer": "[{'answer': 'baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)', 'type': 'extractive'}, {'answer': 'NoKD, PKD, BERTBASE teacher model', 'type': 'extractive'}]",
      "evidence": "For the language modeling evaluation, we also evaluate a baseline without knowledge distillation (termed NoKD), with a model parameterized identically to the distilled student models but trained directly on the teacher model objective from scratch. For downstream tasks, we compare with NoKD as well as Patient Knowledge Distillation (PKD) from BIBREF34, who distill the 12-layer BERTBASE model into 3 and 6-layer BERT models by using the teacher model's hidden states.",
      "pages_used": [
        {
          "doc": "2004.03354.pdf",
          "page": 8,
          "matched": false,
          "score": 0.030261348005502064
        }
      ]
    },
    {
      "doc": "1809.01541.pdf",
      "question": "What languages are reported in the official CoNLL-SIGMORPHON 2018 shared task test set results for evaluating the Copenhagen system's morphological reinflection approach?",
      "prediction": "The official CoNLL-SIGMORPHON 2018 shared task test set results for evaluating the Copenhagen system's morphological reinflection approach report the languages Danish and Norwegian.",
      "answer": "[{'answer': 'German, English, Spanish, Finnish, French, Russian,  Swedish.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Official shared task test set results.",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.17391304347826086
        }
      ]
    },
    {
      "doc": "1809.01541.pdf",
      "question": "What specific type of MSD tag prediction (e.g., V;PST;V.PTCP;PASS) is used as an auxiliary task in the Copenhagen system for the inflection-in-context task in CoNLL--SIGMORPHON 2018, particularly highlighted in Track 1?",
      "prediction": "The auxiliary task in the Copenhagen system for the inflection-in-context task in CoNLL--SIGMORPHON 2018, particularly highlighted in Track 1, is the prediction of the MSD tag V;PST;V.PTCP;PASS.",
      "answer": "[{'answer': 'The task of predicting MSD tags: V, PST, V.PCTP, PASS.', 'type': 'abstractive'}, {'answer': 'morphosyntactic descriptions (MSD)', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Example input sentence. Context MSD tags and lemmas, marked in gray, are only available in Track 1. The cyan square marks the main objective of predicting the word form made. The magenta square marks the auxiliary objective of predicting the MSD tag V;PST;V.PTCP;PASS.",
      "pages_used": [
        {
          "doc": "1809.01541.pdf",
          "page": 2,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1909.13714.pdf",
      "question": "What was the specific improvement in F1 scores for both intent detection and slot filling when incorporating multimodal inputs (speech embeddings, audio, vision) in the NLU models?",
      "prediction": "The specific improvement in F1 scores for both intent detection and slot filling when incorporating multimodal inputs (speech embeddings, audio, vision) in the NLU models was 0.92 for utterance-level intent detection and 0.97 for word-level slot filling.",
      "answer": "[{'answer': 'by 2.3-6.8 points in f1 score for intent recognition and 0.8-3.5 for slot filling', 'type': 'abstractive'}, {'answer': 'F1 score increased from 0.89 to 0.92', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Speech Embeddings Experiments: Precision/Recall/F1-scores (%) of NLU Models",
      "pages_used": [
        {
          "doc": "1909.13714.pdf",
          "page": 2,
          "matched": false,
          "score": 0.021246458923512748
        }
      ]
    },
    {
      "doc": "1702.03342.pdf",
      "question": "Which neural embedding model, CRX or 3C, achieves the highest accuracy in the dataless concept categorization task as presented in the paper?",
      "prediction": "The paper does not provide information about the accuracy of CRX or 3C in the dataless concept categorization task.",
      "answer": "[{'answer': 'the CRX model', 'type': 'abstractive'}, {'answer': '3C model', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 5 Accuracy of concept categorization",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.1111111111111111
        }
      ]
    },
    {
      "doc": "1910.12795.pdf",
      "question": "How does the data manipulation method in this paper improve text classification accuracy on low-data datasets like SST-5, TREC, and IMDB, as well as imbalanced label distributions such as 100:1000 and 20:1000?",
      "prediction": "The data manipulation method in this paper improves text classification accuracy on low-data datasets like SST-5, TREC, and IMDB, as well as imbalanced label distributions such as 100:1000 and 20:1000, by using data augmentation techniques such as adding noise",
      "answer": "[{'answer': 'Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Accuracy of Data Manipulation on Text Classification. All results are averaged over 15 runs ± one standard deviation. The numbers in parentheses next to the dataset names indicate the size of the datasets. For example, (40+2) denotes 40 training instances and 2 validation instances per class.",
      "pages_used": [
        {
          "doc": "1910.12795.pdf",
          "page": 12,
          "matched": false,
          "score": 0.023255813953488372
        }
      ]
    },
    {
      "doc": "1910.04269.pdf",
      "question": "Does the paper indicate that the language identification models combine both log-Mel spectrograms and raw waveforms as input features simultaneously?",
      "prediction": "No, the paper does not indicate that the language identification models combine both log-Mel spectrograms and raw waveforms as input features simultaneously.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 4: Results of the two models and all its variations",
      "pages_used": [
        {
          "doc": "1909.00015.pdf",
          "page": 12,
          "matched": false,
          "score": 0.027359781121751026
        }
      ]
    },
    {
      "doc": "1703.02507.pdf",
      "question": "Which unsupervised models are compared with the proposed sentence embedding method on supervised evaluation tasks in the study?",
      "prediction": "The study compares the proposed sentence embedding method with the following unsupervised models on supervised evaluation tasks: CaptionRep BOW, CaptionRep RNN, DictRep BOW, DictRep BOW + embs, DictRep RNN, and DictRep RNN + embs.",
      "answer": "[{'answer': 'Sequential (Denoising) Autoencoder, TF-IDF BOW, SkipThought, FastSent, Siamese C-BOW, C-BOW, C-PHRASE, ParagraphVector', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Comparison of the performance of different models on different supervised evaluation tasks. An underline indicates the best performance for the dataset. Top 3 performances in each data category are shown in bold. The average is calculated as the average of accuracy for each category (For MSRP, we take the accuracy). )",
      "pages_used": [
        {
          "doc": "1703.02507.pdf",
          "page": 13,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1909.02480.pdf",
      "question": "What is the BLEU score difference between FlowSeq and state-of-the-art baselines that utilize advanced decoding techniques like iterative refinement and NPD rescoring on NMT tasks?",
      "prediction": "The BLEU score difference between FlowSeq and state-of-the-art baselines that utilize advanced decoding techniques like iterative refinement and NPD rescoring on NMT tasks is 1.",
      "answer": "[{'answer': 'Difference is around 1 BLEU score lower on average than state of the art methods.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF40 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring.",
      "pages_used": [
        {
          "doc": "1909.00512.pdf",
          "page": 11,
          "matched": false,
          "score": 0.03838771593090211
        }
      ]
    },
    {
      "doc": "1603.04513.pdf",
      "question": "What are the performance improvements in test scores for MVCNN with pretraining across the classification tasks Binary, Fine-Grained, Senti140, and Subjectivity?",
      "prediction": "The performance improvements in test scores for MVCNN with pretraining across the classification tasks Binary, Fine-Grained, Senti140, and Subjectivity are 89.4, 49.6, 88.2, and 93.9, respectively.",
      "answer": "[{'answer': '0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Minus sign “-” in MVCNN (-Huang) etc. means “Huang” is not used. “versions / filters / tricks / layers” denote the MVCNN variants with different setups: discard certain embedding version / discard certain filter size / discard mutual-learning or pretraining / different numbers of convolution layer.",
      "pages_used": [
        {
          "doc": "1603.04513.pdf",
          "page": 8,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1603.04513.pdf",
      "question": "How do the variable-size convolution filters (especially sizes 5 and 7) contribute to the extraction of multigranular phrase features and overall performance in the MVCNN architecture?",
      "prediction": "The variable-size convolution filters, particularly sizes 5 and 7, contribute to the extraction of multigranular phrase features and overall performance in the MVCNN architecture by allowing the model to capture both fine-grained and coarse-grained features.",
      "answer": "[{'answer': 'The system benefits from filters of each size., features of multigranular phrases are extracted with variable-size convolution filters.', 'type': 'extractive'}]",
      "evidence": "The block “filters” indicates the contribution of each filter size. The system benefits from filters of each size. Sizes 5 and 7 are most important for high performance, especially 7 (rows 25 and 26).",
      "pages_used": [
        {
          "doc": "1603.04513.pdf",
          "page": 8,
          "matched": false,
          "score": 0.01448225923244026
        }
      ]
    },
    {
      "doc": "1603.04513.pdf",
      "question": "How does the removal of individual pretrained word embedding versions in the MVCNN model affect sentence classification performance?",
      "prediction": "The removal of individual pretrained word embedding versions in the MVCNN model does not significantly affect sentence classification performance.",
      "answer": "[{'answer': 'each embedding version is crucial for good performance', 'type': 'extractive'}]",
      "evidence": "In the block “versions”, we see that each embedding version is crucial for good performance: performance drops in every single case. ",
      "pages_used": [
        {
          "doc": "1908.06083.pdf",
          "page": 12,
          "matched": false,
          "score": 0.016913319238900635
        }
      ]
    },
    {
      "doc": "2002.01359.pdf",
      "question": "What are the domains and services listed in the Schema-Guided Dialogue State Tracking Task dataset, as described for intents and dialogues across the train, dev, and test sets?",
      "prediction": "The domains and services listed in the Schema-Guided Dialogue State Tracking Task dataset are not provided in the given text.",
      "answer": "[{'answer': 'Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: The total number of intents (services in parentheses) and dialogues for each domain across train1, dev2 and test3 sets. Superscript indicates the datasets in which dialogues from the domain are present. Multi-domain dialogues contribute to counts of each domain. The domain Services includes salons, dentists, doctors, etc.",
      "pages_used": [
        {
          "doc": "2002.01207.pdf",
          "page": 22,
          "matched": false,
          "score": 0.0684931506849315
        }
      ]
    },
    {
      "doc": "1911.11951.pdf",
      "question": "Which stance detection model achieved state-of-the-art weighted accuracy and standard accuracy on the Fake News Challenge Stage 1 (FNC-I) benchmark by leveraging a RoBERTa-based deep bidirectional transformer with bidirectional cross-attention between claim-article pairs?",
      "prediction": "The stance detection model that achieved state-of-the-art weighted accuracy and standard accuracy on the Fake News Challenge Stage 1 (FNC-I) benchmark by leveraging a RoBERTa-based deep bidirectional transformer with bidirectional cross-attention between claim-article pairs is the one described in the paper \"Neural multi-task learning for",
      "answer": "[{'answer': 'To the best of our knowledge, our method achieves state-of-the-art results in weighted-accuracy and standard accuracy on the dataset', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Performance of various methods on the FNC-I benchmark. The first and second groups are methods introduced during and after the challenge period, respectively. Best results are in bold.",
      "pages_used": [
        {
          "doc": "1904.01608.pdf",
          "page": 11,
          "matched": false,
          "score": 0.02077922077922078
        }
      ]
    },
    {
      "doc": "1806.04511.pdf",
      "question": "Which non-English language achieved the highest sentiment analysis accuracy after translation using the RNN model trained on English reviews, as reported in the experimental results with Russian, Spanish, Turkish, and Dutch?",
      "prediction": "The non-English language that achieved the highest sentiment analysis accuracy after translation using the RNN model trained on English reviews, as reported in the experimental results with Russian, Spanish, Turkish, and Dutch, is Dutch.",
      "answer": "[{'answer': 'Russian', 'type': 'extractive'}, {'answer': 'Russsian', 'type': 'abstractive'}]",
      "evidence": "Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages.",
      "pages_used": [
        {
          "doc": "1806.04511.pdf",
          "page": 4,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2002.05829.pdf",
      "question": "Based on the cost benchmarks of the HULK energy efficiency benchmark platform, what is the minimal fine-tuning cost for a model using a 4-core TPU v3 priced at $8/hour?",
      "prediction": "$32 hours",
      "answer": "[{'answer': '$1,728', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Pretraining costs of baseline models. Hardware and pretraining time are collected from original papers, with which costs are estimated with current TPU price at $8 per hour with 4 core TPU v3 chips and V100 GPU at $3.06 per hour. DistilBERT model is trained upon a pretrained BERT model. Parameter numbers are estimated using the pretrained models implemented in the Transformers (https://github.com/huggingface/ transformers) library (Wolf et al., 2019), shown in million.",
      "pages_used": [
        {
          "doc": "2002.05829.pdf",
          "page": 2,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2002.05829.pdf",
      "question": "What pretrained models are evaluated in the HULK benchmark paper for their pretraining costs, using hardware metrics and current TPU and V100 GPU pricing?",
      "prediction": "The HULK benchmark paper evaluates the pretraining costs of several pretrained models, including BERTBASE, BERTLARGE, XLNetBASE, XLNetLARGE, RoBERTABASE, RoBERTALARGE, ALBERTBASE, ALBERTLARGE, ALBERTXLARGE, ALBERTXXLARGE,",
      "answer": "[{'answer': 'BERT, XLNET RoBERTa, ALBERT, DistilBERT', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Pretraining costs of baseline models. Hardware and pretraining time are collected from original papers, with which costs are estimated with current TPU price at $8 per hour with 4 core TPU v3 chips and V100 GPU at $3.06 per hour. DistilBERT model is trained upon a pretrained BERT model. Parameter numbers are estimated using the pretrained models implemented in the Transformers (https://github.com/huggingface/ transformers) library (Wolf et al., 2019), shown in million.",
      "pages_used": [
        {
          "doc": "2002.05829.pdf",
          "page": 2,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1810.00663.pdf",
      "question": "What were the exact match (EM), generalization metric (GM), edit distance (ED), and F1 scores of the proposed model for both the test-repeated and test-new datasets?",
      "prediction": "The exact match (EM), generalization metric (GM), edit distance (ED), and F1 scores of the proposed model for both the test-repeated and test-new datasets are not provided in the given text.",
      "answer": "[{'answer': 'For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol ↑ indicates that higher results are better in the corresponding column; ↓ indicates that lower is better.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.288135593220339
        }
      ]
    },
    {
      "doc": "1902.09314.pdf",
      "question": "What is the exact parameter count and memory size (in MB) of the Attentional Encoder Network (AEN) for targeted sentiment classification?",
      "prediction": "The exact parameter count and memory size (in MB) of the Attentional Encoder Network (AEN) for targeted sentiment classification are not provided in the given text.",
      "answer": "[{'answer': 'Proposed model has 1.16 million parameters and 11.04 MB.', 'type': 'abstractive'}]",
      "evidence": "Statistical results are reported in Table TABREF37 .",
      "pages_used": [
        {
          "doc": "1908.11546.pdf",
          "page": 7,
          "matched": false,
          "score": 0.014962593516209476
        }
      ]
    },
    {
      "doc": "1911.05153.pdf",
      "question": "How do data augmentation techniques (es, cs, cs+es) influence adversarial performance improvements in the task-oriented dialog system, specifically for the systems, and by what percentage do adversarial sets (Adv es, Adv cs) improve compared to the baseline?",
      "prediction": "The data augmentation techniques (es, cs, cs+es) improve adversarial performance in the task-oriented dialog system. Specifically, the adversarial sets (Adv es, Adv cs) improve by 10% compared to the baseline.",
      "answer": "[{'answer': 'Data augmentation (es)  improved Adv es by 20% comparing to baseline \\nData augmentation (cs) improved Adv cs by 16.5% comparing to baseline\\nData augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline \\nAll models show improvements over adversarial sets  \\n', 'type': 'abstractive'}]",
      "evidence": "The performance of the base model described in the previous section is shown in the first row of Table TABREF8 for the Nematus cs-en ($\\bar{cs}$), FB MT system cs-en (cs) and es-en (es), sequence autoencoder (seq2seq), and the average of the adversarial sets (avg).",
      "pages_used": [
        {
          "doc": "1904.10500.pdf",
          "page": 13,
          "matched": false,
          "score": 0.035899481451934584
        }
      ]
    },
    {
      "doc": "1909.04002.pdf",
      "question": "Which 15 celebrities from domains such as politics, business, music, and entertainment provided the dataset of tweets used in the analysis of characterization scores and tweet popularity in the study on identifying a subject's most representative tweets?",
      "prediction": "The document does not provide information about the 15 celebrities from domains such as politics, business, music, and entertainment that provided the dataset of tweets used in the analysis of characterization scores and tweet popularity.",
      "answer": "[{'answer': 'Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,\\nEllen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey', 'type': 'abstractive'}, {'answer': 'Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. ', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Twitter celebrities in our dataset, with tweet counts before and after filtering (Foll. denotes followers in millions)",
      "pages_used": [
        {
          "doc": "2004.01878.pdf",
          "page": 12,
          "matched": false,
          "score": 0.019656019656019656
        }
      ]
    },
    {
      "doc": "1910.14537.pdf",
      "question": "What are the F1 scores reported for the proposed model's performance on the MSR and AS datasets, and how do they compare to the baseline models in the closed test evaluation?",
      "prediction": "The F1 scores for the proposed model's performance on the MSR and AS datasets were not provided in the document. However, it was mentioned that the proposed model outperformed the baseline models in the closed test evaluation.",
      "answer": "[{'answer': 'F1 score of 97.5 on MSR and 95.7 on AS', 'type': 'abstractive'}, {'answer': 'MSR: 97.7 compared to 97.5 of baseline\\nAS: 95.7 compared to 95.6 of baseline', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: Results on PKU and MSR compared with previous models in closed test. The asterisks indicate the result of model with unsupervised label from (Wang et al., 2019).",
      "pages_used": [
        {
          "doc": "1909.00015.pdf",
          "page": 11,
          "matched": false,
          "score": 0.04466501240694789
        }
      ]
    },
    {
      "doc": "1910.14537.pdf",
      "question": "What are the baseline models that the proposed CWS model is compared against in the closed test setting?",
      "prediction": "The proposed CWS model is compared against the baseline models of the ESPnet toolkit and the baseline models of the multilingual stacked bottleneck neural network structure.",
      "answer": "[{'answer': 'Baseline models are:\\n- Chen et al., 2015a\\n- Chen et al., 2015b\\n- Liu et al., 2016\\n- Cai and Zhao, 2016\\n- Cai et al., 2017\\n- Zhou et al., 2017\\n- Ma et al., 2018\\n- Wang et al., 2019', 'type': 'abstractive'}]",
      "evidence": "Tables TABREF25 and TABREF26 reports the performance of recent models and ours in terms of closed test setting.",
      "pages_used": [
        {
          "doc": "1810.03459.pdf",
          "page": 7,
          "matched": false,
          "score": 0.022377622377622378
        }
      ]
    },
    {
      "doc": "1908.07245.pdf",
      "question": "What types of systems are compared in the GlossBERT paper for fine-grained English all-words WSD, including the baseline, knowledge-based, traditional supervised, and neural-based approaches evaluated in the Raganato et al. (2017b) framework?",
      "prediction": "The GlossBERT paper compares the baseline, knowledge-based, traditional supervised, and neural-based approaches for fine-grained English all-words WSD, all of which are evaluated in the Raganato et al. (2017b) framework.",
      "answer": "[{'answer': 'Two knowledge-based systems,\\ntwo traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: F1-score (%) for fine-grained English all-words WSD on the test sets in the framework of Raganato et al. (2017b) (including the development set SE07). Bold font indicates best systems. The five blocks list the MFS baseline, two knowledge-based systems, two traditional word expert supervised systems, six recent neural-based systems and our systems, respectively. Results in first three blocks come from Raganato et al. (2017b), and others from the corresponding papers.",
      "pages_used": [
        {
          "doc": "1906.10225.pdf",
          "page": 16,
          "matched": false,
          "score": 0.25675675675675674
        }
      ]
    },
    {
      "doc": "1902.00672.pdf",
      "question": "How does the TL-TranSum model, incorporating sentence length and hyperedge coverage, perform compared to the MMR (Maximal Marginal Relevance) baseline in terms of redundancy and relevance on the DUC benchmark datasets?",
      "prediction": "The TL-TranSum model, which takes into account sentence length and hyperedge coverage, outperforms the MMR (Maximal Marginal Relevance) baseline in terms of both redundancy and relevance on the DUC benchmark datasets.",
      "answer": "[{'answer': ' Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\\\%$ ) and MRMR ( $7\\\\%$ )', 'type': 'extractive'}]",
      "evidence": "While long sentences are expected to cover more themes and induce a larger increase in the total weight of covered hyperedges, the division by the node weights (i.e. the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of BIBREF31 , tend to favor the selection of long sentences only.",
      "pages_used": [
        {
          "doc": "1902.00672.pdf",
          "page": 14,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1907.03060.pdf",
      "question": "Which baseline models and advanced methods, including PBSMT, NMT, and their variations, did the authors evaluate in addressing the low-resource Japanese--Russian translation problem prior to introducing their back-translation and multi-directional approaches?",
      "prediction": "The authors evaluated various baseline models and advanced methods, including PBSMT, NMT, and their variations, in addressing the low-resource Japanese-Russian translation problem prior to introducing their back-translation and multi-directional approaches.",
      "answer": "[{'answer': 'pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17', 'type': 'extractive'}, {'answer': 'M2M Transformer', 'type': 'abstractive'}]",
      "evidence": "We began with evaluating standard MT paradigms, i.e., PBSMT BIBREF3 and NMT BIBREF1 . As for PBSMT, we also examined two advanced methods: pivot-based translation relying on a helping language BIBREF10 and induction of phrase tables from monolingual data BIBREF14 .\n\nAs for NMT, we compared two types of encoder-decoder architectures: attentional RNN-based model (RNMT) BIBREF2 and the Transformer model BIBREF18 . In addition to standard uni-directional modeling, to cope with the low-resource problem, we examined two multi-directional models: bi-directional model BIBREF11 and multi-to-multi (M2M) model BIBREF8 .\n\nAfter identifying the best model, we also examined the usefulness of a data augmentation method based on back-translation BIBREF17 .",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.25806451612903225
        }
      ]
    },
    {
      "doc": "1707.03569.pdf",
      "question": "What baseline models are listed comparing the performance of different data representations in fine-grained sentiment classification against the multitask learning approach?",
      "prediction": "The baseline models listed for comparison are the Multitask Learning (MTL) approach and the Single-task Learning (STL) approach.",
      "answer": "[{'answer': 'SVMs, LR, BIBREF2', 'type': 'extractive'}, {'answer': 'SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt', 'type': 'extractive'}]",
      "evidence": "Experimental results Table TABREF9 illustrates the performance of the models for the different data representations. The upper part of the Table summarizes the performance of the baselines. The entry “Balikas et al.” stands for the winning system of the 2016 edition of the challenge BIBREF2 , which to the best of our knowledge holds the state-of-the-art.",
      "pages_used": [
        {
          "doc": "2001.00137.pdf",
          "page": 32,
          "matched": false,
          "score": 0.03405865657521287
        }
      ]
    },
    {
      "doc": "1707.03569.pdf",
      "question": "What was the reported MAE reduction achieved by the multitask biLSTM model for fine-grained sentiment classification in comparison to state-of-the-art methods?",
      "prediction": "The multitask biLSTM model achieved a reported MAE reduction of 10% in comparison to state-of-the-art methods for fine-grained sentiment classification.",
      "answer": "[{'answer': 'They decrease MAE in 0.34', 'type': 'abstractive'}]",
      "evidence": "In this work, we use an extended version of LSTM called bidirectional LSTM (biLSTM)",
      "pages_used": [
        {
          "doc": "1909.09484.pdf",
          "page": 10,
          "matched": false,
          "score": 0.2175732217573222
        }
      ]
    },
    {
      "doc": "1809.08298.pdf",
      "question": "What is the total number of run-on and non-run-on sentences mentioned in the paper on using machine learning models for run-on sentence correction?",
      "prediction": "The paper mentions a total of 10 sentences, with 5 being run-on sentences and 5 being non-run-on sentences.",
      "answer": "[{'answer': '4.756 million sentences', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Number of run-on (RO) and non-run-on (Non-RO) sentences in our datasets.",
      "pages_used": [
        {
          "doc": "1906.03538.pdf",
          "page": 16,
          "matched": false,
          "score": 0.1610738255033557
        }
      ]
    },
    {
      "doc": "1912.01673.pdf",
      "question": "What are the 15 modification types detailed in the annotation instructions for sentence transformations in the Czech-based COSTRA 1.0 dataset, which are used to illustrate various sentence-level changes?",
      "prediction": "The 15 modification types detailed in the annotation instructions for sentence transformations in the Czech-based COSTRA 1.0 dataset are: 1. Adding a word, 2. Removing a word, 3. Replacing a word, 4. Adding a phrase, 5. Removing a phrase,",
      "answer": "[{'answer': '- paraphrase 1\\n- paraphrase 2\\n- different meaning\\n- opposite meaning\\n- nonsense\\n- minimal change\\n- generalization\\n- gossip\\n- formal sentence\\n- non-standard sentence\\n- simple sentence\\n- possibility\\n- ban\\n- future\\n- past', 'type': 'abstractive'}]",
      "evidence": "We selected 15 modifications types to collect COSTRA 1.0. They are presented in annotationinstructions.",
      "pages_used": [
        {
          "doc": "1810.12885.pdf",
          "page": 14,
          "matched": false,
          "score": 0.45977011494252873
        }
      ]
    },
    {
      "doc": "1812.06864.pdf",
      "question": "What architecture is reported as state-of-the-art on WSJ, combining HMM with convolutional, recurrent, and fully connected layers?",
      "prediction": "The architecture that is reported as state-of-the-art on WSJ, combining HMM with convolutional, recurrent, and fully connected layers, is not specified in the given text.",
      "answer": "[{'answer': 'CNN-DNN-BLSTM-HMM', 'type': 'abstractive'}, {'answer': 'HMM-based system', 'type': 'extractive'}]",
      "evidence": "Table TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches INLINEFORM0 WER on nov92.",
      "pages_used": [
        {
          "doc": "1904.07904.pdf",
          "page": 6,
          "matched": false,
          "score": 0.25316455696202533
        }
      ]
    },
    {
      "doc": "1909.08824.pdf",
      "question": "What are the absolute improvements in BLEU scores for xIntent, xReact, and oReact on both the Event2Mind and Atomic datasets using the context-aware variational autoencoder model, as reported in comparison to previous state-of-the-art methods?",
      "prediction": "The document does not provide the absolute improvements in BLEU scores for xIntent, xReact, and oReact on both the Event2Mind and Atomic datasets using the context-aware variational autoencoder model.",
      "answer": "[{'answer': 'ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.', 'type': 'abstractive'}]",
      "evidence": "Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. ",
      "pages_used": [
        {
          "doc": "1911.13066.pdf",
          "page": 6,
          "matched": false,
          "score": 0.02751146310962901
        }
      ]
    },
    {
      "doc": "1909.08824.pdf",
      "question": "Which baseline models, including RNN-based Seq2Seq and CWVAE-Unpretrained, are compared with the proposed context-aware variational autoencoder on the Atomic dataset for If-Then reasoning?",
      "prediction": "The baseline models compared with the proposed context-aware variational autoencoder on the Atomic dataset for If-Then reasoning include RNN-based Seq2Seq and CWVAE-Unpretrained.",
      "answer": "[{'answer': 'RNN-based Seq2Seq, Variational Seq2Seq, VRNMT , CWVAE-Unpretrained', 'type': 'extractive'}]",
      "evidence": "We compared our proposed model with the following four baseline methods:\n\nRNN-based Seq2Seq proposed by BIBREF4 (BIBREF4) for the If-Then reasoning on Atomic.\n\nVariational Seq2Seq combines a latent variable with the encoder-decoder structure through converting the last hidden state of RNN encoder into a Gaussian distributed latent variable BIBREF8.\n\nVRNMT Propose by BIBREF19 (BIBREF19), VRNMT combines CVAE with attention-based encoder-decoder framework through introduces a latent variable to model the semantic distribution of targets.\n\nCWVAE-Unpretrained refers to the CWVAE model without the pretrain stage.\n\nNote that, for each baseline method, we train distinct models for each distinct inference dimension, respectively.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.3103448275862069
        }
      ]
    },
    {
      "doc": "1902.10525.pdf",
      "question": "Based on the results, which language shows the smallest reduction in character error rate when applying all components of the system?",
      "prediction": "Chinese",
      "answer": "[{'answer': 'thai', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 9 Character error rates on the validation data using successively more of the system components described above for English (en), Spanish (es), German (de), Arabic (ar), Korean (ko), Thai (th), Hindi (hi), and Chinese (zh) along with the respective number of items and characters in the test sets. Average latencies for all languages and models were computed on an Intel Xeon E5-2690 CPU running at 2.6GHz.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.20610687022900764
        }
      ]
    },
    {
      "doc": "1804.00079.pdf",
      "question": "What are the specific datasets from which sentence pairs were drawn for each task in the multi-task learning framework, according to the paper?",
      "prediction": "The paper does not provide specific information about the datasets used for each task in the multi-task learning framework.",
      "answer": "[{'answer': '- En-Fr (WMT14)\\n- En-De (WMT15)\\n- Skipthought (BookCorpus)\\n- AllNLI (SNLI + MultiNLI)\\n- Parsing (PTB + 1-billion word)', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: An approximate number of sentence pairs for each task.",
      "pages_used": [
        {
          "doc": "2002.01207.pdf",
          "page": 22,
          "matched": false,
          "score": 0.09042553191489362
        }
      ]
    },
    {
      "doc": "1908.07195.pdf",
      "question": "Based on the automatic evaluation results, how does ARAML improve in reverse perplexity, Self-BLEU, grammaticality, and relevance compared to baseline methods on the COCO and EMNLP2017 WMT datasets?",
      "prediction": "The document does not provide specific information about how ARLAM improves in reverse perplexity, Self-BLEU, grammaticality, and relevance compared to baseline methods on the COCO and EMNLP2017 WMT datasets.",
      "answer": "[{'answer': 'ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.', 'type': 'abstractive'}, {'answer': 'Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Automatic evaluation on COCO and EMNLP2017 WMT. Each metric is presented with mean and standard deviation.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.2719298245614035
        }
      ]
    },
    {
      "doc": "1805.03710.pdf",
      "question": "Which language's subword segmentations, including examples like \"〈hell, o, o, o〉\" and \"〈louis, ana〉,\" are used to generate word embeddings based on cosine distance in the paper?",
      "prediction": "The paper uses English subword segmentations to generate word embeddings based on cosine distance.",
      "answer": "[{'answer': 'English', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: We generate vectors for OOV using subword information and search for the nearest (cosine distance) words in the embedding space. The LV-M segmentation for each word is: {〈hell, o, o, o〉}, {〈marvel, i, cious〉}, {〈louis, ana〉}, {〈re, re, read〉}, {〈 tu, z, read〉}. We omit the LV-N and FT n-grams as they are trivial and too numerous to list.",
      "pages_used": [
        {
          "doc": "1805.03710.pdf",
          "page": 4,
          "matched": false,
          "score": 0.06896551724137931
        }
      ]
    },
    {
      "doc": "2003.08385.pdf",
      "question": "Does the paper list the macro-average F1 scores for the 'favor' and 'against' classes in a zero-shot cross-lingual stance detection setting involving German, French, and Italian languages?",
      "prediction": "No, the paper does not list the macro-average F1 scores for the 'favor' and 'against' classes in a zero-shot cross-lingual stance detection setting involving German, French, and Italian languages.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 3: Baseline scores in the cross-lingual setting. No Italian samples were seen during training, making this a case of zero-shot cross-lingual transfer. The scores are reported as the macro-average of the F1scores for ‘favor’ and for ‘against’.",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.3142857142857143
        }
      ]
    },
    {
      "doc": "2003.08385.pdf",
      "question": "What F1 macro score and accuracy did M-BERT achieve on the supervised stance detection benchmarks in the X-Stance paper?",
      "prediction": "M-BERT achieved an F1 macro score of 80.0 and an accuracy of 81.0 on the supervised stance detection benchmarks in the X-Stance paper.",
      "answer": "[{'answer': 'M-Bert had 76.6 F1 macro score.', 'type': 'abstractive'}, {'answer': '75.1% and 75.6% accuracy', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 6: Performance of BERT-like models on different supervised stance detection benchmarks.",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 13,
          "matched": false,
          "score": 0.248
        }
      ]
    },
    {
      "doc": "2003.08385.pdf",
      "question": "What was the F1 macro score that multilingual BERT achieved on the Swiss election comments dataset for multilingual multi-target stance detection detailed in the X-Stance paper?",
      "prediction": "The F1 macro score that multilingual BERT achieved on the Swiss election comments dataset for multilingual multi-target stance detection detailed in the X-Stance paper was 0.65.",
      "answer": "[{'answer': 'BERT had 76.6 F1 macro score on x-stance dataset.', 'type': 'abstractive'}]",
      "evidence": "To put the supervised score into context we list scores that variants of Bert have achieved on other stance detection datasets in Table TABREF46.",
      "pages_used": [
        {
          "doc": "2001.06888.pdf",
          "page": 13,
          "matched": false,
          "score": 0.25862068965517243
        }
      ]
    },
    {
      "doc": "1908.11365.pdf",
      "question": "Does the model with DS-Init and MAtt have fewer parameters than the baseline Transformer for the WMT14 En-De translation task?",
      "prediction": "Yes",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 3: Tokenized case-sensitive BLEU (in parentheses: sacreBLEU) on WMT14 En-De translation task. #Param: number of model parameters. 4Dec: decoding time (seconds)/speedup on newstest2014 dataset with a batch size of 32. 4Train: training time (seconds)/speedup per training step evaluated on 0.5K steps with a batch size of 1K target tokens. Time is averaged over 3 runs using Tensorflow on a single TITAN X (Pascal). “-”: optimization failed and no result. “?”: the same as model 1©. † and ‡: comparison against 11© and 14© respectively rather than 1©. Base: the baseline Transformer with base setting. Bold indicates best BLEU score. dpa and dpr: dropout rate on attention weights and residual connection. bs: batch size in tokens.",
      "pages_used": [
        {
          "doc": "1603.00968.pdf",
          "page": 7,
          "matched": false,
          "score": 0.04434589800443459
        }
      ]
    },
    {
      "doc": "1709.10217.pdf",
      "question": "Based on the evaluation results, what were the highest F1 scores for Task 1 and the top performance metrics for Task 2, including Ratio, Satisfaction, Fluency, Turns, and Guide, in the iFLYTEK Corporation's evaluation of Chinese human-computer dialogue technology?",
      "prediction": "The highest F1 score for Task 1 was 96.52, and the top performance metrics for Task 2 were Ratio, Satisfaction, Fluency, Turns, and Guide.",
      "answer": "[{'answer': 'For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.\\nFor task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2', 'type': 'abstractive'}]",
      "evidence": "Table TABREF14 and TABREF15 show the evaluation results of the closed test and open test of the task 1 respectively.",
      "pages_used": [
        {
          "doc": "1909.08041.pdf",
          "page": 12,
          "matched": false,
          "score": 0.025614754098360656
        }
      ]
    },
    {
      "doc": "1705.00108.pdf",
      "question": "What is the micro-averaged evaluation metric officially reported in the results for assessing the performance of the semi-supervised bidirectional language model on named entity recognition and chunking tasks?",
      "prediction": "The micro-averaged evaluation metric officially reported in the results for assessing the performance of the semi-supervised bidirectional language model on named entity recognition and chunking tasks is the micro-averaged F1 score.",
      "answer": "[{'answer': 'micro-averaged F1', 'type': 'abstractive'}]",
      "evidence": "We report the official evaluation metric (micro-averaged INLINEFORM0 ). ",
      "pages_used": [
        {
          "doc": "2004.03744.pdf",
          "page": 9,
          "matched": false,
          "score": 0.017605633802816902
        }
      ]
    },
    {
      "doc": "1705.00108.pdf",
      "question": "In the comparison of sequence tagging models without incorporating additional labeled data or task-specific gazetteers, which systems are listed in the semi-supervised sequence tagging with bidirectional language models paper?",
      "prediction": "The systems listed in the paper are CRF++, LSTMs, and BiLSTMs.",
      "answer": "[{'answer': 'Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ', 'type': 'abstractive'}]",
      "evidence": "Tables TABREF15 and TABREF16 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare results of TagLM to other systems that include additional labeled data or gazetteers. ",
      "pages_used": [
        {
          "doc": "1905.10810.pdf",
          "page": 5,
          "matched": false,
          "score": 0.2890625
        }
      ]
    },
    {
      "doc": "1905.11901.pdf",
      "question": "What were the different German-English training data sizes, including subsets of the IWSLT14 corpus, used in the TED shared translation task experiments, and how was the development set split in this study?",
      "prediction": "The different German-English training data sizes used in the TED shared translation task experiments, including subsets of the IWSLT14 corpus, were not specified in the provided information. However, it was mentioned that the development set was split into two subsets.",
      "answer": "[{'answer': 'Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development', 'type': 'abstractive'}, {'answer': 'ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)', 'type': 'extractive'}]",
      "evidence": "We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.",
      "pages_used": [
        {
          "doc": "1611.04642.pdf",
          "page": 14,
          "matched": false,
          "score": 0.03125
        }
      ]
    },
    {
      "doc": "1909.00694.pdf",
      "question": "What are the accuracy results of the BiGRU and BERT models on the ACP test set when trained with different configurations, including the full dataset and subsets, in the paper titled \"Minimally Supervised Learning of Affective Events Using Discourse Relations\"?",
      "prediction": "The accuracy results of the BiGRU and BERT models on the ACP test set when trained with different configurations, including the full dataset and subsets, are not provided in the given text.",
      "answer": "[{'answer': 'Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \\nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.",
      "pages_used": [
        {
          "doc": "1909.00694.pdf",
          "page": 8,
          "matched": false,
          "score": 0.010120177103099304
        }
      ]
    },
    {
      "doc": "1909.00694.pdf",
      "question": "How many affective event pairs were extracted from both the Japanese web corpus and the ACP corpus in your minimally supervised learning of affective events via discourse relations?",
      "prediction": "1000 affective event pairs were extracted from both the Japanese web corpus and the ACP corpus.",
      "answer": "[{'answer': '7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus', 'type': 'abstractive'}, {'answer': 'The ACP corpus has around 700k events split into positive and negative polarity ', 'type': 'abstractive'}]",
      "evidence": "As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ",
      "pages_used": [
        {
          "doc": "2002.02492.pdf",
          "page": 15,
          "matched": false,
          "score": 0.1503267973856209
        }
      ]
    },
    {
      "doc": "1909.00694.pdf",
      "question": "What is the percentage improvement in BERT's performance when using the minimally supervised method that propagates affective polarity through discourse relations, when trained on a small set of labeled Japanese data compared to the baseline supervised approach?",
      "prediction": "The document does not provide information about the percentage improvement in BERT's performance.",
      "answer": "[{'answer': '3%', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.18421052631578946
        }
      ]
    },
    {
      "doc": "1706.08032.pdf",
      "question": "What were the accuracy results reported for the STS, Sanders, and HCR datasets when evaluated using the proposed DeepCNN-BiLSTM model for binary sentiment classification?",
      "prediction": "The accuracy results reported for the STS, Sanders, and HCR datasets when evaluated using the proposed DeepCNN-BiLSTM model for binary sentiment classification were 80%, 75%, and 85% respectively.",
      "answer": "[{'answer': 'accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.2903225806451613
        }
      ]
    },
    {
      "doc": "1706.08032.pdf",
      "question": "Which semantic rules, as referenced in the Twitter sentiment classification model, are excluded from the main method because they are used to compute polarity after POS tagging or parsing steps?",
      "prediction": "The semantic rules that are excluded from the main method because they are used to compute polarity after POS tagging or parsing steps are not specified in the given information.",
      "answer": "[{'answer': 'rules that compute polarity of words after POS tagging or parsing steps', 'type': 'abstractive'}]",
      "evidence": "In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like \"but, while, however, despite, however\" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:\n\n@lonedog bwahahah...you are amazing! However, it was quite the letdown.\n\n@kirstiealley my dentist is great but she's expensive...=(\n\nIn two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.",
      "pages_used": [
        {
          "doc": "1706.08032.pdf",
          "page": 1,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2003.12218.pdf",
      "question": "Does the paper provide a complete list of all 75 fine-grained entity types annotated within the CORD-NER dataset?",
      "prediction": "No",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 2: Examples of the most frequent entities annotated in CORD-NER.",
      "pages_used": [
        {
          "doc": "2003.12218.pdf",
          "page": 4,
          "matched": false,
          "score": 0.041947565543071164
        }
      ]
    },
    {
      "doc": "1909.00512.pdf",
      "question": "What method is used in the paper to compute static embeddings from the contextualized word representations across different layers of ELMo, BERT, and GPT-2?",
      "prediction": "The paper uses a method called average pooling to compute static embeddings from the contextualized word representations across different layers of ELMo, BERT, and GPT-2.",
      "answer": "[{'answer': \"They use the first principal component of a word's contextualized representation in a given layer as its static embedding.\", 'type': 'abstractive'}, {'answer': ' by taking the first principal component (PC) of its contextualized representations in a given layer', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: The performance of various static embeddings on word embedding benchmark tasks. The best result for each task is in bold. For the contextualizing models (ELMo, BERT, GPT-2), we use the first principal component of a word’s contextualized representations in a given layer as its static embedding. The static embeddings created using ELMo and BERT’s contextualized representations often outperform GloVe and FastText vectors.",
      "pages_used": [
        {
          "doc": "2001.05970.pdf",
          "page": 6,
          "matched": false,
          "score": 0.05517241379310345
        }
      ]
    },
    {
      "doc": "1909.11467.pdf",
      "question": "What are the 12 educational subjects listed in the Kurdish Textbooks Corpus (KTC) paper, specifically related to the Sorani dialect's K-12 textbooks, with 693,800 tokens categorized into these subjects?",
      "prediction": "The 12 educational subjects listed in the Kurdish Textbooks Corpus (KTC) paper, specifically related to the Sorani dialect's K-12 textbooks, with 693,800 tokens categorized into these subjects are: Arabic, English, Kurdish, Mathematics, Physics, Chemistry, Biology, History",
      "answer": "[{'answer': 'Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Statistics of the corpus - In the Course Level column, (i) represents Institute2 .",
      "pages_used": [
        {
          "doc": "1909.09484.pdf",
          "page": 10,
          "matched": false,
          "score": 0.2490272373540856
        }
      ]
    },
    {
      "doc": "1709.05413.pdf",
      "question": "Are all Twitter customer service conversations used in the evaluation datasets focused exclusively on English?",
      "prediction": "Yes, all Twitter customer service conversations used in the evaluation datasets are focused exclusively on English.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 1: Example Twitter Customer Service Conversation",
      "pages_used": [
        {
          "doc": "1709.05413.pdf",
          "page": 13,
          "matched": false,
          "score": 0.026490066225165563
        }
      ]
    },
    {
      "doc": "1908.11546.pdf",
      "question": "How does the gCAS model's performance in terms of Entity F1 and Success F1 compare across the movie, taxi, and restaurant domains, and in which domains does it particularly excel?",
      "prediction": "The gCAS model's performance in terms of Entity F1 and Success F1 varies across the movie, taxi, and restaurant domains. It particularly excels in the movie domain.",
      "answer": "[{'answer': 'For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: Entity F1 and Success F1 at dialogue level.",
      "pages_used": [
        {
          "doc": "1705.01265.pdf",
          "page": 7,
          "matched": false,
          "score": 0.04943820224719101
        }
      ]
    },
    {
      "doc": "1711.11221.pdf",
      "question": "What specific evaluation metrics, such as BLEU scores and coherence-related measures, were used to assess the proposed dynamic and topic cache models on the NIST Chinese-English translation tasks?",
      "prediction": "The specific evaluation metrics used to assess the proposed dynamic and topic cache models on the NIST Chinese-English translation tasks were BLEU scores and coherence-related measures.",
      "answer": "[{'answer': 'BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Experiment results on the NIST Chinese-English translation tasks. [+Cd] is the proposed model with the dynamic cache. [+Cd,Ct] is the proposed model with both the dynamic and topic cache. The BLEU scores are case-insensitive. Avg means the average BLEU score on all test sets.",
      "pages_used": [
        {
          "doc": "1906.10225.pdf",
          "page": 16,
          "matched": false,
          "score": 0.3425414364640884
        }
      ]
    },
    {
      "doc": "1909.05855.pdf",
      "question": "What are the 17 domains, including the one excluded from training, in the Schema-Guided Dialogue Dataset paper?",
      "prediction": "The 17 domains, including the one excluded from training, in the Schema-Guided Dialogue Dataset paper are: 1. Accommodation, 2. Attractions, 3. Flights, 4. Restaurants, 5. Shopping, 6. Travel, 7. Weather, 8",
      "answer": "[{'answer': 'Alarm\\nBank\\nBus\\nCalendar\\nEvent\\nFlight\\nHome\\nHotel\\nMedia\\nMovie\\nMusic\\nRentalCar\\nRestaurant\\nRideShare\\nService\\nTravel\\nWeather', 'type': 'abstractive'}]",
      "evidence": "The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. ",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.36123348017621143
        }
      ]
    },
    {
      "doc": "2002.01664.pdf",
      "question": "What are the 7 Indian languages listed in the paper that implements Ghost-VLAD pooling for language identification experiments?",
      "prediction": "The 7 Indian languages listed in the paper are Bengali, Gujarati, Hindi, Marathi, Malayalam, Tamil, and Telugu.",
      "answer": "[{'answer': 'Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam', 'type': 'abstractive'}, {'answer': 'Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Dataset",
      "pages_used": [
        {
          "doc": "1903.00172.pdf",
          "page": 13,
          "matched": false,
          "score": 0.24742268041237114
        }
      ]
    },
    {
      "doc": "1804.08139.pdf",
      "question": "What accuracy metrics are presented to assess the performance of the proposed multi-task learning model on each of the 16 text classification tasks and as an overall average across all tasks?",
      "prediction": "The accuracy metrics presented to assess the performance of the proposed multi-task learning model on each of the 16 text classification tasks and as an overall average across all tasks are not specified in the image.",
      "answer": "[{'answer': 'Accuracy on each dataset and the average accuracy on all datasets.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF34 shows the performances of the different methods.",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 17,
          "matched": false,
          "score": 0.2765957446808511
        }
      ]
    },
    {
      "doc": "1611.03382.pdf",
      "question": "What is the percentage improvement of the model presented over the state-of-the-art systems on the Gigaword dataset in terms of both Rouge-1 and Rouge-L scores?",
      "prediction": "The model presented in the document shows a 10% improvement in terms of both Rouge-1 and Rouge-L scores over the state-of-the-art systems on the Gigaword dataset.",
      "answer": "[{'answer': 'w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%', 'type': 'abstractive'}]",
      "evidence": "As shown in Table TABREF35 , our method outperforms all previous methods on Rouge-1 and Rouge-L, and is comparable on Rouge-2.",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.21232876712328766
        }
      ]
    },
    {
      "doc": "1905.00563.pdf",
      "question": "What are the specific knowledge graphs that were used to evaluate the performance and robustness of the link prediction models in this study?",
      "prediction": "The specific knowledge graphs used to evaluate the performance and robustness of the link prediction models in this study are not provided in the given text.",
      "answer": "[{'answer': ' Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ', 'type': 'abstractive'}, {'answer': 'WN18 and YAGO3-10', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Data Statistics of the benchmarks.",
      "pages_used": [
        {
          "doc": "2002.05058.pdf",
          "page": 9,
          "matched": false,
          "score": 0.022727272727272728
        }
      ]
    },
    {
      "doc": "1901.09755.pdf",
      "question": "Which specific datasets, including those from the ABSA restaurants domain for languages like English, Spanish, French, and Russian, are detailed with token counts, target counts, and multiword targets?",
      "prediction": "The ABSA SemEval 2014-2016 datasets for the restaurant domain are detailed with token counts, target counts, and multiword targets for languages like English, Spanish, French, and Russian.",
      "answer": "[{'answer': 'ABSA SemEval 2014-2016 datasets\\nYelp Academic Dataset\\nWikipedia dumps', 'type': 'abstractive'}]",
      "evidence": "Table TABREF7 shows the ABSA datasets from the restaurants domain for English, Spanish, French, Dutch, Russian and Turkish. From left to right each row displays the number of tokens, number of targets and the number of multiword targets for each training and test set. For English, it should be noted that the size of the 2015 set is less than half with respect to the 2014 dataset in terms of tokens, and only one third in number of targets. The French, Spanish and Dutch datasets are quite similar in terms of tokens although the number of targets in the Dutch dataset is comparatively smaller, possibly due to the tendency to construct compound terms in that language. The Russian dataset is the largest whereas the Turkish set is by far the smallest one.",
      "pages_used": [
        {
          "doc": "1901.09755.pdf",
          "page": 6,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1711.00106.pdf",
      "question": "Based on the paper’s ablation study, how much improvement is achieved in exact match (EM) and F1 scores when using the mixed objective, compared to only using cross-entropy, on the SQuAD development set?",
      "prediction": "The paper does not provide specific information about the improvement in exact match (EM) and F1 scores when using the mixed objective compared to only using cross-entropy on the SQuAD development set.",
      "answer": "[{'answer': 'The mixed objective improves EM by 2.5% and F1 by 2.2%', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Ablation study on the development set of SQuAD.",
      "pages_used": [
        {
          "doc": "1811.02906.pdf",
          "page": 10,
          "matched": false,
          "score": 0.043010752688172046
        }
      ]
    },
    {
      "doc": "1703.07090.pdf",
      "question": "What are the character error rates (CER) for the 9-layer, 2-layer regular-trained, and 2-layer distilled LSTM models as reported in the paper's experiments on Shenma voice search data?",
      "prediction": "The character error rates (CER) for the 9-layer, 2-layer regular-trained, and 2-layer distilled LSTM models are not provided in the given text.",
      "answer": "[{'answer': '2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.', 'type': 'abstractive'}, {'answer': 'Their best model achieved a 2.49% Character Error Rate.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3. The CER and RTF of 9-layers, 2-layers regular-trained and 2-laryers distilled LSTM.",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.291497975708502
        }
      ]
    },
    {
      "doc": "1703.07090.pdf",
      "question": "What were the different layer configurations of the unidirectional LSTM models specifically evaluated for real-time streaming speech recognition in the study?",
      "prediction": "The study evaluated unidirectional LSTM models with different layer configurations for real-time streaming speech recognition, but the specific configurations are not provided in the given text.",
      "answer": "[{'answer': 'Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.', 'type': 'abstractive'}]",
      "evidence": "Unidirectional LSTM network is applied, rather than bidirectional one, because it is well suited to real-time streaming speech recognition.",
      "pages_used": [
        {
          "doc": "1904.01608.pdf",
          "page": 11,
          "matched": false,
          "score": 0.02236024844720497
        }
      ]
    },
    {
      "doc": "2003.06044.pdf",
      "question": "How do previous methods perform on the Switchboard Dialogue Act and DailyDialog datasets compared to the proposed hierarchical attention-based model’s reported accuracies of 80.34% and 85.81%?",
      "prediction": "The previous methods did not perform as well as the proposed hierarchical attention-based model on the Switchboard Dialogue Act and DailyDialog datasets.",
      "answer": "[{'answer': 'Table TABREF20 , Table TABREF22, Table TABREF23', 'type': 'extractive'}]",
      "evidence": "We evaluate the performance of our model on two high-quality datasets: Switchboard Dialogue Act Corpus (SwDA) BIBREF4 and DailyDialog BIBREF24. ",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.18791946308724833
        }
      ]
    },
    {
      "doc": "2003.06044.pdf",
      "question": "Which baseline methods are compared against the proposed hierarchical self-attention-based model for the Switchboard Dialogue Act (SwDA) dataset?",
      "prediction": "The baseline methods compared against the proposed hierarchical self-attention-based model for the Switchboard Dialogue Act (SwDA) dataset are the baseline methods from the original paper.",
      "answer": "[{'answer': 'BLSTM+Attention+BLSTM\\nHierarchical BLSTM-CRF\\nCRF-ASN\\nHierarchical CNN (window 4)\\nmLSTM-RNN\\nDRLM-Conditional\\nLSTM-Softmax\\nRCNN\\nCNN\\nCRF\\nLSTM\\nBERT', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Comparison results with the previous approaches and our approaches on SwDA dataset.",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.24354243542435425
        }
      ]
    },
    {
      "doc": "1909.09270.pdf",
      "question": "What F1 score is reported for the Bengali NER corpus annotated by non-speakers using noisy and incomplete data, as highlighted by the authors' improvement over the previous state of the art?",
      "prediction": "The authors report an F1 score of 85.5 for the Bengali NER corpus annotated by non-speakers using noisy and incomplete data, which represents an improvement over the previous state of the art.",
      "answer": "[{'answer': '52.0%', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: Bengali manual annotation results. Our methods improve on state of the art scores by over 5 points F1 given a relatively small amount of noisy and incomplete annotations from non-speakers.",
      "pages_used": [
        {
          "doc": "2002.05058.pdf",
          "page": 9,
          "matched": false,
          "score": 0.019662921348314606
        }
      ]
    },
    {
      "doc": "1909.08041.pdf",
      "question": "Which baseline systems and corresponding authors are compared against in the evaluation on both the HotpotQA and FEVER datasets in the \"Revealing the Importance of Semantic Retrieval for Machine Reading at Scale\" paper?",
      "prediction": "The baseline systems and corresponding authors compared against in the evaluation on both the HotpotQA and FEVER datasets in the \"Revealing the Importance of Semantic Retrieval for Machine Reading at Scale\" paper are Sennan and Stanf.",
      "answer": "[{'answer': 'HotspotQA: Yang, Ding, Muppet\\nFever: Hanselowski, Yoneda, Nie', 'type': 'abstractive'}]",
      "evidence": "We chose the best system based on the dev set, and used that for submitting private test predictions on both FEVER and HotpotQA .",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.24324324324324326
        }
      ]
    },
    {
      "doc": "1909.08041.pdf",
      "question": "How does the effectiveness of sentence-level versus paragraph-level neural retrieval differ in terms of their impact on FEVER Score, Label Accuracy, and classification F1 scores across the SUPPORT, REFUTE, and NOT ENOUGH INFO labels?",
      "prediction": "The effectiveness of sentence-level versus paragraph-level neural retrieval varies across FEVER Score, Label Accuracy, and classification F1 scores for the SUPPORT, REFUTE, and NOT ENOUGH INFO labels.",
      "answer": "[{'answer': 'This seems to indicate that the downstream QA module relies more on the upstream paragraph-level retrieval whereas the verification module relies more on the upstream sentence-level retrieval.', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Ablation over the paragraph-level and sentence-level neural retrieval sub-modules on FEVER. “LA”=Label Accuracy; “FS”=FEVER Score; “Orcl.” is the oracle upperbound of FEVER Score assuming all downstream modules are perfect. “L-F1 (S/R/N)” means the classification f1 scores on the three verification labels: SUPPORT, REFUTE, and NOT ENOUGH INFO.",
      "pages_used": [
        {
          "doc": "1909.08041.pdf",
          "page": 7,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1908.06083.pdf",
      "question": "What evaluation metrics are reported for the multi-turn adversarial task, specifically comparing models that incorporate dialogue context with BERT dialogue segment features?",
      "prediction": "The evaluation metrics reported for the multi-turn adversarial task include BLEU, ROUGE, and METEOR.",
      "answer": "[{'answer': 'F1 and Weighted-F1', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 10: Results of experiments on the multi-turn adversarial task. We denote the average and one standard deviation from the results of five runs. Models that use the context as input (“with context”) perform better. Encoding this in the architecture as well (via BERT dialogue segment features) gives us the best results.",
      "pages_used": [
        {
          "doc": "1908.06083.pdf",
          "page": 12,
          "matched": false,
          "score": 0.06557377049180328
        }
      ]
    },
    {
      "doc": "1910.06592.pdf",
      "question": "What are the total number of Twitter accounts and tweets in the dataset specifically used for detecting fake news on Twitter through neural recurrent models and timeline-based profiling?",
      "prediction": "The dataset used for detecting fake news on Twitter through neural recurrent models and timeline-based profiling contains 100 Twitter accounts and 1000 tweets.",
      "answer": "[{'answer': 'Total dataset size: 171 account (522967 tweets)', 'type': 'abstractive'}, {'answer': '212 accounts', 'type': 'abstractive'}]",
      "evidence": "Table TABREF13 presents statistics on our dataset.",
      "pages_used": [
        {
          "doc": "1806.07711.pdf",
          "page": 9,
          "matched": false,
          "score": 0.016638935108153077
        }
      ]
    },
    {
      "doc": "1910.03467.pdf",
      "question": "Is the supervised morphological learner evaluated on Japanese data in the Japanese-Vietnamese translation experiments outlined in this paper?",
      "prediction": "Yes, the supervised morphological learner is evaluated on Japanese data in the Japanese-Vietnamese translation experiments outlined in this paper.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "We conduct two out of the three proposed approaches for Japanese-Vietnamese translation systems and the results are given in the Table TABREF15.",
      "pages_used": [
        {
          "doc": "1909.13375.pdf",
          "page": 7,
          "matched": false,
          "score": 0.029166666666666667
        }
      ]
    },
    {
      "doc": "1912.10806.pdf",
      "question": "What is the mean prediction accuracy achieved by the DP-LSTM model for S&P 500 stock predictions in the paper?",
      "prediction": "The mean prediction accuracy achieved by the DP-LSTM model for S&P 500 stock predictions in the paper is 0.981582666.",
      "answer": "[{'answer': 'mean prediction accuracy 0.99582651\\nS&P 500 Accuracy 0.99582651', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Predicted Mean MPA results.",
      "pages_used": [
        {
          "doc": "1912.10806.pdf",
          "page": 7,
          "matched": false,
          "score": 0.02435723951285521
        }
      ]
    },
    {
      "doc": "1911.01680.pdf",
      "question": "What are the train, dev, and test set sizes for the Onsei Intent Slot dataset in the slot filling experiments?",
      "prediction": "The train set size is 13,084, the dev set size is 700, and the test set size is 700.",
      "answer": "[{'answer': 'Dataset has 1737 train, 497 dev and 559 test sentences.', 'type': 'abstractive'}]",
      "evidence": "In our experiments, we use Onsei Intent Slot dataset. Table TABREF21 shows the statics of this dataset.",
      "pages_used": [
        {
          "doc": "1911.01680.pdf",
          "page": 6,
          "matched": false,
          "score": 0.0736
        }
      ]
    },
    {
      "doc": "1906.01081.pdf",
      "question": "Based on the data, how much higher is the correlation of the PARENT metric with human judgments compared to the best-performing baseline on both the WikiBio and WebNLG datasets, and what are the specific improvements in correlation values?",
      "prediction": "The correlation of the PARENT metric with human judgments is higher compared to the best-performing baseline on both the WikiBio and WebNLG datasets. The specific improvements in correlation values are not provided in the given data.",
      "answer": "[{'answer': 'Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.', 'type': 'abstractive'}, {'answer': 'Their average correlation tops the best other model by 0.155 on WikiBio.', 'type': 'abstractive'}]",
      "evidence": "We report the average correlation across all bootstrap samples for each metric in Table TABREF37 .",
      "pages_used": [
        {
          "doc": "1806.07711.pdf",
          "page": 9,
          "matched": false,
          "score": 0.018518518518518517
        }
      ]
    },
    {
      "doc": "2004.01980.pdf",
      "question": "In the human evaluation results, TitleStylist outperforms which summarization model by 9.68% in terms of attraction?",
      "prediction": "TitleStylist outperforms the baseline summarization model by 9.68% in terms of attraction.",
      "answer": "[{'answer': 'pure summarization model NHG', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Human evaluation on three aspects: relevance, attraction, and fluency. “None” represents the original headlines in the dataset.",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.2648401826484018
        }
      ]
    },
    {
      "doc": "2004.01980.pdf",
      "question": "What are the percentage improvements in relevance, attraction, and fluency for humor-styled headlines generated using the TitleStylist method compared to the multitask baseline, as reported in the human evaluation results?",
      "prediction": "The document does not provide the specific percentage improvements in relevance, attraction, and fluency for humor-styled headlines generated using the TitleStylist method compared to the multitask baseline.",
      "answer": "[{'answer': 'Humor in headlines (TitleStylist vs Multitask baseline):\\nRelevance: +6.53% (5.87 vs 5.51)\\nAttraction: +3.72% (8.93 vs 8.61)\\nFluency: 1,98% (9.29 vs 9.11)', 'type': 'abstractive'}]",
      "evidence": "We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57.",
      "pages_used": [
        {
          "doc": "1905.10810.pdf",
          "page": 5,
          "matched": false,
          "score": 0.2857142857142857
        }
      ]
    },
    {
      "doc": "1603.07044.pdf",
      "question": "What are the exact MAP score differences between the neural attention-based RNN encoder model and the handcrafted feature-based approach for Tasks A, B, and C as reported in the SemEval-2016 cQA experiments?",
      "prediction": "The exact MAP score differences between the neural attention-based RNN encoder model and the handcrafted feature-based approach for Tasks A, B, and C as reported in the SemEval-2016 cQA experiments are not provided in the given text.",
      "answer": "[{'answer': '0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Compared with other systems (bold is best).",
      "pages_used": [
        {
          "doc": "2003.03044.pdf",
          "page": 4,
          "matched": false,
          "score": 0.020512820512820513
        }
      ]
    },
    {
      "doc": "2002.00652.pdf",
      "question": "How much improvement in Query Matching and Interaction Matching does the Turn+SQL Attn+Action Copy model in this paper achieve over the previous state-of-the-art on the SParC dataset without using BERT?",
      "prediction": "The Turn+SQL Attn+Action Copy model in this paper achieves a 10% improvement in Query Matching and Interaction Matching over the previous state-of-the-art on the SParC dataset without using BERT.",
      "answer": "[{'answer': 'Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.', 'type': 'extractive'}]",
      "evidence": "EditSQL BIBREF5 is a STOA baseline which mainly makes use of SQL attention and token-level copy (analogous to our Turn+SQL Attn+Action Copy).",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.24915824915824916
        }
      ]
    },
    {
      "doc": "1909.00105.pdf",
      "question": "What specific metrics are reported for evaluating distinctness, user matching accuracy, and preference alignment in generated personalized recipes within this study?",
      "prediction": "The study does not report specific metrics for evaluating distinctness, user matching accuracy, and preference alignment in generated personalized recipes.",
      "answer": "[{'answer': 'Byte-Pair Encoding perplexity  (BPE PPL),\\nBLEU-1,\\nBLEU-4,\\nROUGE-L,\\npercentage of distinct unigram (D-1),\\npercentage of distinct bigrams(D-2),\\nuser matching accuracy(UMA),\\nMean Reciprocal Rank(MRR)\\nPairwise preference over baseline(PP)', 'type': 'abstractive'}, {'answer': 'BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence', 'type': 'extractive'}, {'answer': ' Distinct-1/2, UMA = User Matching Accuracy, MRR\\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Metrics on generated recipes from test set. D-1/2 = Distinct-1/2, UMA = User Matching Accuracy, MRR = Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model).",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 17,
          "matched": false,
          "score": 0.3129251700680272
        }
      ]
    },
    {
      "doc": "2003.03014.pdf",
      "question": "Does the paper specifically analyze how the term \"homosexual\" has come to be more strongly associated with dehumanizing language compared to \"gay and lesbian,\" particularly in the context of the Gallup survey's change in wording?",
      "prediction": "Yes, the paper does specifically analyze how the term \"homosexual\" has come to be more strongly associated with dehumanizing language compared to \"gay and lesbian,\" particularly in the context of the Gallup survey's change in wording.",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": "The Gallup survey asked for opinions on legality of “homosexual relations\" until 2008, but then changed the wording to “gay and lesbian relations\". This was likely because many people who identify as gay and lesbian find the word homosexual to be outdated and derogatory.",
      "pages_used": [
        {
          "doc": "2003.03014.pdf",
          "page": 4,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1911.07228.pdf",
      "question": "According to the Vietnamese NER error analysis paper, how do the F1 scores of the BLSTM-CNN-CRF and BLSTM-CRF models compare when using the VLSP 2016 dataset with Kyubyong Park’s or Edouard Grave’s pre-trained word embeddings?",
      "prediction": "The F1 scores of the BLSTM-CNN-CRF and BLSTM-CRF models are not provided in the document.",
      "answer": "[{'answer': 'Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ', 'type': 'abstractive'}]",
      "evidence": "Table 2 shows our experiments on two models with and without different pre-trained word embedding – KP means the Kyubyong Park’s pre-trained word embeddings and EG means Edouard Grave’s pre-trained word embeddings.",
      "pages_used": [
        {
          "doc": "1911.07228.pdf",
          "page": 6,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1903.09722.pdf",
      "question": "Which prior work, using a task-specific summarization architecture, is compared against the pre-trained sequence-to-sequence model in this paper?",
      "prediction": "The pre-trained sequence-to-sequence model is compared against the task-specific summarization architecture used in the prior work.",
      "answer": "[{'answer': 'BIBREF26 ', 'type': 'extractive'}, {'answer': 'BIBREF26', 'type': 'extractive'}]",
      "evidence": "Following BIBREF11 , we experiment on the non-anonymized version of . When generating summaries, we follow standard practice of tuning the maximum output length and disallow repeating the same trigram BIBREF27 , BIBREF14 . For this task we train language model representations on the combination of newscrawl and the training data. Table TABREF16 shows that pre-trained embeddings can significantly improve on top of a strong baseline transformer. We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline. Pre-trained representations are complementary to their method.",
      "pages_used": [
        {
          "doc": "1804.07789.pdf",
          "page": 12,
          "matched": false,
          "score": 0.3318777292576419
        }
      ]
    },
    {
      "doc": "2003.05377.pdf",
      "question": "What are the 14 genres that categorize the 138,368 Brazilian song lyrics used for genre classification in the study?",
      "prediction": "The 14 genres that categorize the 138,368 Brazilian song lyrics used for genre classification in the study are rock, pop, rap, samba, bossa nova, sertanejo, forro, choro, pagode, frevo, maracatu, freestyle",
      "answer": "[{'answer': 'Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: The number of songs and artists by genre",
      "pages_used": [
        {
          "doc": "1911.02711.pdf",
          "page": 6,
          "matched": false,
          "score": 0.023645320197044337
        }
      ]
    },
    {
      "doc": "1909.01958.pdf",
      "question": "What corpus, consisting of a large Web-crawled dataset of $5 \\times 10^{10}$ tokens from the University of Waterloo and targeted science content from platforms like Wikipedia and SimpleWikipedia, underpins the Aristo system's success on the New York Regents Science Exams?",
      "prediction": "The Aristo system's success on the New York Regents Science Exams is underpinned by the WebQ corpus, which consists of a large Web-crawled dataset of $5 \\times 10^{10}$ tokens from the University of Waterloo and targeted science content from platforms like Wikipedia and SimpleWikipedia",
      "answer": "[{'answer': 'Aristo Corpus\\nRegents 4th\\nRegents 8th\\nRegents `12th\\nARC-Easy\\nARC-challenge ', 'type': 'abstractive'}]",
      "evidence": "Several methods make use of the Aristo Corpus, comprising a large Web-crawled corpus ($5 \\times 10^{10}$ tokens (280GB)) originally from the University of Waterloo, combined with targeted science content from Wikipedia, SimpleWikipedia, and several smaller online science texts (BID25).",
      "pages_used": [
        {
          "doc": "1605.07683.pdf",
          "page": 15,
          "matched": false,
          "score": 0.022727272727272728
        }
      ]
    },
    {
      "doc": "1909.01013.pdf",
      "question": "What are the P@1 accuracy results for EN-IT, IT-EN, and other language pairs on the Vecmap benchmark after applying the proposed duality regularization method?",
      "prediction": "The P@1 accuracy results for EN-IT, IT-EN, and other language pairs on the Vecmap benchmark after applying the proposed duality regularization method are not provided in the given text.",
      "answer": "[{'answer': 'New best results of accuracy (P@1) on Vecmap:\\nOurs-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43', 'type': 'abstractive'}]",
      "evidence": "Table TABREF15 shows the final results on Vecmap.",
      "pages_used": [
        {
          "doc": "1607.06025.pdf",
          "page": 14,
          "matched": false,
          "score": 0.01702127659574468
        }
      ]
    },
    {
      "doc": "1909.01013.pdf",
      "question": "What are the P@1 accuracy results comparing the proposed duality regularization method and the best baseline method for each language pair (EN-IT, IT-EN, EN-DE, DE-EN, EN-FI, FI-EN, EN-ES, and ES-EN) on Vecmap?",
      "prediction": "The P@1 accuracy results comparing the proposed duality regularization method and the best baseline method for each language pair on Vecmap are as follows: EN-IT: 0.85, IT-EN: 0.86, EN-DE: 0.87, DE-EN:",
      "answer": "[{'answer': 'Proposed method vs best baseline result on Vecmap (Accuracy P@1):\\nEN-IT: 50 vs 50\\nIT-EN: 42.67 vs 42.67\\nEN-DE: 51.6 vs 51.47\\nDE-EN: 47.22 vs 46.96\\nEN-FI: 35.88 vs 36.24\\nFI-EN: 39.62 vs 39.57\\nEN-ES: 39.47 vs 39.30\\nES-EN: 36.43 vs 36.06', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Accuracy (P@1) on Vecmap. The best results are bolded. †Results as reported in the original paper. For unsupervised methods, we report the average accuracy across 10 runs.",
      "pages_used": [
        {
          "doc": "1701.02877.pdf",
          "page": 13,
          "matched": false,
          "score": 0.24166666666666667
        }
      ]
    },
    {
      "doc": "1909.01013.pdf",
      "question": "What are the six language pairs on which the duality regularization method significantly reduced back-translation inconsistencies compared to Adv-C?",
      "prediction": "The six language pairs are English-German, English-French, English-Spanish, English-Italian, English-Chinese, and English-Japanese.",
      "answer": "[{'answer': 'EN<->ES\\nEN<->DE\\nEN<->IT\\nEN<->EO\\nEN<->MS\\nEN<->FI', 'type': 'abstractive'}]",
      "evidence": "Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12.",
      "pages_used": [
        {
          "doc": "1709.10367.pdf",
          "page": 11,
          "matched": false,
          "score": 0.013523666416228399
        }
      ]
    },
    {
      "doc": "1909.00578.pdf",
      "question": "What are the Spearman’s ρ, Kendall’s τ, and Pearson’s r correlation values for the BEST-ROUGE versions across the DUC-05, DUC-06, and DUC-07 datasets, as evaluated by the SumQE model on questions Q1–Q5?",
      "prediction": "The Spearman’s ρ, Kendall’s τ, and Pearson’s r correlation values for the BEST-ROUGE versions across the DUC-05, DUC-06, and DUC-07 datasets, as evaluated by the SumQE model on questions Q1–Q5, are as follows",
      "answer": "[{'answer': 'High correlation results range from 0.472 to 0.936', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for Q1–Q5. BEST-ROUGE refers to the version that achieved best correlations and is different across years.",
      "pages_used": [
        {
          "doc": "1909.00578.pdf",
          "page": 4,
          "matched": false,
          "score": 0.032197853476434904
        }
      ]
    },
    {
      "doc": "1910.00458.pdf",
      "question": "Which baseline models were used for comparison against MMM on the DREAM dataset to demonstrate the accuracy improvements in the paper?",
      "prediction": "The paper used recurrent neural networks as baseline models for comparison against MMM on the DREAM dataset to demonstrate the accuracy improvements.",
      "answer": "[{'answer': 'FTLM++, BERT-large, XLNet', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Accuracy on the DREAM dataset. Performance marked by ? is reported by (Sun et al. 2019). Numbers in parentheses indicate the accuracy increased by MMM compared to the baselines.",
      "pages_used": [
        {
          "doc": "1910.12129.pdf",
          "page": 9,
          "matched": false,
          "score": 0.01863932898415657
        }
      ]
    },
    {
      "doc": "1910.05154.pdf",
      "question": "What bilingual model specifically translates word-level bilingual data into unsegmented phonemic sequences during the language documentation process detailed in the paper?",
      "prediction": "The bilingual model that translates word-level bilingual data into unsegmented phonemic sequences during the language documentation process is the Feature-Rich biLSTM model.",
      "answer": "[{'answer': ' Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target', 'type': 'extractive'}]",
      "evidence": "In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence).",
      "pages_used": [
        {
          "doc": "2002.01207.pdf",
          "page": 21,
          "matched": false,
          "score": 0.029217719132893498
        }
      ]
    },
    {
      "doc": "1810.12885.pdf",
      "question": "What models are evaluated in the ReCoRD dataset paper, comparing machine reading comprehension methods to human performance on commonsense reasoning tasks?",
      "prediction": "The ReCoRD dataset paper evaluates the performance of machine reading comprehension models against human performance on commonsense reasoning tasks.",
      "answer": "[{'answer': 'DocQA, SAN, QANet, ASReader, LM, Random Guess', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Performance of various methods and human.",
      "pages_used": [
        {
          "doc": "1810.12885.pdf",
          "page": 14,
          "matched": false,
          "score": 0.22641509433962265
        }
      ]
    },
    {
      "doc": "2003.07996.pdf",
      "question": "Which four languages are included in the SER datasets presented in the *Cross Lingual Cross Corpus Speech Emotion Recognition* paper?",
      "prediction": "The four languages included in the SER datasets are English, Spanish, German, and French.",
      "answer": "[{'answer': 'German, English, Italian, Chinese', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Datasets used for various SER experiments.",
      "pages_used": [
        {
          "doc": "2002.02492.pdf",
          "page": 15,
          "matched": false,
          "score": 0.24334600760456274
        }
      ]
    },
    {
      "doc": "2002.04181.pdf",
      "question": "Which sentiment class was predicted with the highest accuracy by the NLP tools in the ELS task in the study comparing named-entity recognition and entity-level sentiment analysis for political tweets?",
      "prediction": "neutral",
      "answer": "[{'answer': 'neutral sentiment', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Average Correct Classification Rate (CCR) for named-entity recognition (NER) of four presidential candidates and entity-level sentiment (ELS) analysis by NLP tools and crowdworkers",
      "pages_used": [
        {
          "doc": "2002.04181.pdf",
          "page": 3,
          "matched": false,
          "score": 0.02419091206276561
        }
      ]
    },
    {
      "doc": "2002.11910.pdf",
      "question": "What are the precision, recall, and F1 score improvements for both named entity recognition and nominal mention when applying the boundary assembling method compared to the models by He and Sun (2017) and Peng and Dredze (2017)?",
      "prediction": "The precision, recall, and F1 score improvements for both named entity recognition and nominal mention when applying the boundary assembling method compared to the models by He and Sun (2017) and Peng and Dredze (2017) are 2%.",
      "answer": "[{'answer': 'Overall F1 score:\\n- He and Sun (2017) 58.23\\n- Peng and Dredze (2017) 58.99\\n- Xu et al. (2018) 59.11', 'type': 'abstractive'}, {'answer': 'For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: The results of two previous models, and results of this study, in which we apply a boundary assembling method. Precision, recall, and F1 scores are shown for both named entity and nominal mention. For both tasks and their overall performance, we outperform the other two models.",
      "pages_used": [
        {
          "doc": "2002.11910.pdf",
          "page": 4,
          "matched": false,
          "score": 0.03473491773308958
        }
      ]
    },
    {
      "doc": "1909.11297.pdf",
      "question": "Which attention-based models are compared as soft-selection approaches against the proposed hard-selection method in the paper's evaluation of opinion snippet detection for aspect-based sentiment analysis?",
      "prediction": "The paper compares attention-based models such as the attention-based LSTM and the attention-based memory networks as soft-selection approaches against the proposed hard-selection method in the evaluation of opinion snippet detection for aspect-based sentiment analysis.",
      "answer": "[{'answer': 'LSTM and BERT ', 'type': 'abstractive'}]",
      "evidence": "Previous attention-based methods can be categorized as soft-selection approaches since the attention weights scatter across the whole sentence and every word is taken into consideration with different weights. ",
      "pages_used": [
        {
          "doc": "1909.11297.pdf",
          "page": 10,
          "matched": false,
          "score": 0.03990326481257558
        }
      ]
    },
    {
      "doc": "1810.10254.pdf",
      "question": "What perplexity scores on the dev and test sets are reported for the language model trained using augmented code-switched sentences in the \"Learn to Code-Switch\" paper?",
      "prediction": "The perplexity scores on the dev and test sets for the language model trained using augmented code-switched sentences in the \"Learn to Code-Switch\" paper are not provided in the given information.",
      "answer": "[{'answer': 'Perplexity score 142.84 on dev and 138.91 on test', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3. Language Modeling Results (in perplexity).",
      "pages_used": [
        {
          "doc": "2001.06888.pdf",
          "page": 13,
          "matched": false,
          "score": 0.2727272727272727
        }
      ]
    },
    {
      "doc": "2002.06644.pdf",
      "question": "What baseline models were used in the experiments on subjective bias detection in the Wiki Neutrality Corpus before comparing them to the proposed BERT-based ensembles?",
      "prediction": "The experiments on subjective bias detection in the Wiki Neutrality Corpus used baseline models such as BERT, RoBERTa, and DistilBERT before comparing them to the proposed BERT-based ensembles.",
      "answer": "[{'answer': 'FastText, BiLSTM, BERT', 'type': 'extractive'}, {'answer': 'FastText, BERT , two-layer BiLSTM architecture with GloVe word embeddings', 'type': 'extractive'}]",
      "evidence": "FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 17,
          "matched": false,
          "score": 0.27450980392156865
        }
      ]
    },
    {
      "doc": "2003.11563.pdf",
      "question": "What metrics are reported that assess BERT's performance on the unseen portion of the training set and the development set, when applying different augmentation techniques to the Propaganda Techniques Corpus (PTC)?",
      "prediction": "The metrics reported include accuracy, precision, recall, and F1-score.",
      "answer": "[{'answer': 'precision, recall , F1 score', 'type': 'extractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: F1 scores on an unseen (not used for training) part of the training set and the development set on BERT using different augmentation techniques.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.2724458204334365
        }
      ]
    },
    {
      "doc": "1810.09774.pdf",
      "question": "Which training dataset showed the highest test accuracy and best generalization across different natural language inference benchmarks?",
      "prediction": "The training dataset that showed the highest test accuracy and best generalization across different natural language inference benchmarks was MultiNLI.",
      "answer": "[{'answer': 'MultiNLI', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 4: Test accuracies (%). For the baseline results (highlighted in bold) the training data and test data have been drawn from the same benchmark corpus. ∆ is the difference between the test accuracy and the baseline accuracy for the same training set. Results marked with * are for the development set, as no annotated test set is openly available. Best scores with respect to accuracy and difference in accuracy are underlined.",
      "pages_used": [
        {
          "doc": "1810.09774.pdf",
          "page": 6,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1911.08962.pdf",
      "question": "What baseline models are listed in the CAIL2019-SCM paper for the Similar Case Matching task on the valid and test datasets?",
      "prediction": "The baseline models listed in the CAIL2019-SCM paper for the Similar Case Matching task on the valid and test datasets are SVM, LR, and CNN.",
      "answer": "[{'answer': 'CNN, LSTM, BERT', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Results of baselines and scores of top 3 participants on valid and test datasets.",
      "pages_used": [
        {
          "doc": "2001.06888.pdf",
          "page": 13,
          "matched": false,
          "score": 0.16753926701570682
        }
      ]
    },
    {
      "doc": "1804.11346.pdf",
      "question": "Does the NLI-PT dataset indicate that the distribution of essays among the various L1 groups is balanced?",
      "prediction": "No, the NLI-PT dataset does not indicate that the distribution of essays among the various L1 groups is balanced.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 2: Distribution by L1s and source corpora.",
      "pages_used": [
        {
          "doc": "1902.10525.pdf",
          "page": 14,
          "matched": false,
          "score": 0.0234192037470726
        }
      ]
    },
    {
      "doc": "2001.06888.pdf",
      "question": "What baseline NER models were compared to the proposed multimodal deep learning approaches that incorporate image features?",
      "prediction": "The baseline NER models compared to the proposed multimodal deep learning approaches that incorporate image features were the Stanford NER and the Distantly Supervised NER.",
      "answer": "[{'answer': 'Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Evaluation results of different approaches compared to ours",
      "pages_used": [
        {
          "doc": "2001.06888.pdf",
          "page": 13,
          "matched": false,
          "score": 0.17647058823529413
        }
      ]
    },
    {
      "doc": "1904.05584.pdf",
      "question": "Which references are cited in the caption when evaluating 11 downstream sentence-level transfer tasks using word vectors fed into a BiLSTM with max-pooling?",
      "prediction": "The references cited in the caption are not specified in the image.",
      "answer": "[{'answer': 'BIBREF13 , BIBREF18', 'type': 'extractive'}]",
      "evidence": "Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 .",
      "pages_used": [
        {
          "doc": "1906.03538.pdf",
          "page": 16,
          "matched": false,
          "score": 0.1572052401746725
        }
      ]
    },
    {
      "doc": "1912.10435.pdf",
      "question": "What were the exact F1 improvements reported after incorporating both Simple Skip and Transformer Skip connections into the coattention-enhanced base BERT model, in comparison to the baseline performance?",
      "prediction": "The exact F1 improvements reported after incorporating both Simple Skip and Transformer Skip connections into the coattention-enhanced base BERT model, in comparison to the baseline performance, were not provided in the given text.",
      "answer": "[{'answer': 'Simple Skip improves F1 from 74.34 to 74.81\\nTransformer Skip improes F1 from 74.34 to 74.95 ', 'type': 'abstractive'}]",
      "evidence": "Table TABREF20 reports the F1 and EM scores obtained for the experiments on the base model. The first column reports the base BERT baseline scores, while the second reports the results for the C2Q/Q2C attention addition. The two skip columns report scores for the skip connection connecting the BERT embedding layer to the coattention output (Simple Skip) and the scores for the same skip connection containing a Transformer block (Transformer Skip).",
      "pages_used": [
        {
          "doc": "2001.08868.pdf",
          "page": 15,
          "matched": false,
          "score": 0.1864406779661017
        }
      ]
    },
    {
      "doc": "1809.05752.pdf",
      "question": "What are the domain-specific F1 scores for the topic extraction models, and how do the Substance, Interpersonal, and Mood domains compare in terms of model performance in predicting psychiatric readmissions?",
      "prediction": "The domain-specific F1 scores for the topic extraction models are not provided in the given text. However, it is mentioned that the Substance, Interpersonal, and Mood domains show similar model performance in predicting psychiatric readmissions.",
      "answer": "[{'answer': 'Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 5: Overall and domain-specific Precision, Recall, and F1 scores for our models. The first row computes similarity directly from the TF-IDF matrix, as in (McCoy et al., 2015). All other rows are classifier outputs.",
      "pages_used": [
        {
          "doc": "1901.03866.pdf",
          "page": 9,
          "matched": false,
          "score": 0.032520325203252036
        }
      ]
    },
    {
      "doc": "1809.04960.pdf",
      "question": "In the retrieval and generative evaluation settings, by how much does the proposed model + IR2 outperform NVDM in terms of MRR, MR, Recall@10, BLEU, CIDEr, ROUGE, and METEOR in the \"Unsupervised Machine Commenting with Neural Variational Topic Model\" paper?",
      "prediction": "The proposed model + IR2 outperforms NVDM by 10% in terms of MRR, MR, Recall@10, BLEU, CIDEr, ROUGE, and METEOR in the \"Unsupervised Machine Commenting with Neural Variational Topic Model\" paper.",
      "answer": "[{'answer': 'Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . \\nUnder the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029', 'type': 'abstractive'}, {'answer': 'Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.', 'type': 'abstractive'}]",
      "evidence": "NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.",
      "pages_used": [
        {
          "doc": "1809.04960.pdf",
          "page": 9,
          "matched": false,
          "score": 0.3045267489711934
        }
      ]
    },
    {
      "doc": "1909.03405.pdf",
      "question": "What is the reported absolute performance gain on the RTE dataset when applying the 3-class NSP with PSP method, and how does it compare to BERTBase?",
      "prediction": "The reported absolute performance gain on the RTE dataset when applying the 3-class NSP with PSP method is 0.01, which is lower than BERTBase.",
      "answer": "[{'answer': ' improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase', 'type': 'extractive'}, {'answer': 'The average score improved by 1.4 points over the previous best result.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF21 illustrates the experimental results, showing that our method is beneficial for all of NLI tasks. The improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase.",
      "pages_used": [
        {
          "doc": "1801.05147.pdf",
          "page": 9,
          "matched": false,
          "score": 0.21904761904761905
        }
      ]
    },
    {
      "doc": "1612.08205.pdf",
      "question": "What are the 14 industry categories that were used for classifying social media users in the industry-detection system presented in this paper?",
      "prediction": "The 14 industry categories used for classifying social media users in the industry-detection system presented in this paper are: Automotive, Consumer Electronics, Consumer Goods, Consumer Services, Entertainment, Financial Services, Food and Beverage, Healthcare, Technology, Travel, Retail, and Sports.",
      "answer": "[{'answer': 'technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive', 'type': 'abstractive'}, {'answer': 'Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Industry categories and number of users per category.",
      "pages_used": [
        {
          "doc": "1810.12885.pdf",
          "page": 14,
          "matched": false,
          "score": 0.2542372881355932
        }
      ]
    },
    {
      "doc": "1812.06705.pdf",
      "question": "On which specific datasets did the authors validate the performance of the conditional BERT contextual augmentation, particularly in the domains of sentiment analysis, subjectivity detection, opinion polarity, and question classification?",
      "prediction": "The authors validated the performance of the conditional BERT contextual augmentation on the SST (Stanford Sentiment Treebank), TREC, MPQA, and RT datasets.",
      "answer": "[{'answer': 'SST (Stanford Sentiment Treebank), Subj (Subjectivity dataset), MPQA Opinion Corpus, RT is another movie review sentiment dataset, TREC is a dataset for classification of the six question types', 'type': 'extractive'}]",
      "evidence": "SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative, or very negative) or two labels (SST2: positive or negative).\n\nSubj BIBREF26 Subj (Subjectivity dataset) is annotated with whether a sentence is subjective or objective.\n\nMPQA BIBREF27 MPQA Opinion Corpus is an opinion polarity detection dataset of short phrases rather than sentences, which contains news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).\n\nRT BIBREF28 RT is another movie review sentiment dataset contains a collection of short review excerpts from Rotten Tomatoes collected by Bo Pang and Lillian Lee.\n\nTREC BIBREF29 TREC is a dataset for classification of the six question types (whether the question is about person, location, numeric information, etc.).",
      "pages_used": [
        {
          "doc": "1812.06705.pdf",
          "page": 5,
          "matched": false,
          "score": 0.056732223903177004
        }
      ]
    },
    {
      "doc": "1812.06705.pdf",
      "question": "What metric is used to evaluate CBERT's performance on six different datasets across two classifier architectures?",
      "prediction": "The evaluation metric used for CBERT's performance on six different datasets across two classifier architectures is the F1 score.",
      "answer": "[{'answer': 'Accuracy across six datasets', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018).",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.3125
        }
      ]
    },
    {
      "doc": "1812.06705.pdf",
      "question": "In the experiments presented, are any other pretrained language models besides conditional BERT evaluated for contextual augmentation across all datasets?",
      "prediction": "Yes, the experiments also evaluated other pretrained language models besides conditional BERT for contextual augmentation across all datasets.",
      "answer": "[{'answer': 'No', 'type': 'boolean'}]",
      "evidence": "FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018).",
      "pages_used": [
        {
          "doc": "1704.00939.pdf",
          "page": 6,
          "matched": false,
          "score": 0.2631578947368421
        }
      ]
    },
    {
      "doc": "1901.02257.pdf",
      "question": "Which baseline models, both in ensemble and single variations, are the MPFN model compared against in the experimental results?",
      "prediction": "The MPFN model is compared against the round-trip baseline models in both ensemble and single variations in the experimental results.",
      "answer": "[{'answer': 'SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Experimental Results of Models",
      "pages_used": [
        {
          "doc": "1911.03597.pdf",
          "page": 5,
          "matched": false,
          "score": 0.023980815347721823
        }
      ]
    },
    {
      "doc": "1904.10500.pdf",
      "question": "Based on the dataset's intent annotation results, do the intent labels show an imbalance in the distribution for the utterance-level AV intent detection task?",
      "prediction": "Yes",
      "answer": "[{'answer': 'Yes', 'type': 'boolean'}]",
      "evidence": " Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table TABREF7 and Table TABREF8 as a summary of dataset statistics.",
      "pages_used": [
        {
          "doc": "2002.01207.pdf",
          "page": 22,
          "matched": false,
          "score": 0.060085836909871244
        }
      ]
    },
    {
      "doc": "1704.06194.pdf",
      "question": "According to the performance results, on which QA benchmarks did their proposed KBQA system achieve either state-of-the-art or near-state-of-the-art accuracy?",
      "prediction": "The proposed KBQA system achieved either state-of-the-art or near-state-of-the-art accuracy on the SQuAD and HotpotQA benchmarks.",
      "answer": "[{'answer': 'SimpleQuestions, WebQSP', 'type': 'extractive'}, {'answer': 'WebQSP, SimpleQuestions', 'type': 'extractive'}]",
      "evidence": "As shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP",
      "pages_used": [
        {
          "doc": "1812.10479.pdf",
          "page": 39,
          "matched": false,
          "score": 0.05044843049327354
        }
      ]
    },
    {
      "doc": "1701.03214.pdf",
      "question": "How much does the \"mixed fine tuning\" method improve BLEU-4 scores compared to standard fine tuning for the IWSLT-CE domain on the 2011, 2012, and 2013 test sets using the NTCIR-CE data for domain adaptation?",
      "prediction": "The \"mixed fine tuning\" method improves BLEU-4 scores by 0.01, 0.02, and 0.03 for the 2011, 2012, and 2013 test sets, respectively.",
      "answer": "[{'answer': '0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE.",
      "pages_used": [
        {
          "doc": "1908.11546.pdf",
          "page": 7,
          "matched": false,
          "score": 0.021377672209026127
        }
      ]
    },
    {
      "doc": "1901.03866.pdf",
      "question": "How much does HAS-QA improve over traditional RC models like GA, BiDAF, and AQA on the QuasarT dataset in terms of EM and F1 scores?",
      "prediction": "HAS-QA improves 16.8% in EM score and 20.4% in F1 score over traditional RC models like GA, BiDAF, and AQA on the QuasarT dataset.",
      "answer": "[{'answer': 'For example, in QuasarT, it improves 16.8% in EM score and 20.4% in F1 score. , For example, in QuasarT, it improves 4.6% in EM score and 3.5% in F1 score.', 'type': 'extractive'}]",
      "evidence": "HAS-QA outperforms traditional RC baselines with a large gap, such as GA, BiDAF, AQA listed in the first part. For example, in QuasarT, it improves 16.8% in EM score and 20.4% in F1 score. As RC task is just a special case of OpenQA task. Some experiments on standard SQuAD dataset(dev-set) BIBREF9 show that HAS-QA yields EM/F1:0.719/0.798, which is comparable with the best released single model Reinforced Mnemonic Reader BIBREF25 in the leaderboard (dev-set) EM/F1:0.721/0.816. ",
      "pages_used": [
        {
          "doc": "1901.03866.pdf",
          "page": 6,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "2004.03744.pdf",
      "question": "How many corrected image-sentence pairs are there in the validation and test sets of the e-SNLI-VE-2.0 dataset with human-written natural language explanations?",
      "prediction": "There are 1,000 corrected image-sentence pairs in the validation and test sets of the e-SNLI-VE-2.0 dataset with human-written natural language explanations.",
      "answer": "[{'answer': 'Totally 6980 validation and test image-sentence pairs have been corrected.', 'type': 'abstractive'}]",
      "evidence": "The statistics of e-SNLI-VE-2.0 are shown in Table TABREF40.",
      "pages_used": [
        {
          "doc": "1904.07904.pdf",
          "page": 6,
          "matched": false,
          "score": 0.2
        }
      ]
    },
    {
      "doc": "1604.00400.pdf",
      "question": "How do different variants of ROUGE scores (e.g., ROUGE-1-P and ROUGE-3-F) correlate with human Pyramid scores using Pearson, Spearman, and Kendall correlations in the context of scientific article summarization?",
      "prediction": "The different variants of ROUGE scores (e.g., ROUGE-1-P and ROUGE-3-F) correlate with human Pyramid scores using Pearson, Spearman, and Kendall correlations in the context of scientific article summarization.",
      "answer": "[{'answer': 'we observe that many variants of Rouge scores do not have high correlations with human pyramid scores', 'type': 'extractive'}, {'answer': 'Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF23 shows the Pearson, Spearman and Kendall correlation of Rouge and Sera, with pyramid scores.",
      "pages_used": [
        {
          "doc": "2001.06888.pdf",
          "page": 13,
          "matched": false,
          "score": 0.2198952879581152
        }
      ]
    },
    {
      "doc": "1912.03457.pdf",
      "question": "What language technologies are listed as being particularly challenging to adopt for extremely-low resource languages, such as Gondi, due to data collection difficulties?",
      "prediction": "The document does not provide information about language technologies that are particularly challenging to adopt for extremely-low resource languages like Gondi due to data collection difficulties.",
      "answer": "[{'answer': '- Font & Keyboard\\n- Speech-to-Text\\n- Text-to-Speech\\n- Text Prediction\\n- Spell Checker\\n- Grammar Checker\\n- Text Search\\n- Machine Translation\\n- Voice to Text Search\\n- Voice to Speech Search', 'type': 'abstractive'}]",
      "evidence": "Table TABREF6 describes the various technologies and their presence concerning languages with different levels of resource availability and the ease of data collection. We can observe that for low resource languages, there is considerable difficulty in adopting these tools.",
      "pages_used": [
        {
          "doc": "1904.07904.pdf",
          "page": 6,
          "matched": false,
          "score": 0.2792207792207792
        }
      ]
    },
    {
      "doc": "1710.06700.pdf",
      "question": "What was the lemmatization accuracy reported for the WikiNews test set in the paper that evaluated an Arabic lemmatizer?",
      "prediction": "The paper did not report the lemmatization accuracy for the WikiNews test set.",
      "answer": "[{'answer': '97.32%', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 3: Lemmatization accuracy using WikiNews testset",
      "pages_used": [
        {
          "doc": "1710.06700.pdf",
          "page": 4,
          "matched": false,
          "score": 0.07392197125256673
        }
      ]
    },
    {
      "doc": "1910.02339.pdf",
      "question": "What are the Full Testing Set and Cleaned Testing Set accuracies achieved by TP-N2F on the AlgoLisp dataset, and how does this performance compare to LSTM-based Seq2Seq models?",
      "prediction": "The TP-N2F model achieved accuracies of 65.0% and 64.0% on the Full Testing Set and Cleaned Testing Set, respectively, for the AlgoLisp dataset. This performance is comparable to LSTM-based Seq2Seq models.",
      "answer": "[{'answer': 'Full Testing Set accuracy: 84.02\\nCleaned Testing Set accuracy: 93.48', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Results of AlgoLisp dataset",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.28708133971291866
        }
      ]
    },
    {
      "doc": "1910.02339.pdf",
      "question": "What are the Full Testing Set Accuracy and Cleaned Testing Set Accuracy of the TP-N2F model on the AlgoLisp dataset?",
      "prediction": "The Full Testing Set Accuracy and Cleaned Testing Set Accuracy of the TP-N2F model on the AlgoLisp dataset are not provided in the image.",
      "answer": "[{'answer': 'Full Testing Set Accuracy: 84.02\\nCleaned Testing Set Accuracy: 93.48', 'type': 'abstractive'}]",
      "evidence": "FLOAT SELECTED: Table 2: Results of AlgoLisp dataset",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.28708133971291866
        }
      ]
    },
    {
      "doc": "1910.02339.pdf",
      "question": "What operation and execution accuracies did the TP-N2F model achieve on the MathQA dataset in comparison to the SEQ2PROG model?",
      "prediction": "The TP-N2F model achieved 90.0% operation accuracy and 85.0% execution accuracy on the MathQA dataset, compared to 85.0% operation accuracy and 80.0% execution accuracy for the SEQ2PROG model.",
      "answer": "[{'answer': 'Operation accuracy: 71.89\\nExecution accuracy: 55.95', 'type': 'abstractive'}]",
      "evidence": "Our model outperforms both the original seq2prog, designated SEQ2PROG-orig, and the best reimplemented seq2prog after an extensive hyperparameter search, designated SEQ2PROG-best. Table TABREF16 presents the results.",
      "pages_used": [
        {
          "doc": "1910.02339.pdf",
          "page": 18,
          "matched": false,
          "score": 0.28865979381443296
        }
      ]
    },
    {
      "doc": "1701.06538.pdf",
      "question": "What reduction in test perplexity does the MoE model achieve compared to the best published model after 10 epochs in the language modeling benchmark?",
      "prediction": "The MoE model achieves a reduction in test perplexity of 10% compared to the best published model after 10 epochs in the language modeling benchmark.",
      "answer": "[{'answer': 'Perpexity is improved from 34.7 to 28.0.', 'type': 'abstractive'}]",
      "evidence": "The two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 .",
      "pages_used": [
        {
          "doc": "1605.08675.pdf",
          "page": 12,
          "matched": false,
          "score": 0.3533834586466165
        }
      ]
    },
    {
      "doc": "1909.00252.pdf",
      "question": "How did the accuracy and F1 score of the Transformer model on the Short Jokes dataset, as reported in the \"Humor Detection: A Transformer Gets the Last Laugh\" paper, compare to previous models like CNNs, and what was the magnitude of improvement?",
      "prediction": "The accuracy and F1 score of the Transformer model on the Short Jokes dataset were significantly higher than previous models like CNNs, with a magnitude of improvement of 10%.",
      "answer": "[{'answer': 'It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%', 'type': 'abstractive'}]",
      "evidence": "Our experiment with the Short Jokes dataset found the Transformer model's accuracy and F1 score to be 0.986. This was a jump of 8 percent from the most recent work done with CNNs (Table 4).",
      "pages_used": [
        {
          "doc": "2002.02492.pdf",
          "page": 15,
          "matched": false,
          "score": 0.2028985507246377
        }
      ]
    },
    {
      "doc": "1908.06151.pdf",
      "question": "In the experiment where you modified the layer configuration to 6-6-4 in the $N_{src}$-$N_{mt}$-$N_{pe}$ setup, which result demonstrated that reducing the number of layers in the final decoder ($N_{pe}$) does not notably affect APE performance, as detailed in Section 5.1?",
      "prediction": "The result that showed reducing the number of layers in the final decoder ($N_{pe}$) does not notably affect APE performance, as described in Section 5.1, was the one where the layer configuration was modified to 6-6-4 in the $N_{src}$-$N_{mt}$",
      "answer": "[{'answer': 'Exp. 5.1', 'type': 'extractive'}]",
      "evidence": "Last, we analyze the importance of our second encoder ($enc_{src \\rightarrow mt}$), compared to the source encoder ($enc_{src}$) and the decoder ($dec_{pe}$), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for $N_{src}$-$N_{mt}$-$N_{pe}$ (cf. Figure FIGREF1), where 6 is the value that was proposed by Vaswani:NIPS2017 for the base model. We investigate what happens in terms of APE performance if we change this setting to 6-6-4 and 6-4-6. ",
      "pages_used": [
        {
          "doc": "1908.06151.pdf",
          "page": 4,
          "matched": true,
          "score": 1.0
        }
      ]
    },
    {
      "doc": "1908.06151.pdf",
      "question": "Based on the experimental evidence presented in the paper, how does reducing the number of layers in the $enc_{src \\rightarrow mt}$ encoder block (Exp. 5.2) affect BLEU and TER scores, particularly for test2016 and test2017, and how do these results compare to the impact of reducing the number of decoder layers as discussed in Exp. 5.1?",
      "prediction": "Reducing the number of layers in the $enc_{src \\rightarrow mt}$ encoder block (Exp. 5.2) leads to a decrease in BLEU and TER scores for both test2016 and test2017, similar to the impact of reducing the number of decoder layers as discussed in",
      "answer": "[{'answer': 'comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ', 'type': 'abstractive'}]",
      "evidence": "Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \\rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder.",
      "pages_used": [
        {
          "doc": "1909.09484.pdf",
          "page": 10,
          "matched": false,
          "score": 0.24521072796934865
        }
      ]
    },
    {
      "doc": "1605.07333.pdf",
      "question": "By how much does the combined voting-based model of ER-CNN and R-RNN surpass the top state-of-the-art performance on the SemEval 2010 relation classification task?",
      "prediction": "The combined voting-based model of ER-CNN and R-RNN surpasses the top state-of-the-art performance on the SemEval 2010 relation classification task by 1.5 points.",
      "answer": "[{'answer': '0.8% F1 better than the best state-of-the-art', 'type': 'abstractive'}, {'answer': 'Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.', 'type': 'abstractive'}]",
      "evidence": "Table TABREF16 shows the results of our models ER-CNN (extended ranking CNN) and R-RNN (ranking RNN) in the context of other state-of-the-art models.",
      "pages_used": [
        {
          "doc": "1909.09587.pdf",
          "page": 8,
          "matched": false,
          "score": 0.06153846153846154
        }
      ]
    }
  ]
}